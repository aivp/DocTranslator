"""
æœ¯è¯­ç­›é€‰æ¨¡å—

æ ¹æ®æ–‡æœ¬å†…å®¹ç­›é€‰æœ€ç›¸å…³çš„æœ¯è¯­ï¼Œè§£å†³API Tokené™åˆ¶é—®é¢˜ã€‚
ä¿ç•™ç°æœ‰æœ¯è¯­åº“æ ¼å¼ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„æœ¯è¯­ã€‚

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼š1.0.0
"""

import re
import time
import logging
from typing import Dict, List, Tuple
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)

def calculate_similarity(text: str, term_source: str) -> float:
    """
    è®¡ç®—æ–‡æœ¬ä¸æœ¯è¯­çš„ç›¸ä¼¼åº¦åˆ†æ•°
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        term_source: æœ¯è¯­åŸæ–‡
        
    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if not text or not term_source:
        return 0.0
    
    # 1. ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
    if term_source in text:
        logger.debug(f"ç²¾ç¡®åŒ¹é…: '{term_source}' in '{text[:50]}...'")
        return 100.0
    
    # 2. å¿½ç•¥å¤§å°å†™çš„ç²¾ç¡®åŒ¹é…
    if term_source.lower() in text.lower():
        logger.debug(f"å¿½ç•¥å¤§å°å†™åŒ¹é…: '{term_source}' in '{text[:50]}...'")
        return 95.0
    
    # 3. æ­£åˆ™åŒ¹é…
    try:
        if re.search(re.escape(term_source), text, re.IGNORECASE):
            logger.debug(f"æ­£åˆ™åŒ¹é…: '{term_source}' in '{text[:50]}...'")
            return 90.0
    except re.error:
        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æœ‰é”™è¯¯ï¼Œè·³è¿‡æ­£åˆ™åŒ¹é…
        pass
    
    # 4. è¯é¢‘åŒ¹é…ï¼ˆè®¡ç®—å…±åŒè¯æ±‡ï¼‰
    text_words = set(text.lower().split())
    term_words = set(term_source.lower().split())
    common_words = text_words & term_words
    
    if common_words:
        # è®¡ç®—è¯æ±‡é‡å åº¦
        overlap_ratio = len(common_words) / len(term_words)
        score = overlap_ratio * 70.0  # æœ€é«˜70åˆ†
        logger.debug(f"è¯æ±‡åŒ¹é…: '{term_source}' ä¸ '{text[:50]}...' é‡å åº¦ {overlap_ratio:.2f}")
        return score
    
    # 5. åºåˆ—ç›¸ä¼¼åº¦ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
    similarity = SequenceMatcher(None, text.lower(), term_source.lower()).ratio()
    if similarity > 0.3:  # åªè€ƒè™‘ç›¸ä¼¼åº¦å¤§äº30%çš„
        score = similarity * 50.0  # æœ€é«˜50åˆ†
        logger.debug(f"åºåˆ—ç›¸ä¼¼åº¦: '{term_source}' ä¸ '{text[:50]}...' ç›¸ä¼¼åº¦ {similarity:.2f}")
        return score
    
    return 0.0

def filter_relevant_terms(text: str, all_terms: Dict[str, str], max_terms: int = 50) -> List[Dict[str, str]]:
    """
    æ ¹æ®æ–‡æœ¬å†…å®¹ç­›é€‰æœ€ç›¸å…³çš„æœ¯è¯­ï¼ˆæŒ‰è¯æ±‡åŒ¹é…ï¼‰
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        max_terms: æœ€å¤§æœ¯è¯­æ•°é‡ï¼ˆé»˜è®¤50ä¸ªï¼‰
        
    Returns:
        List[Dict]: ç­›é€‰åçš„æœ¯è¯­åˆ—è¡¨ [{"source": "...", "target": "..."}]
    """
    if not text or not all_terms:
        logger.debug("æ–‡æœ¬æˆ–æœ¯è¯­åº“ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    logger.info(f"å¼€å§‹è¯æ±‡åŒ¹é…ç­›é€‰æœ¯è¯­ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}, æœ¯è¯­åº“å¤§å°: {len(all_terms)}")
    
    # åˆ†è¯å¤„ç†ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·åˆ†è¯ï¼‰
    import re
    words = re.findall(r'\b\w+\b', text.lower())
    logger.debug(f"æ–‡æœ¬åˆ†è¯ç»“æœ: {words}")
    
    # ä¸ºæ¯ä¸ªè¯æ±‡æ‰¾åˆ°æœ€ç›¸å…³çš„æœ¯è¯­
    word_term_mapping = {}  # {word: [terms]}
    
    for word in words:
        if len(word) < 2:  # è·³è¿‡å¤ªçŸ­çš„è¯
            continue
            
        word_terms = []
        for source, target in all_terms.items():
            # è®¡ç®—è¯æ±‡ä¸æœ¯è¯­çš„åŒ¹é…åº¦
            score = calculate_word_similarity(word, source)
            if score > 0:
                word_terms.append({
                    'source': source,
                    'target': target,
                    'score': score,
                    'matched_word': word
                })
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œæ¯ä¸ªè¯æ±‡æœ€å¤šå–5ä¸ªæœ€ç›¸å…³çš„æœ¯è¯­
        word_terms.sort(key=lambda x: x['score'], reverse=True)
        word_term_mapping[word] = word_terms[:5]
    
    # åˆå¹¶æ‰€æœ‰è¯æ±‡çš„æœ¯è¯­ï¼Œå»é‡
    all_scored_terms = {}
    for word, terms in word_term_mapping.items():
        for term in terms:
            term_key = f"{term['source']}:{term['target']}"
            if term_key not in all_scored_terms:
                all_scored_terms[term_key] = term
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œå–æœ€é«˜åˆ†
                if term['score'] > all_scored_terms[term_key]['score']:
                    all_scored_terms[term_key] = term
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
    scored_terms = list(all_scored_terms.values())
    scored_terms.sort(key=lambda x: x['score'], reverse=True)
    
    # é™åˆ¶æ€»æ•°ä¸è¶…è¿‡max_terms
    selected_terms = scored_terms[:max_terms]
    
    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    result = []
    for term in selected_terms:
        result.append({
            'source': term['source'],
            'target': term['target']
        })
    
    # logger.info(f"æœ¯è¯­ç­›é€‰å®Œæˆ: {len(all_terms)} -> {len(result)} ä¸ªæœ¯è¯­")
    
    return result

def calculate_word_similarity(word: str, term_source: str) -> float:
    """
    è®¡ç®—å•ä¸ªè¯æ±‡ä¸æœ¯è¯­çš„ç›¸ä¼¼åº¦
    
    Args:
        word: å•ä¸ªè¯æ±‡
        term_source: æœ¯è¯­åŸæ–‡
        
    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if not word or not term_source:
        return 0.0
    
    # 1. ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
    if word == term_source.lower():
        logger.debug(f"è¯æ±‡ç²¾ç¡®åŒ¹é…: '{word}' == '{term_source}'")
        return 100.0
    
    # 2. è¯æ±‡åŒ…å«å…³ç³»
    if word in term_source.lower():
        logger.debug(f"è¯æ±‡åŒ…å«åŒ¹é…: '{word}' in '{term_source}'")
        return 90.0
    
    # 3. æœ¯è¯­åŒ…å«è¯æ±‡
    if term_source.lower() in word:
        logger.debug(f"æœ¯è¯­åŒ…å«è¯æ±‡: '{term_source}' in '{word}'")
        return 85.0
    
    # 4. è¯æ±‡é‡å åº¦
    word_chars = set(word)
    term_chars = set(term_source.lower())
    common_chars = word_chars & term_chars
    
    if common_chars:
        overlap_ratio = len(common_chars) / max(len(word_chars), len(term_chars))
        if overlap_ratio > 0.5:  # å­—ç¬¦é‡å åº¦å¤§äº50%
            score = overlap_ratio * 70.0
            logger.debug(f"å­—ç¬¦é‡å åŒ¹é…: '{word}' ä¸ '{term_source}' é‡å åº¦ {overlap_ratio:.2f}")
            return score
    
    # 5. åºåˆ—ç›¸ä¼¼åº¦ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
    similarity = SequenceMatcher(None, word, term_source.lower()).ratio()
    if similarity > 0.6:  # ç›¸ä¼¼åº¦å¤§äº60%
        score = similarity * 60.0
        logger.debug(f"åºåˆ—ç›¸ä¼¼åº¦: '{word}' ä¸ '{term_source}' ç›¸ä¼¼åº¦ {similarity:.2f}")
        return score
    
    return 0.0

def optimize_terms_for_api(text: str, all_terms: Dict[str, str], max_terms: int = 50) -> List[Dict[str, str]]:
    """
    ä¸ºAPIè°ƒç”¨ä¼˜åŒ–æœ¯è¯­åº“
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        max_terms: æœ€å¤§æœ¯è¯­æ•°é‡
        
    Returns:
        List[Dict]: ä¼˜åŒ–åçš„æœ¯è¯­åˆ—è¡¨ï¼Œæ ¼å¼ä¸Qwen APIå…¼å®¹
    """
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ç­›é€‰ç›¸å…³æœ¯è¯­
    relevant_terms = filter_relevant_terms(text, all_terms, max_terms)
    
    # æ£€æŸ¥æœ¯è¯­åº“å¤§å°
    total_chars = sum(len(term['source']) + len(term['target']) for term in relevant_terms)
    # å¦‚æœæœ¯è¯­åº“å¤ªå¤§ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–
    if total_chars > 2000:  # å‡è®¾2000å­—ç¬¦ä¸ºå®‰å…¨é™åˆ¶
        # logger.info(f"æœ¯è¯­åº“è¿‡å¤§({total_chars}å­—ç¬¦)ï¼Œè¿›è¡Œä¼˜åŒ–")
        # æŒ‰åˆ†æ•°æ’åºï¼Œä¼˜å…ˆä¿ç•™é«˜åˆ†æœ¯è¯­
        scored_terms = []
        for term in relevant_terms:
            score = calculate_similarity(text, term['source'])
            scored_terms.append((term, score))
        
        scored_terms.sort(key=lambda x: x[1], reverse=True)
        
        # é€æ­¥å‡å°‘æœ¯è¯­æ•°é‡ï¼Œç›´åˆ°æ»¡è¶³å­—ç¬¦é™åˆ¶
        optimized_terms = []
        current_chars = 0
        for term, score in scored_terms:
            term_chars = len(term['source']) + len(term['target'])
            if current_chars + term_chars <= 2000:
                optimized_terms.append(term)
                current_chars += term_chars
            else:
                break
        
        # logger.info(f"æœ¯è¯­åº“ä¼˜åŒ–å®Œæˆ: {len(optimized_terms)}ä¸ªæœ¯è¯­")
        
        # è®¡ç®—æ€»ç”¨æ—¶
        end_time = time.time()
        duration = end_time - start_time
        logging.info(f"ğŸ“š æœ¯è¯­ç­›é€‰ç®—æ³•ç”¨æ—¶: {duration:.3f}ç§’, ç­›é€‰ç»“æœ: {len(optimized_terms)}ä¸ªæœ¯è¯­")
        
        return optimized_terms
    
    # è®¡ç®—æ€»ç”¨æ—¶
    end_time = time.time()
    duration = end_time - start_time
    logging.info(f"ğŸ“š æœ¯è¯­ç­›é€‰ç®—æ³•ç”¨æ—¶: {duration:.3f}ç§’, ç­›é€‰ç»“æœ: {len(relevant_terms)}ä¸ªæœ¯è¯­")
    
    return relevant_terms

def batch_filter_terms(texts: List[str], all_terms: Dict[str, str], max_terms: int = 10) -> List[List[Dict[str, str]]]:
    """
    æ‰¹é‡ç­›é€‰æœ¯è¯­ï¼ˆç”¨äºå¤šæ–‡æœ¬ç¿»è¯‘ï¼‰
    
    Args:
        texts: è¦ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        max_terms: æ¯ä¸ªæ–‡æœ¬çš„æœ€å¤§æœ¯è¯­æ•°é‡
        
    Returns:
        List[List[Dict]]: æ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„æœ¯è¯­åˆ—è¡¨
    """
    results = []
    for i, text in enumerate(texts):
        logger.debug(f"å¤„ç†ç¬¬ {i+1}/{len(texts)} ä¸ªæ–‡æœ¬")
        terms = optimize_terms_for_api(text, all_terms, max_terms)
        results.append(terms)
    
    return results 