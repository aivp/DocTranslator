import os
import time
import datetime
import traceback
from pathlib import Path
import threading
from docx import Document
from docx.oxml.ns import qn
from . import to_translate
from . import common
import zipfile
import xml.etree.ElementTree as ET
from threading import Lock
import re
import shutil

from ..utils.doc2x import Doc2XService

# çº¿ç¨‹å®‰å…¨æ‰“å°é”
print_lock = Lock()

# ç‰¹æ®Šç¬¦å·å’Œæ•°å­¦ç¬¦å·çš„æ­£åˆ™è¡¨è¾¾å¼
SPECIAL_SYMBOLS_PATTERN = re.compile(
    r'^[â˜…â˜†â™¥â™¦â™£â™ â™€â™‚â˜¼â˜¾â˜½â™ªâ™«â™¬â˜‘â˜’âœ“âœ”âœ•âœ–âœ—âœ˜âŠ•âŠ—âˆâˆ‘âˆÏ€Î Â±Ã—Ã·âˆšâˆ›âˆœâˆ«âˆ®âˆ‡âˆ‚âˆ†âˆâˆ‘âˆšâˆâˆâˆŸâˆ âˆ¡âˆ¢âˆ£âˆ¤âˆ¥âˆ¦âˆ§âˆ¨âˆ©âˆªâˆ«âˆ¬âˆ­âˆ®âˆ¯âˆ°âˆ±âˆ²âˆ³âˆ´âˆµâˆ¶âˆ·âˆ¸âˆ¹âˆºâˆ»âˆ¼âˆ½âˆ¾âˆ¿â‰€â‰â‰‚â‰ƒâ‰„â‰…â‰†â‰‡â‰ˆâ‰‰â‰Šâ‰‹â‰Œâ‰â‰â‰â‰â‰‘â‰’â‰“â‰”â‰•â‰–â‰—â‰˜â‰™â‰šâ‰›â‰œâ‰â‰â‰Ÿâ‰ â‰¡â‰¢â‰£â‰¤â‰¥â‰¦â‰§â‰¨â‰©â‰ªâ‰«â‰¬â‰­â‰®â‰¯â‰°â‰±â‰²â‰³â‰´â‰µâ‰¶â‰·â‰¸â‰¹â‰ºâ‰»â‰¼â‰½â‰¾â‰¿âŠ€âŠâŠ‚âŠƒâŠ„âŠ…âŠ†âŠ‡âŠˆâŠ‰âŠŠâŠ‹âŠŒâŠâŠâŠâŠâŠ‘âŠ’âŠ“âŠ”âŠ•âŠ–âŠ—âŠ˜âŠ™âŠšâŠ›âŠœâŠâŠâŠŸâŠ âŠ¡âŠ¢âŠ£âŠ¤âŠ¥âŠ¦âŠ§âŠ¨âŠ©âŠªâŠ«âŠ¬âŠ­âŠ®âŠ¯âŠ°âŠ±âŠ²âŠ³âŠ´âŠµâŠ¶âŠ·âŠ¸âŠ¹âŠºâŠ»âŠ¼âŠ½âŠ¾âŠ¿â‹€â‹â‹‚â‹ƒâ‹„â‹…â‹†â‹‡â‹ˆâ‹‰â‹Šâ‹‹â‹Œâ‹â‹â‹â‹â‹‘â‹’â‹“â‹”â‹•â‹–â‹—â‹˜â‹™â‹šâ‹›â‹œâ‹â‹â‹Ÿâ‹ â‹¡â‹¢â‹£â‹¤â‹¥â‹¦â‹§â‹¨â‹©â‹ªâ‹«â‹¬â‹­â‹®â‹¯â‹°â‹±â‹²â‹³â‹´â‹µâ‹¶â‹·â‹¸â‹¹â‹ºâ‹»â‹¼â‹½â‹¾â‹¿]+$')

# çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹çš„æ­£åˆ™è¡¨è¾¾å¼
NUMBERS_PATTERN = re.compile(r'^[\d\s\.,\-\+\*\/\(\)\[\]\{\}]+$')


def check_docx_quality(docx_path):
    """æ£€æŸ¥è½¬æ¢åçš„DOCXæ–‡ä»¶è´¨é‡ï¼Œåˆ†æç¼–ç å’Œæ–‡æœ¬å†…å®¹"""
    try:
        print(f"\n=== å¼€å§‹DOCXæ–‡ä»¶è´¨é‡æ£€æŸ¥ ===")
        print(f"æ–‡ä»¶è·¯å¾„: {docx_path}")
        
        # æ£€æŸ¥æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        file_size = os.path.getsize(docx_path)
        print(f"æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        
        if file_size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
            print("âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤§å°å¼‚å¸¸ï¼Œå¯èƒ½è½¬æ¢å¤±è´¥")
            return False
        
        # å°è¯•åŠ è½½æ–‡æ¡£
        try:
            document = Document(docx_path)
            print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
            return False
        
        # åˆ†ææ–‡æ¡£ç»“æ„
        paragraph_count = len(document.paragraphs)
        table_count = len(document.tables)
        section_count = len(document.sections)
        
        print(f"æ–‡æ¡£ç»“æ„: {paragraph_count} ä¸ªæ®µè½, {table_count} ä¸ªè¡¨æ ¼, {section_count} ä¸ªèŠ‚")
        
        # åˆ†æå‰å‡ ä¸ªæ®µè½çš„æ–‡æœ¬å†…å®¹
        print(f"\n--- å‰5ä¸ªæ®µè½å†…å®¹åˆ†æ ---")
        chinese_chars = 0
        total_chars = 0
        sample_texts = []
        
        for i, para in enumerate(document.paragraphs[:5]):
            if para.text.strip():
                text = para.text.strip()
                sample_texts.append(text)
                
                # ç»Ÿè®¡å­—ç¬¦
                para_chinese = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
                para_total = len(text)
                chinese_chars += para_chinese
                total_chars += para_total
                
                # æ˜¾ç¤ºæ®µè½å†…å®¹ï¼ˆé™åˆ¶é•¿åº¦é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                display_text = text[:100] + "..." if len(text) > 100 else text
                print(f"æ®µè½{i+1}: '{display_text}'")
                print(f"  é•¿åº¦: {para_total}, ä¸­æ–‡å­—ç¬¦: {para_chinese}")
                
                # æ˜¾ç¤ºç¼–ç ä¿¡æ¯
                try:
                    encoded = text.encode('utf-8')
                    print(f"  UTF-8ç¼–ç : {encoded}")
                except Exception as e:
                    print(f"  ç¼–ç æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # åˆ†æè¡¨æ ¼å†…å®¹
        if table_count > 0:
            print(f"\n--- è¡¨æ ¼å†…å®¹åˆ†æ ---")
            table_chinese = 0
            table_total = 0
            
            for i, table in enumerate(document.tables[:2]):  # åªåˆ†æå‰2ä¸ªè¡¨æ ¼
                print(f"è¡¨æ ¼{i+1}:")
                for row_idx, row in enumerate(table.rows[:3]):  # åªåˆ†æå‰3è¡Œ
                    for col_idx, cell in enumerate(row.cells[:3]):  # åªåˆ†æå‰3åˆ—
                        if cell.text.strip():
                            text = cell.text.strip()
                            cell_chinese = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
                            cell_total = len(text)
                            table_chinese += cell_chinese
                            table_total += cell_total
                            
                            if cell_total > 0:
                                display_text = text[:50] + "..." if len(text) > 50 else text
                                print(f"  å•å…ƒæ ¼[{row_idx+1},{col_idx+1}]: '{display_text}' (ä¸­æ–‡å­—ç¬¦: {cell_chinese})")
            
            chinese_chars += table_chinese
            total_chars += table_total
            print(f"è¡¨æ ¼æ€»è®¡: {table_chinese} ä¸ªä¸­æ–‡å­—ç¬¦, {table_total} ä¸ªæ€»å­—ç¬¦")
        
        # ç»Ÿè®¡ç»“æœ
        print(f"\n--- è´¨é‡æ£€æŸ¥ç»“æœ ---")
        if total_chars > 0:
            chinese_ratio = (chinese_chars / total_chars) * 100
            print(f"ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹: {chinese_chars}/{total_chars} ({chinese_ratio:.1f}%)")
            
            if chinese_ratio < 10:
                print("âš ï¸  è­¦å‘Š: ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜")
            elif chinese_ratio > 80:
                print("âœ… ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹æ­£å¸¸")
            else:
                print("â„¹ï¸  ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ä¸­ç­‰")
        else:
            print("âš ï¸  è­¦å‘Š: æœªå‘ç°ä»»ä½•æ–‡æœ¬å†…å®¹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é—®é¢˜
        if paragraph_count == 0 and table_count == 0:
            print("âŒ ä¸¥é‡é—®é¢˜: æ–‡æ¡£æ²¡æœ‰ä»»ä½•å†…å®¹")
            return False
        
        if chinese_chars == 0 and total_chars > 0:
            print("âš ï¸  è­¦å‘Š: æœ‰æ–‡æœ¬å†…å®¹ä½†æ²¡æœ‰ä¸­æ–‡å­—ç¬¦ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜")
        
        print(f"=== DOCXè´¨é‡æ£€æŸ¥å®Œæˆ ===\n")
        return True
        
    except Exception as e:
        print(f"âŒ DOCXè´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        traceback.print_exc()
        return False


def get_doc2x_save_dir():
    # è·å–åŸºç¡€å­˜å‚¨ç›®å½•
    base_dir = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))).parent.absolute()

    # åˆ›å»ºæ—¥æœŸå­ç›®å½•ï¼ˆYYYY-MM-DDï¼‰
    date_str = datetime.datetime.now().strftime('%Y-%m-%d')
    save_dir = base_dir / "storage" / "doc2x_results" / date_str

    save_dir.mkdir(parents=True, exist_ok=True)

    return str(save_dir)


def start(trans):
    """PDFç¿»è¯‘å…¥å£"""
    print("ğŸš¨ DEBUG: PDFç¿»è¯‘å‡½æ•°è¢«è°ƒç”¨ï¼")
    print("ğŸš¨ DEBUG: è¿™æ˜¯å¼ºåˆ¶è¾“å‡ºæµ‹è¯•")
    
    try:
        # å¼€å§‹æ—¶é—´
        start_time = datetime.datetime.now()
        print(f"=== å¼€å§‹PDFç¿»è¯‘ä»»åŠ¡ ===")
        print(f"ä»»åŠ¡ID: {trans['id']}")
        print(f"æºæ–‡ä»¶: {trans['file_path']}")
        print(f"ç›®æ ‡æ–‡ä»¶: {trans['target_file']}")
        print(f"å¼€å§‹æ—¶é—´: {start_time}")
        print("=" * 50)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        original_path = Path(trans['file_path'])
        print(f"æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨: {original_path}")
        if not original_path.exists():
            print(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {trans['file_path']}")
            to_translate.error(trans['id'], f"æ–‡ä»¶ä¸å­˜åœ¨: {trans['file_path']}")
            return False
        print(f"âœ… æºæ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {os.path.getsize(original_path)} å­—èŠ‚")

        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target_dir = os.path.dirname(trans['target_file'])
        print(f"åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
        os.makedirs(target_dir, exist_ok=True)
        print(f"âœ… ç›®æ ‡ç›®å½•å·²å‡†å¤‡")

        # è·å–doc2xä¿å­˜ç›®å½•
        doc2x_save_dir = get_doc2x_save_dir()
        print(f"ğŸ“ doc2xä¿å­˜ç›®å½•: {doc2x_save_dir}")

        # è®¾ç½®è½¬æ¢åçš„Wordæ–‡ä»¶è·¯å¾„ï¼ˆä¿å­˜åœ¨doc2x_resultsç›®å½•ä¸‹ï¼‰
        docx_filename = f"{original_path.stem}_doc2x.docx"
        docx_path = os.path.join(doc2x_save_dir, docx_filename)
        print(f"ğŸ“„ è½¬æ¢åçš„DOCXæ–‡ä»¶è·¯å¾„: {docx_path}")

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(docx_path):
            try:
                os.remove(docx_path)
                print(f"å·²åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {docx_path}")
            except Exception as e:
                print(f"åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶å¤±è´¥: {str(e)}")
                # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
                docx_filename = f"{original_path.stem}_doc2x_{int(time.time())}.docx"
                docx_path = os.path.join(doc2x_save_dir, docx_filename)

        print(f"è½¬æ¢åçš„DOCXæ–‡ä»¶å°†ä¿å­˜åœ¨: {docx_path}")

        # ä½¿ç”¨Doc2XæœåŠ¡å°†PDFè½¬æ¢ä¸ºDOCX
        print(f"å¼€å§‹å°†PDFè½¬æ¢ä¸ºDOCX: {original_path}")

        # è·å–APIå¯†é’¥
        api_key = trans.get('doc2x_api_key', '')
        print(f"ğŸ”‘ æ£€æŸ¥APIå¯†é’¥: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
        if not api_key:
            print(f"âŒ ç¼ºå°‘Doc2X APIå¯†é’¥")
            to_translate.error(trans['id'], "ç¼ºå°‘Doc2X APIå¯†é’¥")
            return False
        print(f"âœ… APIå¯†é’¥å·²è®¾ç½®")

        # 1. å¯åŠ¨è½¬æ¢ä»»åŠ¡
        print(f"ğŸš€ å¼€å§‹å¯åŠ¨Doc2Xè½¬æ¢ä»»åŠ¡...")
        try:
            uid = Doc2XService.start_task(api_key, str(original_path))
            print(f"âœ… Doc2Xä»»åŠ¡å¯åŠ¨æˆåŠŸï¼ŒUID: {uid}")
        except Exception as e:
            print(f"âŒ å¯åŠ¨Doc2Xä»»åŠ¡å¤±è´¥: {str(e)}")
            to_translate.error(trans['id'], f"å¯åŠ¨Doc2Xä»»åŠ¡å¤±è´¥: {str(e)}")
            return False

        # 2. ç­‰å¾…è§£æå®Œæˆ
        max_retries = 60  # æœ€å¤šç­‰å¾…10åˆ†é’Ÿ
        retry_count = 0
        while retry_count < max_retries:
            try:
                status_info = Doc2XService.check_parse_status(api_key, uid)
                if status_info['status'] == 'success':
                    print(f"PDFè§£ææˆåŠŸ: {uid}")
                    break
                elif status_info['status'] == 'failed':
                    to_translate.error(trans['id'],
                                       f"PDFè§£æå¤±è´¥: {status_info.get('detail', 'æœªçŸ¥é”™è¯¯')}")
                    return False

                # æ›´æ–°è¿›åº¦
                progress = int(status_info.get('progress', 0) * 50)  # è§£æé˜¶æ®µå æ€»è¿›åº¦çš„50%
                print(f"PDFè§£æè¿›åº¦: {progress}%")

                # ç­‰å¾…10ç§’åå†æ¬¡æ£€æŸ¥
                time.sleep(10)
                retry_count += 1
            except Exception as e:
                to_translate.error(trans['id'], f"æ£€æŸ¥è§£æçŠ¶æ€å¤±è´¥: {str(e)}")
                return False

        if retry_count >= max_retries:
            to_translate.error(trans['id'], "PDFè§£æè¶…æ—¶")
            return False

        # 3. è§¦å‘å¯¼å‡º
        try:
            export_success = Doc2XService.trigger_export(api_key, uid, original_path.stem)
            if not export_success:
                to_translate.error(trans['id'], "è§¦å‘å¯¼å‡ºå¤±è´¥")
                return False
            print(f"å·²è§¦å‘å¯¼å‡º: {uid}")
        except Exception as e:
            to_translate.error(trans['id'], f"è§¦å‘å¯¼å‡ºå¤±è´¥: {str(e)}")
            return False

        # 4. ç­‰å¾…å¯¼å‡ºå®Œæˆå¹¶ä¸‹è½½
        try:
            download_url = Doc2XService.check_export_status(api_key, uid)
            print(f"è·å–åˆ°ä¸‹è½½é“¾æ¥: {download_url}")

            # ä¸‹è½½æ–‡ä»¶
            download_success = Doc2XService.download_file(download_url, docx_path)
            if not download_success:
                to_translate.error(trans['id'], "ä¸‹è½½è½¬æ¢åçš„DOCXæ–‡ä»¶å¤±è´¥")
                return False
            print(f"DOCXæ–‡ä»¶ä¸‹è½½æˆåŠŸ: {docx_path}")
        except Exception as e:
            to_translate.error(trans['id'], f"ä¸‹è½½DOCXæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

        # ç¡®è®¤æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(docx_path):
            to_translate.error(trans['id'], "è½¬æ¢åçš„DOCXæ–‡ä»¶ä¸å­˜åœ¨")
            return False

        # 4.5. æ£€æŸ¥è½¬æ¢åçš„DOCXæ–‡ä»¶è´¨é‡
        print(f"å¼€å§‹æ£€æŸ¥è½¬æ¢åçš„DOCXæ–‡ä»¶è´¨é‡...")
        if not check_docx_quality(docx_path):
            print("âš ï¸  DOCXæ–‡ä»¶è´¨é‡æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œä½†ç»§ç»­å¤„ç†...")
        else:
            print("âœ… DOCXæ–‡ä»¶è´¨é‡æ£€æŸ¥é€šè¿‡")

        # 5. ä½¿ç”¨Wordç¿»è¯‘é€»è¾‘å¤„ç†DOCXæ–‡ä»¶
        # åˆ›å»ºä¸€ä¸ªæ–°çš„transå¯¹è±¡ï¼ŒåŒ…å«DOCXæ–‡ä»¶è·¯å¾„
        docx_trans = trans.copy()
        docx_trans['file_path'] = docx_path

        # è®¾ç½®ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ˜¯PDFï¼Œæ·»åŠ .docxæ‰©å±•åï¼‰
        target_file = trans['target_file']
        if target_file.lower().endswith('.pdf'):
            target_file = target_file + '.docx'
        docx_trans['target_file'] = target_file

        # åŠ è½½Wordæ–‡æ¡£
        try:
            document = Document(docx_path)
            print(f"æˆåŠŸåŠ è½½DOCXæ–‡æ¡£: {docx_path}")
        except Exception as e:
            to_translate.error(trans['id'], f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
            return False

        # æå–éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
        texts = []
        extract_content_for_translation(document, docx_path, texts)
        print(f"ä»DOCXæå–äº† {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

        # è¿‡æ»¤æ‰ç‰¹æ®Šç¬¦å·å’Œçº¯æ•°å­—
        filtered_texts = []
        skipped_count = 0
        for i, item in enumerate(texts):
            if should_translate(item['text']):
                filtered_texts.append(item)
            else:
                # å¯¹äºä¸éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼Œæ ‡è®°ä¸ºå·²å®Œæˆ
                item['complete'] = True
                text = item['text']
                reason = get_skip_reason(text)
                skipped_count += 1
                
                # é™åˆ¶æ—¥å¿—é•¿åº¦ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                if skipped_count <= 50:  # åªæ˜¾ç¤ºå‰50ä¸ªè·³è¿‡çš„é¡¹ç›®
                    display_text = text[:50] + "..." if len(text) > 50 else text
                    print(f"è·³è¿‡ç¿»è¯‘: '{display_text}' - åŸå› : {reason}")
                elif skipped_count == 51:
                    print(f"... è¿˜æœ‰æ›´å¤šè·³è¿‡çš„é¡¹ç›®ï¼Œä¸å†æ˜¾ç¤ºè¯¦ç»†åŸå›  ...")

        print(f"è¿‡æ»¤åéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ç‰‡æ®µ: {len(filtered_texts)}")
        print(f"è·³è¿‡çš„æ–‡æœ¬ç‰‡æ®µ: {skipped_count}")

        # å¤šçº¿ç¨‹ç¿»è¯‘
        run_translation(docx_trans, filtered_texts, max_threads=10)

        # å†™å…¥ç¿»è¯‘ç»“æœï¼ˆå®Œå…¨ä¿ç•™åŸå§‹æ ¼å¼ï¼‰
        text_count = apply_translations(document, texts)
        print(f"åº”ç”¨äº† {text_count} ä¸ªç¿»è¯‘ç»“æœ")

        # ä¿å­˜æ–‡æ¡£
        try:
            document.save(target_file)
            print(f"ç¿»è¯‘åçš„æ–‡æ¡£ä¿å­˜æˆåŠŸ: {target_file}")
        except Exception as e:
            to_translate.error(trans['id'], f"ä¿å­˜æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False

        # å¤„ç†æ‰¹æ³¨ç­‰ç‰¹æ®Šå…ƒç´ 
        update_special_elements(target_file, texts)

        # 7. å®Œæˆå¤„ç†
        end_time = datetime.datetime.now()
        spend_time = common.display_spend(start_time, end_time)
        if trans['run_complete']:
            to_translate.complete(trans, text_count, spend_time)
        print(f"PDFç¿»è¯‘ä»»åŠ¡å®Œæˆ: {trans['id']}")
        return True

    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print(f"âŒ PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: {str(e)}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        # ç¡®ä¿é”™è¯¯çŠ¶æ€è¢«æ­£ç¡®è®°å½•
        to_translate.error(trans['id'], f"PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return False


def check_image(run):
    """æ£€æŸ¥runæ˜¯å¦åŒ…å«å›¾ç‰‡"""
    try:
        # æ£€æŸ¥runä¸­æ˜¯å¦æœ‰å›¾ç‰‡å…ƒç´ 
        for element in run._element:
            if element.tag.endswith('drawing') or element.tag.endswith('picture'):
                return True
        return False
    except:
        return False


def are_runs_compatible(run1, run2):
    """æ£€æŸ¥ä¸¤ä¸ªrunæ˜¯å¦å…¼å®¹ï¼ˆæœ€ä¸¥æ ¼çš„å…¼å®¹æ€§æ£€æŸ¥ï¼‰"""
    try:
        # 1. å­—ä½“åç§°å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.name != run2.font.name:
            return False
        
        # 2. å­—ä½“å¤§å°å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.size != run2.font.size:
            return False
        
        # 3. ç²—ä½“çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.bold != run2.font.bold:
            return False
        
        # 4. æ–œä½“çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.italic != run2.font.italic:
            return False
        
        # 5. ä¸‹åˆ’çº¿çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.underline != run2.font.underline:
            return False
        
        # 6. åˆ é™¤çº¿çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.strike != run2.font.strike:
            return False
        
        # 7. å­—ä½“é¢œè‰²å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.color.rgb != run2.font.color.rgb:
            return False
        
        # 8. èƒŒæ™¯è‰²å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.highlight_color != run2.font.highlight_color:
            return False
        
        # 9. ä¸Šæ ‡/ä¸‹æ ‡çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.superscript != run2.font.superscript:
            return False
        
        # 10. å°å‹å¤§å†™å­—æ¯çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.small_caps != run2.font.small_caps:
            return False
        
        return True
    except:
        # å¦‚æœä»»ä½•æ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤ä¸å…¼å®¹
        return False


def extract_paragraph_with_merge(paragraph, texts, context_type):
    """æå–æ®µè½å†…å®¹ï¼Œä½¿ç”¨ä¿å®ˆrunåˆå¹¶ï¼ˆä¸æ·»åŠ ä¸Šä¸‹æ–‡ï¼‰"""
    paragraph_runs = list(paragraph.runs)
    
    # ä½¿ç”¨ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥
    merged_runs = conservative_run_merge(paragraph_runs)
    
    for merged_item in merged_runs:
        if not check_text(merged_item['text']):
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
        has_image = False
        for run in merged_item['runs']:
            if check_image(run):
                has_image = True
                break
        
        if has_image:
            continue
        
        texts.append({
            "text": merged_item['text'],
            "type": "merged_run",
            "merged_item": merged_item,
            "complete": False,
            "context_type": context_type  # æ ‡è®°ç±»å‹
        })


def conservative_run_merge(paragraph_runs, max_merge_length=500):
    """æœ€ä¸¥æ ¼çš„runåˆå¹¶ç­–ç•¥"""
    
    merged = []
    current_group = []
    current_length = 0
    original_count = len([r for r in paragraph_runs if check_text(r.text)])
    merged_count = 0
    
    for run in paragraph_runs:
        # è·³è¿‡åŒ…å«å›¾ç‰‡çš„run
        if check_image(run):
            # ä¿å­˜å½“å‰ç»„
            if current_group:
                merged.append(merge_compatible_runs(current_group))
                if len(current_group) > 1:
                    merged_count += 1
                current_group = []
                current_length = 0
            
            # å›¾ç‰‡runå•ç‹¬å¤„ç†ï¼Œä½†ä¸æ·»åŠ åˆ°ç¿»è¯‘åˆ—è¡¨
            continue
        
        if not check_text(run.text):
            continue
        
        # æœ€ä¸¥æ ¼æ¡ä»¶ï¼šåªåˆå¹¶æçŸ­çš„runï¼ˆé€šå¸¸æ˜¯ç©ºæ ¼ã€æ ‡ç‚¹ã€å•ä¸ªå­—ç¬¦ï¼‰
        if len(run.text) <= 5 and current_length < max_merge_length:
            # æ£€æŸ¥æ ¼å¼å…¼å®¹æ€§ï¼ˆæœ€ä¸¥æ ¼æ£€æŸ¥ï¼‰
            if not current_group or are_runs_compatible(current_group[-1], run):
                # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿åˆå¹¶åçš„æ–‡æœ¬ä¸ä¼šè¿‡é•¿
                if current_length + len(run.text) <= 10:
                    current_group.append(run)
                    current_length += len(run.text)
                    continue
        
        # ä¿å­˜å½“å‰ç»„
        if current_group:
            merged.append(merge_compatible_runs(current_group))
            if len(current_group) > 1:
                merged_count += 1
            current_group = []
            current_length = 0
        
        # å½“å‰runå•ç‹¬å¤„ç†
        merged.append({
            'text': run.text,
            'runs': [run],
            'type': 'single'
        })
    
    if current_group:
        merged.append(merge_compatible_runs(current_group))
        if len(current_group) > 1:
            merged_count += 1
    
    # æ‰“å°åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
    if merged_count > 0:
        print(f"Runåˆå¹¶ä¼˜åŒ–ï¼ˆæœ€ä¸¥æ ¼ç­–ç•¥ï¼‰: åŸå§‹{original_count}ä¸ªrun -> åˆå¹¶å{len(merged)}ä¸ªï¼Œå‡å°‘äº†{merged_count}ä¸ªAPIè°ƒç”¨")
    else:
        print(f"Runåˆå¹¶ï¼ˆæœ€ä¸¥æ ¼ç­–ç•¥ï¼‰: æœªæ‰¾åˆ°å¯åˆå¹¶çš„runï¼Œä¿æŒåŸå§‹{original_count}ä¸ªrun")
    
    return merged


def merge_compatible_runs(run_group):
    """åˆå¹¶å…¼å®¹çš„runç»„ï¼ˆæœ€ä¸¥æ ¼ç­–ç•¥ï¼‰"""
    
    # åˆå¹¶æ–‡æœ¬
    merged_text = "".join(run.text for run in run_group)
    
    # é¢å¤–å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åˆå¹¶åçš„æ–‡æœ¬ä¸ä¼šè¿‡é•¿
    if len(merged_text) > 20:
        print(f"âš ï¸  è­¦å‘Š: åˆå¹¶åæ–‡æœ¬è¿‡é•¿({len(merged_text)}å­—ç¬¦)ï¼Œå–æ¶ˆåˆå¹¶")
        # å¦‚æœåˆå¹¶åè¿‡é•¿ï¼Œè¿”å›å•ä¸ªrun
        return {
            'text': run_group[0].text,
            'runs': [run_group[0]],
            'type': 'single'
        }
    
    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰runçš„æ ¼å¼å®Œå…¨ä¸€è‡´
    first_run = run_group[0]
    for run in run_group[1:]:
        if not are_runs_compatible(first_run, run):
            print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°æ ¼å¼ä¸ä¸€è‡´ï¼Œå–æ¶ˆåˆå¹¶")
            return {
                'text': run_group[0].text,
                'runs': [run_group[0]],
                'type': 'single'
            }
    
    return {
        'text': merged_text,
        'runs': run_group,
        'type': 'merged'
    }


def get_skip_reason(text):
    """è·å–æ–‡æœ¬è¢«è·³è¿‡çš„åŸå› """
    if not text or len(text) == 0:
        return "ç©ºæ–‡æœ¬"
    
    if len(text.strip()) <= 2:
        text_stripped = text.strip()
        
        # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦
        if all('\u4e00' <= char <= '\u9fff' for char in text_stripped):
            return f"çŸ­ä¸­æ–‡å­—ç¬¦'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        # æ£€æŸ¥æ•°å­—
        if text_stripped.isdigit():
            return f"çŸ­æ•°å­—'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        # æ£€æŸ¥å•ä½ç¬¦å·
        if all(char in 'â€¢â„ƒVAhH' for char in text_stripped):
            return f"å¸¸ç”¨å•ä½ç¬¦å·'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        # æ£€æŸ¥ä¸­è‹±æ–‡æ··åˆ
        if len(text_stripped) == 2:
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text_stripped)
            if has_chinese:
                return f"ä¸­è‹±æ–‡æ··åˆçŸ­æ–‡æœ¬'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        return f"çŸ­æ–‡æœ¬'{text_stripped}'ï¼ˆæ— æ„ä¹‰ï¼‰"
    
    if SPECIAL_SYMBOLS_PATTERN.match(text):
        return "çº¯ç‰¹æ®Šç¬¦å·"
    
    if NUMBERS_PATTERN.match(text):
        return "çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹"
    
    if common.is_all_punc(text):
        return "çº¯æ ‡ç‚¹ç¬¦å·"
    
    return "å…¶ä»–åŸå› "


def should_translate(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼ˆæ”¹è¿›çš„è¿‡æ»¤é€»è¾‘ï¼‰"""
    if not text or len(text) == 0:
        return False

    # å¯¹äºçŸ­æ–‡æœ¬ï¼ˆâ‰¤2å­—ç¬¦ï¼‰ï¼Œè¿›è¡Œç‰¹æ®Šå¤„ç†
    if len(text.strip()) <= 2:
        text_stripped = text.strip()
        
        # å…è®¸çŸ­ä¸­æ–‡å­—ç¬¦ï¼ˆé€šå¸¸æ˜¯æœ‰æ„ä¹‰çš„ï¼‰
        if all('\u4e00' <= char <= '\u9fff' for char in text_stripped):
            return True
        
        # å…è®¸çŸ­æ•°å­—ç»„åˆ
        if text_stripped.isdigit():
            return True
        
        # å…è®¸å¸¸ç”¨å•ä½ç¬¦å·ç»„åˆ
        if all(char in 'â€¢â„ƒVAhH' for char in text_stripped):
            return True
        
        # å…è®¸ä¸­è‹±æ–‡æ··åˆçš„çŸ­æ–‡æœ¬ï¼ˆå¦‚"å›¾1"ã€"è¡¨2"ï¼‰
        if len(text_stripped) == 2:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text_stripped)
            if has_chinese:
                return True
        
        # è¿‡æ»¤å…¶ä»–çŸ­æ–‡æœ¬
        return False

    # è¿‡æ»¤çº¯ç‰¹æ®Šç¬¦å·
    if SPECIAL_SYMBOLS_PATTERN.match(text):
        return False

    # è¿‡æ»¤çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹
    if NUMBERS_PATTERN.match(text):
        return False

    # è¿‡æ»¤å…¨æ˜¯æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬
    if common.is_all_punc(text):
        return False

    return True


def extract_content_for_translation(document, file_path, texts):
    """æå–éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼Œä½¿ç”¨runåˆå¹¶ä¼˜åŒ–"""
    # æ­£æ–‡æ®µè½
    for paragraph in document.paragraphs:
        extract_paragraph_with_merge(paragraph, texts, "paragraph")
        
        # å¤„ç†è¶…é“¾æ¥ï¼ˆå•ç‹¬å¤„ç†ï¼Œé¿å…ä¸æ™®é€šrunæ··åˆï¼‰
        for hyperlink in paragraph.hyperlinks:
            for run in hyperlink.runs:
                if check_text(run.text):
                    texts.append({
                        "text": run.text,
                        "type": "run",
                        "run": run,
                        "complete": False
                    })

    # è¡¨æ ¼å†…å®¹
    for table in document.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    extract_paragraph_with_merge(paragraph, texts, "table_cell")
                    
                    # å¤„ç†è¡¨æ ¼ä¸­çš„è¶…é“¾æ¥
                    for hyperlink in paragraph.hyperlinks:
                        for run in hyperlink.runs:
                            if check_text(run.text):
                                texts.append({
                                    "text": run.text,
                                    "type": "run",
                                    "run": run,
                                    "complete": False
                                })

    # é¡µçœ‰é¡µè„š
    for section in document.sections:
        # å¤„ç†é¡µçœ‰
        for paragraph in section.header.paragraphs:
            extract_paragraph_with_merge(paragraph, texts, "header")
            
            # å¤„ç†é¡µçœ‰ä¸­çš„è¶…é“¾æ¥
            for hyperlink in paragraph.hyperlinks:
                for run in hyperlink.runs:
                    if check_text(run.text):
                        texts.append({
                            "text": run.text,
                            "type": "run",
                            "run": run,
                            "complete": False
                        })

        # å¤„ç†é¡µè„š
        for paragraph in section.footer.paragraphs:
            extract_paragraph_with_merge(paragraph, texts, "footer")
            
            # å¤„ç†é¡µè„šä¸­çš„è¶…é“¾æ¥
            for hyperlink in paragraph.hyperlinks:
                for run in hyperlink.runs:
                    if check_text(run.text):
                        texts.append({
                            "text": run.text,
                            "type": "run",
                            "run": run,
                            "complete": False
                        })

    # æ‰¹æ³¨å†…å®¹
    extract_comments(file_path, texts)


def extract_comments(file_path, texts):
    """æå–æ–‡æ¡£ä¸­çš„æ‰¹æ³¨"""
    try:
        with zipfile.ZipFile(file_path, 'r') as docx:
            if 'word/comments.xml' in docx.namelist():
                with docx.open('word/comments.xml') as f:
                    tree = ET.parse(f)
                    root = tree.getroot()

                    # å®šä¹‰å‘½åç©ºé—´
                    namespaces = {
                        'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                    }

                    # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ³¨
                    for comment in root.findall('.//w:comment', namespaces) or root.findall(
                            './/{*}comment'):
                        # å°è¯•ä¸åŒæ–¹å¼è·å–ID
                        comment_id = None
                        for attr_name in [
                            '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id',
                            'id']:
                            if attr_name in comment.attrib:
                                comment_id = comment.attrib[attr_name]
                                break

                        if not comment_id:
                            continue

                        # æ”¶é›†æ‰¹æ³¨ä¸­çš„æ‰€æœ‰æ–‡æœ¬
                        text_elements = comment.findall('.//w:t', namespaces) or comment.findall(
                            './/{*}t')
                        for t_elem in text_elements:
                            if t_elem.text and check_text(t_elem.text):
                                texts.append({
                                    "text": t_elem.text,
                                    "type": "comment",
                                    "comment_id": comment_id,
                                    "complete": False,
                                    "original_text": t_elem.text  # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºåŒ¹é…
                                })
    except Exception as e:
        print(f"æå–æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")


def run_translation(trans, texts, max_threads=10):
    """æ‰§è¡Œå¤šçº¿ç¨‹ç¿»è¯‘"""
    if not texts:
        print("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„å†…å®¹")
        return

    event = threading.Event()
    run_index = 0
    active_count = threading.activeCount()

    print(f"å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

    while run_index < len(texts):
        if threading.activeCount() < max_threads + active_count and not event.is_set():
            thread = threading.Thread(
                target=to_translate.get,
                args=(trans, event, texts, run_index)
            )
            thread.start()
            print(f"å¯åŠ¨ç¿»è¯‘çº¿ç¨‹ {run_index}")
            run_index += 1
        time.sleep(0.1)

    # ç­‰å¾…ç¿»è¯‘å®Œæˆ
    while not all(t.get('complete') for t in texts) and not event.is_set():
        time.sleep(1)

    print("æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡å·²å®Œæˆ")


def apply_translations(document, texts):
    """åº”ç”¨ç¿»è¯‘ç»“æœåˆ°æ–‡æ¡£ï¼Œå®Œå…¨ä¿ç•™åŸå§‹æ ¼å¼"""
    text_count = 0

    for item in texts:
        if not item.get('complete', False):
            continue

        if item['type'] == 'run':
            # ç›´æ¥æ›¿æ¢runæ–‡æœ¬ï¼Œä¿ç•™æ‰€æœ‰æ ¼å¼
            item['run'].text = item.get('text', item['run'].text)
            text_count += 1
        elif item['type'] == 'merged_run':
            # å¤„ç†åˆå¹¶çš„runï¼Œéœ€è¦å°†ç¿»è¯‘ç»“æœåˆ†é…å›åŸå§‹run
            merged_item = item['merged_item']
            translated_text = item.get('text', merged_item['text'])
            
            if merged_item['type'] == 'merged':
                # åˆå¹¶çš„runç»„ï¼Œéœ€è¦æ™ºèƒ½åˆ†é…ç¿»è¯‘ç»“æœ
                distribute_translation_to_runs(merged_item['runs'], translated_text)
            else:
                # å•ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
                merged_item['runs'][0].text = translated_text
            
            text_count += 1

    return text_count


def distribute_translation_to_runs(runs, translated_text):
    """å°†ç¿»è¯‘ç»“æœæ™ºèƒ½åˆ†é…å›åŸå§‹runï¼Œä¿æŒæ ¼å¼"""
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
    if len(runs) == 1:
        runs[0].text = translated_text
        return
    
    # è®¡ç®—åŸå§‹æ–‡æœ¬çš„æ€»é•¿åº¦
    original_total_length = sum(len(run.text) for run in runs)
    
    # å¦‚æœç¿»è¯‘åæ–‡æœ¬é•¿åº¦å˜åŒ–ä¸å¤§ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
    if abs(len(translated_text) - original_total_length) <= 2:
        # æŒ‰åŸå§‹æ¯”ä¾‹åˆ†é…
        current_pos = 0
        for run in runs:
            original_ratio = len(run.text) / original_total_length
            target_length = int(len(translated_text) * original_ratio)
            
            if current_pos < len(translated_text):
                end_pos = min(current_pos + target_length, len(translated_text))
                run.text = translated_text[current_pos:end_pos]
                current_pos = end_pos
            else:
                run.text = ""
    else:
        # é•¿åº¦å˜åŒ–è¾ƒå¤§ï¼Œå°è¯•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
        words = translated_text.split()
        if len(words) >= len(runs):
            # æŒ‰runæ•°é‡åˆ†é…å•è¯
            for i, run in enumerate(runs):
                if i < len(words):
                    run.text = words[i]
                else:
                    run.text = ""
        else:
            # å•è¯æ•°é‡å°‘äºrunæ•°é‡ï¼Œå¹³å‡åˆ†é…
            chars_per_run = len(translated_text) // len(runs)
            for i, run in enumerate(runs):
                start_pos = i * chars_per_run
                end_pos = start_pos + chars_per_run if i < len(runs) - 1 else len(translated_text)
                run.text = translated_text[start_pos:end_pos]


def update_special_elements(docx_path, texts):
    """æ›´æ–°æ‰¹æ³¨ç­‰ç‰¹æ®Šå…ƒç´ """
    # ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„æ‰¹æ³¨
    comment_texts = [t for t in texts if t.get('type') == 'comment' and t.get('complete')]
    if not comment_texts:
        return  # æ²¡æœ‰éœ€è¦å¤„ç†çš„æ‰¹æ³¨

    temp_path = f"temp_{int(time.time())}_{os.path.basename(docx_path)}"

    try:
        with zipfile.ZipFile(docx_path, 'r') as zin, \
                zipfile.ZipFile(temp_path, 'w') as zout:

            for item in zin.infolist():
                with zin.open(item) as file:
                    if item.filename == 'word/comments.xml':
                        try:
                            tree = ET.parse(file)
                            root = tree.getroot()

                            # å®šä¹‰å‘½åç©ºé—´
                            namespaces = {
                                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                            }

                            # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ³¨
                            for comment in root.findall('.//w:comment', namespaces) or root.findall(
                                    './/{*}comment'):
                                # å°è¯•ä¸åŒæ–¹å¼è·å–ID
                                comment_id = None
                                for attr_name in [
                                    '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id',
                                    'id']:
                                    if attr_name in comment.attrib:
                                        comment_id = comment.attrib[attr_name]
                                        break

                                if not comment_id:
                                    continue

                                # æŸ¥æ‰¾åŒ¹é…çš„ç¿»è¯‘ç»“æœ
                                matching_texts = [t for t in comment_texts if
                                                  t.get('comment_id') == comment_id]
                                if not matching_texts:
                                    continue

                                # æ›´æ–°æ‰¹æ³¨æ–‡æœ¬
                                text_elements = comment.findall('.//w:t',
                                                                namespaces) or comment.findall(
                                    './/{*}t')
                                for i, t_elem in enumerate(text_elements):
                                    if i < len(matching_texts):
                                        # æ‰¾åˆ°åŒ¹é…çš„åŸå§‹æ–‡æœ¬
                                        for match in matching_texts:
                                            if match.get('original_text') == t_elem.text:
                                                t_elem.text = match.get('text', t_elem.text)
                                                break

                            # å†™å…¥ä¿®æ”¹åçš„XML
                            modified_xml = ET.tostring(root, encoding='utf-8', xml_declaration=True)
                            zout.writestr(item.filename, modified_xml)
                        except Exception as e:
                            print(f"å¤„ç†æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")
                            # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥å¤åˆ¶åŸæ–‡ä»¶
                            file.seek(0)
                            zout.writestr(item.filename, file.read())
                    else:
                        # ç›´æ¥å¤åˆ¶å…¶ä»–æ–‡ä»¶
                        file.seek(0)
                        zout.writestr(item.filename, file.read())

        # æ›¿æ¢åŸå§‹æ–‡ä»¶
        os.replace(temp_path, docx_path)
    except Exception as e:
        print(f"æ›´æ–°æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass


def check_text(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”éçº¯æ ‡ç‚¹ï¼‰"""
    return text and len(text) > 0 and not common.is_all_punc(text)
