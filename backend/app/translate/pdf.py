# -*- coding: utf-8 -*-
import os
import time
import datetime
import traceback
from pathlib import Path
import threading
from docx import Document
from docx.oxml.ns import qn
from . import to_translate
from . import common
import zipfile
import xml.etree.ElementTree as ET
from threading import Lock
import re
import shutil
import fitz
import json
import logging

from ..utils.doc2x import Doc2XService

# çº¿ç¨‹å®‰å…¨æ‰“å°é”
print_lock = Lock()

# ç‰¹æ®Šç¬¦å·å’Œæ•°å­¦ç¬¦å·çš„æ­£åˆ™è¡¨è¾¾å¼
SPECIAL_SYMBOLS_PATTERN = re.compile(
    r'^[â˜…â˜†â™¥â™¦â™£â™ â™€â™‚â˜¼â˜¾â˜½â™ªâ™«â™¬â˜‘â˜’âœ“âœ”âœ•âœ–âœ—âœ˜âŠ•âŠ—âˆâˆ‘âˆÏ€Î Â±Ã—Ã·âˆšâˆ›âˆœâˆ«âˆ®âˆ‡âˆ‚âˆ†âˆâˆ‘âˆšâˆâˆâˆŸâˆ âˆ¡âˆ¢âˆ£âˆ¤âˆ¥âˆ¦âˆ§âˆ¨âˆ©âˆªâˆ«âˆ¬âˆ­âˆ®âˆ¯âˆ°âˆ±âˆ²âˆ³âˆ´âˆµâˆ¶âˆ·âˆ¸âˆ¹âˆºâˆ»âˆ¼âˆ½âˆ¾âˆ¿â‰€â‰â‰‚â‰ƒâ‰„â‰…â‰†â‰‡â‰ˆâ‰‰â‰Šâ‰‹â‰Œâ‰â‰â‰â‰â‰‘â‰’â‰“â‰”â‰•â‰–â‰—â‰˜â‰™â‰šâ‰›â‰œâ‰â‰â‰Ÿâ‰ â‰¡â‰¢â‰£â‰¤â‰¥â‰¦â‰§â‰¨â‰©â‰ªâ‰«â‰¬â‰­â‰®â‰¯â‰°â‰±â‰²â‰³â‰´â‰µâ‰¶â‰·â‰¸â‰¹â‰ºâ‰»â‰¼â‰½â‰¾â‰¿âŠ€âŠâŠ‚âŠƒâŠ„âŠ…âŠ†âŠ‡âŠˆâŠ‰âŠŠâŠ‹âŠŒâŠâŠâŠâŠâŠ‘âŠ’âŠ“âŠ”âŠ•âŠ–âŠ—âŠ˜âŠ™âŠšâŠ›âŠœâŠâŠâŠŸâŠ âŠ¡âŠ¢âŠ£âŠ¤âŠ¥âŠ¦âŠ§âŠ¨âŠ©âŠªâŠ«âŠ¬âŠ­âŠ®âŠ¯âŠ°âŠ±âŠ²âŠ³âŠ´âŠµâŠ¶âŠ·âŠ¸âŠ¹âŠºâŠ»âŠ¼âŠ½âŠ¾âŠ¿â‹€â‹â‹‚â‹ƒâ‹„â‹…â‹†â‹‡â‹ˆâ‹‰â‹Šâ‹‹â‹Œâ‹â‹â‹â‹â‹‘â‹’â‹“â‹”â‹•â‹–â‹—â‹˜â‹™â‹šâ‹›â‹œâ‹â‹â‹Ÿâ‹ â‹¡â‹¢â‹£â‹¤â‹¥â‹¦â‹§â‹¨â‹©â‹ªâ‹«â‹¬â‹­â‹®â‹¯â‹°â‹±â‹²â‹³â‹´â‹µâ‹¶â‹·â‹¸â‹¹â‹ºâ‹»â‹¼â‹½â‹¾â‹¿]+$')

# çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹çš„æ­£åˆ™è¡¨è¾¾å¼
NUMBERS_PATTERN = re.compile(r'^[\d\s\.,\-\+\*\/\(\)\[\]\{\}]+$')

def check_docx_quality(docx_path):
    """æ£€æŸ¥è½¬æ¢åçš„DOCXæ–‡ä»¶è´¨é‡ï¼Œåˆ†æç¼–ç å’Œæ–‡æœ¬å†…å®¹"""
    try:
        print("\n=== å¼€å§‹DOCXæ–‡ä»¶è´¨é‡æ£€æŸ¥ ===")
        print("æ–‡ä»¶è·¯å¾„: " + docx_path)
        
        # æ£€æŸ¥æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        file_size = os.path.getsize(docx_path)
        print("æ–‡ä»¶å¤§å°: " + str(file_size) + " å­—èŠ‚")
        
        if file_size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
            print("âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤§å°å¼‚å¸¸ï¼Œå¯èƒ½è½¬æ¢å¤±è´¥")
            return False
        
        # å°è¯•åŠ è½½æ–‡æ¡£
        try:
            document = Document(docx_path)
            print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
            return False
        
        # åˆ†ææ–‡æ¡£ç»“æ„
        paragraph_count = len(document.paragraphs)
        table_count = len(document.tables)
        section_count = len(document.sections)
        
        print(f"æ–‡æ¡£ç»“æ„: {paragraph_count} ä¸ªæ®µè½, {table_count} ä¸ªè¡¨æ ¼, {section_count} ä¸ªèŠ‚")
        
        # åˆ†æå‰å‡ ä¸ªæ®µè½çš„æ–‡æœ¬å†…å®¹
        print(f"\n--- å‰5ä¸ªæ®µè½å†…å®¹åˆ†æ ---")
        chinese_chars = 0
        total_chars = 0
        sample_texts = []
        
        for i, para in enumerate(document.paragraphs[:5]):
            if para.text.strip():
                text = para.text.strip()
                sample_texts.append(text)
                
                # ç»Ÿè®¡å­—ç¬¦
                para_chinese = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
                para_total = len(text)
                chinese_chars += para_chinese
                total_chars += para_total
                
                # æ˜¾ç¤ºæ®µè½å†…å®¹ï¼ˆé™åˆ¶é•¿åº¦é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                display_text = text[:100] + "..." if len(text) > 100 else text
                print(f"æ®µè½{i+1}: '{display_text}'")
                print(f"  é•¿åº¦: {para_total}, ä¸­æ–‡å­—ç¬¦: {para_chinese}")
                
                # æ˜¾ç¤ºç¼–ç ä¿¡æ¯
                try:
                    encoded = text.encode('utf-8')
                    print(f"  UTF-8ç¼–ç : {encoded}")
                except Exception as e:
                    print(f"  ç¼–ç æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # åˆ†æè¡¨æ ¼å†…å®¹
        if table_count > 0:
            print(f"\n--- è¡¨æ ¼å†…å®¹åˆ†æ ---")
            table_chinese = 0
            table_total = 0
            
            for i, table in enumerate(document.tables[:2]):  # åªåˆ†æå‰2ä¸ªè¡¨æ ¼
                print(f"è¡¨æ ¼{i+1}:")
                for row_idx, row in enumerate(table.rows[:3]):  # åªåˆ†æå‰3è¡Œ
                    for col_idx, cell in enumerate(row.cells[:3]):  # åªåˆ†æå‰3åˆ—
                        if cell.text.strip():
                            text = cell.text.strip()
                            cell_chinese = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
                            cell_total = len(text)
                            table_chinese += cell_chinese
                            table_total += cell_total
                            
                            if cell_total > 0:
                                display_text = text[:50] + "..." if len(text) > 50 else text
                                print(f"  å•å…ƒæ ¼[{row_idx+1},{col_idx+1}]: '{display_text}' (ä¸­æ–‡å­—ç¬¦: {cell_chinese})")
            
            chinese_chars += table_chinese
            total_chars += table_total
            print(f"è¡¨æ ¼æ€»è®¡: {table_chinese} ä¸ªä¸­æ–‡å­—ç¬¦, {table_total} ä¸ªæ€»å­—ç¬¦")
        
        # ç»Ÿè®¡ç»“æœ
        print(f"\n--- è´¨é‡æ£€æŸ¥ç»“æœ ---")
        if total_chars > 0:
            chinese_ratio = (chinese_chars / total_chars) * 100
            print(f"ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹: {chinese_chars}/{total_chars} ({chinese_ratio:.1f}%)")
            
            if chinese_ratio < 10:
                print("âš ï¸  è­¦å‘Š: ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜")
            elif chinese_ratio > 80:
                print("âœ… ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹æ­£å¸¸")
            else:
                print("â„¹ï¸  ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ä¸­ç­‰")
        else:
            print("âš ï¸  è­¦å‘Š: æœªå‘ç°ä»»ä½•æ–‡æœ¬å†…å®¹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„é—®é¢˜
        if paragraph_count == 0 and table_count == 0:
            print("âŒ ä¸¥é‡é—®é¢˜: æ–‡æ¡£æ²¡æœ‰ä»»ä½•å†…å®¹")
            return False
        
        if chinese_chars == 0 and total_chars > 0:
            print("âš ï¸  è­¦å‘Š: æœ‰æ–‡æœ¬å†…å®¹ä½†æ²¡æœ‰ä¸­æ–‡å­—ç¬¦ï¼Œå¯èƒ½å­˜åœ¨ç¼–ç é—®é¢˜")
        
        print(f"=== DOCXè´¨é‡æ£€æŸ¥å®Œæˆ ===\n")
        return True
        
    except Exception as e:
        print(f"âŒ DOCXè´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        traceback.print_exc()
        return False


def get_doc2x_save_dir():
    # è·å–åŸºç¡€å­˜å‚¨ç›®å½•
    base_dir = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))).parent.absolute()

    # åˆ›å»ºæ—¥æœŸå­ç›®å½•ï¼ˆYYYY-MM-DDï¼‰
    date_str = datetime.datetime.now().strftime('%Y-%m-%d')
    save_dir = base_dir / "storage" / "doc2x_results" / date_str

    save_dir.mkdir(parents=True, exist_ok=True)

    return str(save_dir)


def start(trans):
    """PDFç¿»è¯‘å…¥å£"""
    print("ğŸš¨ DEBUG: PDFç¿»è¯‘å‡½æ•°è¢«è°ƒç”¨ï¼")
    print("ğŸš¨ DEBUG: è¿™æ˜¯å¼ºåˆ¶è¾“å‡ºæµ‹è¯•")
    
    try:
        # å¼€å§‹æ—¶é—´
        start_time = datetime.datetime.now()
        print(f"=== å¼€å§‹PDFç¿»è¯‘ä»»åŠ¡ ===")
        print(f"ä»»åŠ¡ID: {trans['id']}")
        print(f"æºæ–‡ä»¶: {trans['file_path']}")
        print(f"ç›®æ ‡æ–‡ä»¶: {trans['target_file']}")
        print(f"å¼€å§‹æ—¶é—´: {start_time}")
        print("=" * 50)
        
        # æ£€æŸ¥PDFç¿»è¯‘æ–¹æ³•è®¾ç½®
        # ä¼˜å…ˆä½¿ç”¨transä¸­çš„pdf_translate_methodï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»ç³»ç»Ÿè®¾ç½®ä¸­è·å–
        pdf_translate_method = trans.get('pdf_translate_method')
        if not pdf_translate_method:
            pdf_translate_method = get_pdf_translate_method()
        print(f"ğŸ“‹ PDFç¿»è¯‘æ–¹æ³•: {pdf_translate_method}")
        
        # æ ¹æ®è®¾ç½®é€‰æ‹©ç¿»è¯‘æ–¹æ³•
        if pdf_translate_method == 'direct':
            print("ğŸ¯ ä½¿ç”¨ç›´æ¥PDFç¿»è¯‘æ–¹æ³•")
            return start_direct_pdf_translation(trans)
        else:
            print("ğŸ¯ ä½¿ç”¨Doc2xè½¬æ¢åç¿»è¯‘æ–¹æ³•")
            return start_doc2x_pdf_translation(trans)

    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print("âŒ PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: " + str(e))
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        # ç¡®ä¿é”™è¯¯çŠ¶æ€è¢«æ­£ç¡®è®°å½•
        to_translate.error(trans['id'], "PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: " + str(e))
        return False


def start_doc2x_pdf_translation(trans):
    """Doc2xè½¬æ¢åç¿»è¯‘æ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
    try:
        # å¼€å§‹æ—¶é—´
        start_time = datetime.datetime.now()
        print(f"=== å¼€å§‹Doc2x PDFç¿»è¯‘ä»»åŠ¡ ===")
        print(f"ä»»åŠ¡ID: {trans['id']}")
        print(f"æºæ–‡ä»¶: {trans['file_path']}")
        print(f"ç›®æ ‡æ–‡ä»¶: {trans['target_file']}")
        print(f"å¼€å§‹æ—¶é—´: {start_time}")
        print("=" * 50)
        
        # ç«‹å³æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º"changing"ï¼Œè®¾ç½®PDFè½¬æ¢åˆå§‹è¿›åº¦0%
        try:
            from .to_translate import db
            db.execute("update translate set status='changing', process='0' where id=%s", trans['id'])
            print("âœ… å·²æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºchangingï¼Œè¿›åº¦0%ï¼ˆå¼€å§‹PDFè½¬æ¢ï¼‰")
        except Exception as e:
            print(f"âš ï¸  æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        original_path = Path(trans['file_path'])
        print(f"æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨: {original_path}")
        if not original_path.exists():
            print(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {trans['file_path']}")
            to_translate.error(trans['id'], "æ–‡ä»¶ä¸å­˜åœ¨: " + trans['file_path'])
            return False
        print(f"âœ… æºæ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {os.path.getsize(original_path)} å­—èŠ‚")

        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target_dir = os.path.dirname(trans['target_file'])
        print(f"åˆ›å»ºç›®æ ‡ç›®å½•: {target_dir}")
        os.makedirs(target_dir, exist_ok=True)
        print(f"âœ… ç›®æ ‡ç›®å½•å·²å‡†å¤‡")

        # è·å–doc2xä¿å­˜ç›®å½•
        doc2x_save_dir = get_doc2x_save_dir()
        print(f"ğŸ“ doc2xä¿å­˜ç›®å½•: {doc2x_save_dir}")

        # è®¾ç½®è½¬æ¢åçš„Wordæ–‡ä»¶è·¯å¾„ï¼ˆä¿å­˜åœ¨doc2x_resultsç›®å½•ä¸‹ï¼‰
        docx_filename = f"{original_path.stem}_doc2x.docx"
        docx_path = os.path.join(doc2x_save_dir, docx_filename)
        print(f"ğŸ“„ è½¬æ¢åçš„DOCXæ–‡ä»¶è·¯å¾„: {docx_path}")

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if os.path.exists(docx_path):
            try:
                os.remove(docx_path)
                print(f"å·²åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {docx_path}")
            except Exception as e:
                print(f"åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶å¤±è´¥: {str(e)}")
                # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
                docx_filename = f"{original_path.stem}_doc2x_{int(time.time())}.docx"
                docx_path = os.path.join(doc2x_save_dir, docx_filename)

        print(f"è½¬æ¢åçš„DOCXæ–‡ä»¶å°†ä¿å­˜åœ¨: {docx_path}")

        # ä½¿ç”¨Doc2XæœåŠ¡å°†PDFè½¬æ¢ä¸ºDOCX
        print(f"å¼€å§‹å°†PDFè½¬æ¢ä¸ºDOCX: {original_path}")
        import logging
        logger = logging.getLogger(__name__)
        
        # è®°å½•Doc2Xå¼€å§‹æ—¶é—´
        doc2x_start_time = datetime.datetime.now()
        logger.info(f"ğŸš€ Doc2Xè½¬æ¢å¼€å§‹æ—¶é—´: {doc2x_start_time}")

        # è·å–APIå¯†é’¥
        api_key = trans.get('doc2x_api_key', '')
        print(f"ğŸ”‘ æ£€æŸ¥APIå¯†é’¥: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
        if not api_key:
            print(f"âŒ ç¼ºå°‘Doc2X APIå¯†é’¥")
            to_translate.error(trans['id'], "ç¼ºå°‘Doc2X APIå¯†é’¥")
            return False
        print(f"âœ… APIå¯†é’¥å·²è®¾ç½®")

        # 1. å¯åŠ¨è½¬æ¢ä»»åŠ¡
        print(f"ğŸš€ å¼€å§‹å¯åŠ¨Doc2Xè½¬æ¢ä»»åŠ¡...")
        task_start_time = datetime.datetime.now()
        logger.info(f"ğŸ“¤ Doc2Xä»»åŠ¡å¯åŠ¨æ—¶é—´: {task_start_time}")
        try:
            uid = Doc2XService.start_task(api_key, str(original_path))
            task_end_time = datetime.datetime.now()
            task_duration = task_end_time - task_start_time
            logger.info(f"ğŸ“¤ Doc2Xä»»åŠ¡å¯åŠ¨è€—æ—¶: {task_duration.total_seconds():.2f}ç§’")
            print(f"âœ… Doc2Xä»»åŠ¡å¯åŠ¨æˆåŠŸï¼ŒUID: {uid}")
            
            # Doc2Xä»»åŠ¡å¯åŠ¨æˆåŠŸï¼Œè®¾ç½®ä¸ºchangingçŠ¶æ€
            try:
                from .to_translate import db
                print(f"ğŸ” å‡†å¤‡æ›´æ–°ä»»åŠ¡çŠ¶æ€: ä»»åŠ¡ID={trans['id']}, æ–°çŠ¶æ€=changing, æ–°è¿›åº¦=0")
                result = db.execute("update translate set status='changing', process='0' where id=%s", trans['id'])
                print(f"ğŸ” æ•°æ®åº“æ›´æ–°ç»“æœ: {result}")
                print("âœ… å·²æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºchangingï¼Œè¿›åº¦0%ï¼ˆDoc2Xä»»åŠ¡å¯åŠ¨æˆåŠŸï¼‰")
            except Exception as e:
                print(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
                # å¦‚æœçŠ¶æ€æ›´æ–°å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­æµç¨‹
                import traceback
                traceback.print_exc()
                
        except Exception as e:
            print(f"âŒ å¯åŠ¨Doc2Xä»»åŠ¡å¤±è´¥: {str(e)}")
            to_translate.error(trans['id'], f"å¯åŠ¨Doc2Xä»»åŠ¡å¤±è´¥: {str(e)}")
            return False

        # 2. ç­‰å¾…è§£æå®Œæˆ
        max_retries = 60  # æœ€å¤šç­‰å¾…10åˆ†é’Ÿ
        retry_count = 0
        parse_start_time = datetime.datetime.now()
        logger.info(f"ğŸ”„ Doc2Xè§£æå¼€å§‹æ—¶é—´: {parse_start_time}")
        
        while retry_count < max_retries:
            try:
                status_info = Doc2XService.check_parse_status(api_key, uid)
                if status_info['status'] == 'success':
                    parse_end_time = datetime.datetime.now()
                    parse_duration = parse_end_time - parse_start_time
                    logger.info(f"ğŸ”„ Doc2Xè§£æå®Œæˆæ—¶é—´: {parse_end_time}")
                    logger.info(f"â±ï¸  Doc2Xè§£ææ€»è€—æ—¶: {parse_duration.total_seconds():.2f}ç§’")
                    print(f"PDFè§£ææˆåŠŸ: {uid}")
                    
                    # Doc2Xè§£æå®Œæˆï¼Œä½†ä¿æŒchangingçŠ¶æ€ï¼Œç»§ç»­å¯¼å‡ºå’Œä¸‹è½½
                    print(f"PDFè§£ææˆåŠŸ: {uid}")
                    # ä¸åœ¨è¿™é‡Œåˆ‡æ¢çŠ¶æ€ï¼Œç­‰æ‰€æœ‰Doc2Xé˜¶æ®µå®Œæˆåå†åˆ‡æ¢
                    break
                elif status_info['status'] == 'failed':
                    to_translate.error(trans['id'],
                                       f"PDFè§£æå¤±è´¥: {status_info.get('detail', 'æœªçŸ¥é”™è¯¯')}")
                    return False

                # æ¨¡æ‹Ÿè¿›åº¦ï¼šåŸºäºé‡è¯•æ¬¡æ•°è®¡ç®—è¿›åº¦ï¼ˆ0-90%ï¼‰
                # å‰90%ç”¨äºè§£æé˜¶æ®µï¼Œæœ€å10%ç•™ç»™å¯¼å‡ºå’Œä¸‹è½½
                simulated_progress = min(90, int((retry_count / max_retries) * 90))
                print(f"ğŸ” æ¨¡æ‹Ÿè¿›åº¦: {simulated_progress}% (é‡è¯•æ¬¡æ•°: {retry_count}/{max_retries})")
                
                # å®æ—¶æ›´æ–°æ•°æ®åº“è¿›åº¦
                try:
                    print(f"ğŸ” å‡†å¤‡æ›´æ–°è¿›åº¦: ä»»åŠ¡ID={trans['id']}, æ–°è¿›åº¦={simulated_progress}%")
                    result = db.execute("update translate set process=%s where id=%s", str(simulated_progress), trans['id'])
                    print(f"ğŸ” è¿›åº¦æ›´æ–°ç»“æœ: {result}")
                    print(f"âœ… å·²æ›´æ–°è¿›åº¦ä¸º {simulated_progress}%")
                except Exception as e:
                    print(f"âŒ æ›´æ–°è¿›åº¦å¤±è´¥: {str(e)}")
                    import traceback
                    traceback.print_exc()

                # ç­‰å¾…10ç§’åå†æ¬¡æ£€æŸ¥
                time.sleep(10)
                retry_count += 1
            except Exception as e:
                to_translate.error(trans['id'], f"æ£€æŸ¥è§£æçŠ¶æ€å¤±è´¥: {str(e)}")
                return False

        if retry_count >= max_retries:
            to_translate.error(trans['id'], "PDFè§£æè¶…æ—¶")
            return False

        # 3. è§¦å‘å¯¼å‡º
        export_start_time = datetime.datetime.now()
        logger.info(f"ğŸ“¤ Doc2Xå¯¼å‡ºå¼€å§‹æ—¶é—´: {export_start_time}")
        
        # å¼€å§‹å¯¼å‡ºé˜¶æ®µï¼Œè¿›åº¦è®¾ä¸º95%ï¼ˆchangingçŠ¶æ€ä¸‹çš„è¿›åº¦ï¼‰
        print("ğŸ“¤ å¼€å§‹å¯¼å‡ºé˜¶æ®µ")
        try:
            from .to_translate import db
            db.execute("update translate set process='95' where id=%s", trans['id'])
            print("âœ… å·²æ›´æ–°è¿›åº¦ä¸º95%ï¼ˆå¼€å§‹å¯¼å‡ºï¼‰")
        except Exception as e:
            print("âš ï¸  æ›´æ–°è¿›åº¦å¤±è´¥: " + str(e))
            
        try:
            export_success = Doc2XService.trigger_export(api_key, uid, original_path.stem)
            if not export_success:
                to_translate.error(trans['id'], "è§¦å‘å¯¼å‡ºå¤±è´¥")
                return False
            export_end_time = datetime.datetime.now()
            export_duration = export_end_time - export_start_time
            logger.info(f"ğŸ“¤ Doc2Xå¯¼å‡ºè€—æ—¶: {export_duration.total_seconds():.2f}ç§’")
            print(f"å·²è§¦å‘å¯¼å‡º: {uid}")
        except Exception as e:
            to_translate.error(trans['id'], f"è§¦å‘å¯¼å‡ºå¤±è´¥: {str(e)}")
            return False

        # 4. ç­‰å¾…å¯¼å‡ºå®Œæˆå¹¶ä¸‹è½½
        download_start_time = datetime.datetime.now()
        logger.info(f"ğŸ“¥ Doc2Xä¸‹è½½å¼€å§‹æ—¶é—´: {download_start_time}")
        
        # å¼€å§‹ä¸‹è½½é˜¶æ®µï¼Œè¿›åº¦è®¾ä¸º98%ï¼ˆchangingçŠ¶æ€ä¸‹çš„è¿›åº¦ï¼‰
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½é˜¶æ®µ")
        try:
            db.execute("update translate set process='98' where id=%s", trans['id'])
            print("âœ… å·²æ›´æ–°è¿›åº¦ä¸º98%ï¼ˆå¼€å§‹ä¸‹è½½ï¼‰")
        except Exception as e:
            print("âš ï¸  æ›´æ–°è¿›åº¦å¤±è´¥: " + str(e))
            
        try:
            download_url = Doc2XService.check_export_status(api_key, uid)
            print(f"è·å–åˆ°ä¸‹è½½é“¾æ¥: {download_url}")

            # ä¸‹è½½æ–‡ä»¶
            download_success = Doc2XService.download_file(download_url, docx_path)
            if not download_success:
                to_translate.error(trans['id'], "ä¸‹è½½è½¬æ¢åçš„DOCXæ–‡ä»¶å¤±è´¥")
                return False
            download_end_time = datetime.datetime.now()
            download_duration = download_end_time - download_start_time
            logger.info(f"ğŸ“¥ Doc2Xä¸‹è½½è€—æ—¶: {download_duration.total_seconds():.2f}ç§’")
            print(f"DOCXæ–‡ä»¶ä¸‹è½½æˆåŠŸ: {docx_path}")
            
            # ä¸‹è½½å®Œæˆï¼Œè¿›åº¦è®¾ä¸º100%ï¼ˆchangingçŠ¶æ€ä¸‹çš„è¿›åº¦ï¼‰
            try:
                db.execute("update translate set process='100' where id=%s", trans['id'])
                print("âœ… å·²æ›´æ–°è¿›åº¦ä¸º100%ï¼ˆä¸‹è½½å®Œæˆï¼‰")
            except Exception as e:
                print("âš ï¸  æ›´æ–°è¿›åº¦å¤±è´¥: " + str(e))
            
            # Doc2Xæ‰€æœ‰é˜¶æ®µå®Œæˆï¼Œåˆ‡æ¢åˆ°processçŠ¶æ€å¼€å§‹ç¿»è¯‘
            try:
                print(f"ğŸ” å‡†å¤‡åˆ‡æ¢åˆ°processçŠ¶æ€: ä»»åŠ¡ID={trans['id']}, æ–°çŠ¶æ€=process, æ–°è¿›åº¦=0")
                result = db.execute("update translate set status='process', process='0' where id=%s", trans['id'])
                print(f"ğŸ” çŠ¶æ€åˆ‡æ¢ç»“æœ: {result}")
                print("âœ… å·²æ›´æ–°çŠ¶æ€ä¸ºprocessï¼Œè¿›åº¦0%ï¼ˆDoc2Xå…¨éƒ¨å®Œæˆï¼Œå¼€å§‹ç¿»è¯‘ï¼‰")
            except Exception as e:
                print(f"âŒ æ›´æ–°çŠ¶æ€å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                
        except Exception as e:
            to_translate.error(trans['id'], f"ä¸‹è½½DOCXæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False

        # ç¡®è®¤æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(docx_path):
            to_translate.error(trans['id'], "è½¬æ¢åçš„DOCXæ–‡ä»¶ä¸å­˜åœ¨")
            return False

        # 4.5. æ£€æŸ¥è½¬æ¢åçš„DOCXæ–‡ä»¶è´¨é‡
        print(f"å¼€å§‹æ£€æŸ¥è½¬æ¢åçš„DOCXæ–‡ä»¶è´¨é‡...")
        if not check_docx_quality(docx_path):
            print("âš ï¸  DOCXæ–‡ä»¶è´¨é‡æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œä½†ç»§ç»­å¤„ç†...")
        else:
            print("âœ… DOCXæ–‡ä»¶è´¨é‡æ£€æŸ¥é€šè¿‡")
            
        # è®°å½•Doc2Xå®Œæˆæ—¶é—´å¹¶è®¡ç®—è€—æ—¶
        doc2x_end_time = datetime.datetime.now()
        doc2x_duration = doc2x_end_time - doc2x_start_time
        logger.info(f"âœ… Doc2Xè½¬æ¢å®Œæˆæ—¶é—´: {doc2x_end_time}")
        logger.info(f"â±ï¸  Doc2Xè½¬æ¢æ€»è€—æ—¶: {doc2x_duration}")
        logger.info(f"ğŸ“Š Doc2Xè½¬æ¢è€—æ—¶è¯¦æƒ…:")
        logger.info(f"   - å¼€å§‹æ—¶é—´: {doc2x_start_time}")
        logger.info(f"   - å®Œæˆæ—¶é—´: {doc2x_end_time}")
        logger.info(f"   - æ€»è€—æ—¶: {doc2x_duration}")
        logger.info(f"   - è€—æ—¶ç§’æ•°: {doc2x_duration.total_seconds():.2f}ç§’")

        # 5. ä½¿ç”¨Wordç¿»è¯‘é€»è¾‘å¤„ç†DOCXæ–‡ä»¶
        # åˆ›å»ºä¸€ä¸ªæ–°çš„transå¯¹è±¡ï¼ŒåŒ…å«DOCXæ–‡ä»¶è·¯å¾„
        docx_trans = trans.copy()
        docx_trans['file_path'] = docx_path

        # è®¾ç½®ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ˜¯PDFï¼Œæ·»åŠ .docxæ‰©å±•åï¼‰
        target_file = trans['target_file']
        if target_file.lower().endswith('.pdf'):
            target_file = target_file + '.docx'
        docx_trans['target_file'] = target_file

        # åŠ è½½Wordæ–‡æ¡£
        try:
            document = Document(docx_path)
            print("æˆåŠŸåŠ è½½DOCXæ–‡æ¡£: " + docx_path)
        except Exception as e:
            to_translate.error(trans['id'], f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
            return False

        # æå–éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
        texts = []
        extract_content_for_translation(document, docx_path, texts)
        print("ä»DOCXæå–äº† " + str(len(texts)) + " ä¸ªæ–‡æœ¬ç‰‡æ®µ")

        # è¿‡æ»¤æ‰ç‰¹æ®Šç¬¦å·å’Œçº¯æ•°å­—
        filtered_texts = []
        skipped_count = 0
        for i, item in enumerate(texts):
            if should_translate(item['text']):
                filtered_texts.append(item)
            else:
                # å¯¹äºä¸éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼Œæ ‡è®°ä¸ºå·²å®Œæˆ
                item['complete'] = True
                text = item['text']
                reason = get_skip_reason(text)
                skipped_count += 1
                
                # é™åˆ¶æ—¥å¿—é•¿åº¦ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
                if skipped_count <= 50:  # åªæ˜¾ç¤ºå‰50ä¸ªè·³è¿‡çš„é¡¹ç›®
                    display_text = text[:50] + "..." if len(text) > 50 else text
                    print("è·³è¿‡ç¿»è¯‘: '" + display_text + "' - åŸå› : " + reason)
                elif skipped_count == 51:
                    print("... è¿˜æœ‰æ›´å¤šè·³è¿‡çš„é¡¹ç›®ï¼Œä¸å†æ˜¾ç¤ºè¯¦ç»†åŸå›  ...")

        print("è¿‡æ»¤åéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ç‰‡æ®µ: " + str(len(filtered_texts)))
        print("è·³è¿‡çš„æ–‡æœ¬ç‰‡æ®µ: " + str(skipped_count))

        # ä½¿ç”¨Okapiè¿›è¡ŒXLIFFè½¬æ¢ï¼Œç„¶åQwenç¿»è¯‘
        print("ğŸ”„ ä½¿ç”¨Okapiè¿›è¡ŒXLIFFè½¬æ¢ï¼Œç„¶åQwenç¿»è¯‘...")
        
        try:
            # å¯¼å…¥ Okapi é›†æˆæ¨¡å—
            from .okapi_integration import OkapiWordTranslator, verify_okapi_installation
            
            # éªŒè¯ Okapi å®‰è£…
            if not verify_okapi_installation():
                print("âŒ Okapi å®‰è£…éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                run_translation(docx_trans, filtered_texts, max_threads=30)
            else:
                print("âœ… Okapi å®‰è£…éªŒè¯æˆåŠŸï¼Œä½¿ç”¨XLIFFè½¬æ¢æ–¹æ¡ˆ")
                
                # åˆ›å»º Okapi ç¿»è¯‘å™¨
                translator = OkapiWordTranslator()
                print("âœ… Okapi ç¿»è¯‘å™¨åˆ›å»ºæˆåŠŸ")
                
                # è®¾ç½®ç¿»è¯‘æœåŠ¡ï¼ˆQwenï¼‰
                if docx_trans.get('model') == 'qwen-mt-plus':
                    docx_trans['server'] = 'qwen'
                    print("âœ… è®¾ç½®ç¿»è¯‘æœåŠ¡ä¸º Qwen")
                
                # é¢„åŠ è½½æœ¯è¯­åº“
                comparison_id = docx_trans.get('comparison_id')
                if comparison_id:
                    print("ğŸ“š å¼€å§‹é¢„åŠ è½½æœ¯è¯­åº“: " + str(comparison_id))
                    from .main import get_comparison
                    preloaded_terms = get_comparison(comparison_id)
                    if preloaded_terms:
                        print("ğŸ“š æœ¯è¯­åº“é¢„åŠ è½½æˆåŠŸ: " + str(len(preloaded_terms)) + " ä¸ªæœ¯è¯­")
                        docx_trans['preloaded_terms'] = preloaded_terms
                    else:
                        print("ğŸ“š æœ¯è¯­åº“é¢„åŠ è½½å¤±è´¥: " + str(comparison_id))
                
                # è®¾ç½®ç¿»è¯‘æœåŠ¡
                translator.set_translation_service(docx_trans)
                print("âœ… Okapi ç¿»è¯‘æœåŠ¡è®¾ç½®æˆåŠŸ")
                
                # è¯­è¨€æ˜ å°„ï¼šå°†ä¸­æ–‡è¯­è¨€åç§°è½¬æ¢ä¸ºè‹±æ–‡å…¨æ‹¼
                def map_language_to_qwen_format(lang_name):
                    language_mapping = {
                        'ä¸­æ–‡': 'Chinese',
                        'è‹±è¯­': 'English',
                        'æ—¥è¯­': 'Japanese',
                        'éŸ©è¯­': 'Korean',
                        'æ³•è¯­': 'French',
                        'å¾·è¯­': 'German',
                        'è¥¿ç­ç‰™è¯­': 'Spanish',
                        'ä¿„è¯­': 'Russian',
                        'é˜¿æ‹‰ä¼¯è¯­': 'Arabic',
                        'è‘¡è„ç‰™è¯­': 'Portuguese',
                        'æ„å¤§åˆ©è¯­': 'Italian',
                        'æ³°è¯­': 'Thai',
                        'è¶Šå—è¯­': 'Vietnamese',
                        'å°å°¼è¯­': 'Indonesian',
                        'é©¬æ¥è¯­': 'Malay',
                        'è²å¾‹å®¾è¯­': 'Filipino',
                        'ç¼…ç”¸è¯­': 'Burmese',
                        'æŸ¬åŸ”å¯¨è¯­': 'Khmer',
                        'è€æŒè¯­': 'Lao',
                        'æŸ¬è¯­': 'Khmer'
                    }
                    return language_mapping.get(lang_name.strip(), lang_name.strip())
                
                # è·å–å¹¶æ˜ å°„è¯­è¨€
                source_lang = "auto"  # å†™æ­»ä¸ºautoï¼Œè®©APIè‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
                target_lang = map_language_to_qwen_format(docx_trans.get('lang', 'è‹±è¯­'))
                
                print("ğŸ” è¯­è¨€æ˜ å°„è°ƒè¯•:")
                print("  åŸå§‹ç›®æ ‡è¯­è¨€: " + str(docx_trans.get('lang', 'è‹±è¯­')))
                print("  æ˜ å°„åç›®æ ‡è¯­è¨€: " + target_lang)
                
                # æ‰§è¡Œç¿»è¯‘ï¼šOkapiè½¬æ¢XLIFFï¼ŒQwenç¿»è¯‘ï¼Œç„¶ååˆå¹¶
                success = translator.translate_document(
                    docx_path,
                    target_file,
                    source_lang,
                    target_lang
                )
                
                if success:
                    print("âœ… Okapi XLIFFè½¬æ¢ + Qwenç¿»è¯‘å®Œæˆ")
                    
                    # æ›´æ–°è¿›åº¦ä¸º100%ï¼ˆç¿»è¯‘å®Œæˆï¼‰
                    try:
                        db.execute("update translate set process='100' where id=%s", trans['id'])
                        print("âœ… å·²æ›´æ–°è¿›åº¦ä¸º100%ï¼ˆç¿»è¯‘å®Œæˆï¼‰")
                    except Exception as e:
                        print("âš ï¸  æ›´æ–°è¿›åº¦å¤±è´¥: " + str(e))
                    
                    # å®Œæˆå¤„ç†
                    end_time = datetime.datetime.now()
                    spend_time = common.display_spend(start_time, end_time)
                    
                    # ç»Ÿè®¡ç¿»è¯‘çš„æ–‡æœ¬æ•°é‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                    text_count = len(filtered_texts)  # ä½¿ç”¨è¿‡æ»¤åçš„æ–‡æœ¬æ•°é‡
                    
                    if docx_trans['run_complete']:
                        to_translate.complete(docx_trans, text_count, spend_time)
                    
                    print("âœ… Okapi XLIFFè½¬æ¢ + Qwenç¿»è¯‘å®Œæˆï¼Œç”¨æ—¶: " + spend_time)
                    return True
                else:
                    print("âŒ Okapi XLIFFè½¬æ¢ + Qwenç¿»è¯‘å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                    run_translation(docx_trans, filtered_texts, max_threads=30)
                    
        except Exception as e:
            print("âŒ Okapi XLIFFè½¬æ¢ + Qwenç¿»è¯‘å‡ºé”™: " + str(e) + "ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            run_translation(docx_trans, filtered_texts, max_threads=30)

        # å†™å…¥ç¿»è¯‘ç»“æœï¼ˆå®Œå…¨ä¿ç•™åŸå§‹æ ¼å¼ï¼‰
        text_count = apply_translations(document, texts)
        print("åº”ç”¨äº† " + str(text_count) + " ä¸ªç¿»è¯‘ç»“æœ")

        # ä¿å­˜æ–‡æ¡£
        try:
            document.save(target_file)
            print("ç¿»è¯‘åçš„æ–‡æ¡£ä¿å­˜æˆåŠŸ: " + target_file)
        except Exception as e:
            to_translate.error(trans['id'], "ä¿å­˜æ–‡æ¡£å¤±è´¥: " + str(e))
            return False

        # å¤„ç†æ‰¹æ³¨ç­‰ç‰¹æ®Šå…ƒç´ 
        update_special_elements(target_file, texts)

        # 7. å®Œæˆå¤„ç†
        end_time = datetime.datetime.now()
        spend_time = common.display_spend(start_time, end_time)
        
        # æ›´æ–°è¿›åº¦ä¸º100%ï¼ˆä¼ ç»Ÿæ–¹æ³•ç¿»è¯‘å®Œæˆï¼‰
        try:
            db.execute("update translate set process='100' where id=%s", trans['id'])
            print("âœ… å·²æ›´æ–°è¿›åº¦ä¸º100%ï¼ˆä¼ ç»Ÿæ–¹æ³•ç¿»è¯‘å®Œæˆï¼‰")
        except Exception as e:
            print("âš ï¸  æ›´æ–°è¿›åº¦å¤±è´¥: " + str(e))
        
        if trans['run_complete']:
            to_translate.complete(trans, text_count, spend_time)
        print("PDFç¿»è¯‘ä»»åŠ¡å®Œæˆ: " + str(trans['id']))
        return True

    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print("âŒ PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: " + str(e))
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        # ç¡®ä¿é”™è¯¯çŠ¶æ€è¢«æ­£ç¡®è®°å½•
        to_translate.error(trans['id'], "PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: " + str(e))
        return False


def check_image(run):
    """æ£€æŸ¥runæ˜¯å¦åŒ…å«å›¾ç‰‡"""
    try:
        # æ£€æŸ¥runä¸­æ˜¯å¦æœ‰å›¾ç‰‡å…ƒç´ 
        for element in run._element:
            if element.tag.endswith('drawing') or element.tag.endswith('picture'):
                return True
        return False
    except:
        return False


def are_runs_compatible(run1, run2):
    """æ£€æŸ¥ä¸¤ä¸ªrunæ˜¯å¦å…¼å®¹ï¼ˆæœ€ä¸¥æ ¼çš„å…¼å®¹æ€§æ£€æŸ¥ï¼‰"""
    try:
        # 1. å­—ä½“åç§°å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.name != run2.font.name:
            return False
        
        # 2. å­—ä½“å¤§å°å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.size != run2.font.size:
            return False
        
        # 3. ç²—ä½“çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.bold != run2.font.bold:
            return False
        
        # 4. æ–œä½“çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.italic != run2.font.italic:
            return False
        
        # 5. ä¸‹åˆ’çº¿çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.underline != run2.font.underline:
            return False
        
        # 6. åˆ é™¤çº¿çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.strike != run2.font.strike:
            return False
        
        # 7. å­—ä½“é¢œè‰²å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.color.rgb != run2.font.color.rgb:
            return False
        
        # 8. èƒŒæ™¯è‰²å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.highlight_color != run2.font.highlight_color:
            return False
        
        # 9. ä¸Šæ ‡/ä¸‹æ ‡çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.superscript != run2.font.superscript:
            return False
        
        # 10. å°å‹å¤§å†™å­—æ¯çŠ¶æ€å¿…é¡»å®Œå…¨ç›¸åŒ
        if run1.font.small_caps != run2.font.small_caps:
            return False
        
        return True
    except:
        # å¦‚æœä»»ä½•æ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤ä¸å…¼å®¹
        return False


def extract_paragraph_with_merge(paragraph, texts, context_type):
    """æå–æ®µè½å†…å®¹ï¼Œä½¿ç”¨ä¿å®ˆrunåˆå¹¶ï¼ˆä¸æ·»åŠ ä¸Šä¸‹æ–‡ï¼‰"""
    paragraph_runs = list(paragraph.runs)
    
    # ä½¿ç”¨ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥
    merged_runs = conservative_run_merge(paragraph_runs)
    
    for merged_item in merged_runs:
        if not check_text(merged_item['text']):
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
        has_image = False
        for run in merged_item['runs']:
            if check_image(run):
                has_image = True
                break
        
        if has_image:
            continue
        
        texts.append({
            "text": merged_item['text'],
            "type": "merged_run",
            "merged_item": merged_item,
            "complete": False,
            "context_type": context_type  # æ ‡è®°ç±»å‹
        })


def conservative_run_merge(paragraph_runs, max_merge_length=500):
    """æœ€ä¸¥æ ¼çš„runåˆå¹¶ç­–ç•¥"""
    
    merged = []
    current_group = []
    current_length = 0
    original_count = len([r for r in paragraph_runs if check_text(r.text)])
    merged_count = 0
    
    for run in paragraph_runs:
        # è·³è¿‡åŒ…å«å›¾ç‰‡çš„run
        if check_image(run):
            # ä¿å­˜å½“å‰ç»„
            if current_group:
                merged.append(merge_compatible_runs(current_group))
                if len(current_group) > 1:
                    merged_count += 1
                current_group = []
                current_length = 0
            
            # å›¾ç‰‡runå•ç‹¬å¤„ç†ï¼Œä½†ä¸æ·»åŠ åˆ°ç¿»è¯‘åˆ—è¡¨
            continue
        
        if not check_text(run.text):
            continue
        
        # æœ€ä¸¥æ ¼æ¡ä»¶ï¼šåªåˆå¹¶æçŸ­çš„runï¼ˆé€šå¸¸æ˜¯ç©ºæ ¼ã€æ ‡ç‚¹ã€å•ä¸ªå­—ç¬¦ï¼‰
        if len(run.text) <= 5 and current_length < max_merge_length:
            # æ£€æŸ¥æ ¼å¼å…¼å®¹æ€§ï¼ˆæœ€ä¸¥æ ¼æ£€æŸ¥ï¼‰
            if not current_group or are_runs_compatible(current_group[-1], run):
                # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿åˆå¹¶åçš„æ–‡æœ¬ä¸ä¼šè¿‡é•¿
                if current_length + len(run.text) <= 10:
                    current_group.append(run)
                    current_length += len(run.text)
                    continue
        
        # ä¿å­˜å½“å‰ç»„
        if current_group:
            merged.append(merge_compatible_runs(current_group))
            if len(current_group) > 1:
                merged_count += 1
            current_group = []
            current_length = 0
        
        # å½“å‰runå•ç‹¬å¤„ç†
        merged.append({
            'text': run.text,
            'runs': [run],
            'type': 'single'
        })
    
    if current_group:
        merged.append(merge_compatible_runs(current_group))
        if len(current_group) > 1:
            merged_count += 1
    
    # æ‰“å°åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
    if merged_count > 0:
        print(f"Runåˆå¹¶ä¼˜åŒ–ï¼ˆæœ€ä¸¥æ ¼ç­–ç•¥ï¼‰: åŸå§‹{original_count}ä¸ªrun -> åˆå¹¶å{len(merged)}ä¸ªï¼Œå‡å°‘äº†{merged_count}ä¸ªAPIè°ƒç”¨")
    else:
        print(f"Runåˆå¹¶ï¼ˆæœ€ä¸¥æ ¼ç­–ç•¥ï¼‰: æœªæ‰¾åˆ°å¯åˆå¹¶çš„runï¼Œä¿æŒåŸå§‹{original_count}ä¸ªrun")
    
    return merged


def merge_compatible_runs(run_group):
    """åˆå¹¶å…¼å®¹çš„runç»„ï¼ˆæœ€ä¸¥æ ¼ç­–ç•¥ï¼‰"""
    
    # åˆå¹¶æ–‡æœ¬
    merged_text = "".join(run.text for run in run_group)
    
    # é¢å¤–å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åˆå¹¶åçš„æ–‡æœ¬ä¸ä¼šè¿‡é•¿
    if len(merged_text) > 20:
        print(f"âš ï¸  è­¦å‘Š: åˆå¹¶åæ–‡æœ¬è¿‡é•¿({len(merged_text)}å­—ç¬¦)ï¼Œå–æ¶ˆåˆå¹¶")
        # å¦‚æœåˆå¹¶åè¿‡é•¿ï¼Œè¿”å›å•ä¸ªrun
        return {
            'text': run_group[0].text,
            'runs': [run_group[0]],
            'type': 'single'
        }
    
    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰runçš„æ ¼å¼å®Œå…¨ä¸€è‡´
    first_run = run_group[0]
    for run in run_group[1:]:
        if not are_runs_compatible(first_run, run):
            print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°æ ¼å¼ä¸ä¸€è‡´ï¼Œå–æ¶ˆåˆå¹¶")
            return {
                'text': run_group[0].text,
                'runs': [run_group[0]],
                'type': 'single'
            }
    
    return {
        'text': merged_text,
        'runs': run_group,
        'type': 'merged'
    }


def get_skip_reason(text):
    """è·å–æ–‡æœ¬è¢«è·³è¿‡çš„åŸå› """
    if not text or len(text) == 0:
        return "ç©ºæ–‡æœ¬"
    
    if len(text.strip()) <= 2:
        text_stripped = text.strip()
        
        # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦
        if all('\u4e00' <= char <= '\u9fff' for char in text_stripped):
            return f"çŸ­ä¸­æ–‡å­—ç¬¦'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        # æ£€æŸ¥æ•°å­—
        if text_stripped.isdigit():
            return f"çŸ­æ•°å­—'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        # æ£€æŸ¥å•ä½ç¬¦å·
        if all(char in 'â€¢â„ƒVAhH' for char in text_stripped):
            return f"å¸¸ç”¨å•ä½ç¬¦å·'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        # æ£€æŸ¥ä¸­è‹±æ–‡æ··åˆ
        if len(text_stripped) == 2:
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text_stripped)
            if has_chinese:
                return f"ä¸­è‹±æ–‡æ··åˆçŸ­æ–‡æœ¬'{text_stripped}'ï¼ˆå·²å…è®¸ç¿»è¯‘ï¼‰"
        
        return f"çŸ­æ–‡æœ¬'{text_stripped}'ï¼ˆæ— æ„ä¹‰ï¼‰"
    
    if SPECIAL_SYMBOLS_PATTERN.match(text):
        return "çº¯ç‰¹æ®Šç¬¦å·"
    
    if NUMBERS_PATTERN.match(text):
        return "çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹"
    
    if common.is_all_punc(text):
        return "çº¯æ ‡ç‚¹ç¬¦å·"
    
    return "å…¶ä»–åŸå› "


def should_translate(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼ˆæ”¹è¿›çš„è¿‡æ»¤é€»è¾‘ï¼‰"""
    if not text or len(text) == 0:
        return False

    # å¯¹äºçŸ­æ–‡æœ¬ï¼ˆâ‰¤2å­—ç¬¦ï¼‰ï¼Œè¿›è¡Œç‰¹æ®Šå¤„ç†
    if len(text.strip()) <= 2:
        text_stripped = text.strip()
        
        # å…è®¸çŸ­ä¸­æ–‡å­—ç¬¦ï¼ˆé€šå¸¸æ˜¯æœ‰æ„ä¹‰çš„ï¼‰
        if all('\u4e00' <= char <= '\u9fff' for char in text_stripped):
            return True
        
        # å…è®¸çŸ­æ•°å­—ç»„åˆ
        if text_stripped.isdigit():
            return True
        
        # å…è®¸å¸¸ç”¨å•ä½ç¬¦å·ç»„åˆ
        if all(char in 'â€¢â„ƒVAhH' for char in text_stripped):
            return True
        
        # å…è®¸ä¸­è‹±æ–‡æ··åˆçš„çŸ­æ–‡æœ¬ï¼ˆå¦‚"å›¾1"ã€"è¡¨2"ï¼‰
        if len(text_stripped) == 2:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text_stripped)
            if has_chinese:
                return True
        
        # è¿‡æ»¤å…¶ä»–çŸ­æ–‡æœ¬
        return False

    # è¿‡æ»¤çº¯ç‰¹æ®Šç¬¦å·
    if SPECIAL_SYMBOLS_PATTERN.match(text):
        return False

    # è¿‡æ»¤çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹
    if NUMBERS_PATTERN.match(text):
        return False

    # è¿‡æ»¤å…¨æ˜¯æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬
    if common.is_all_punc(text):
        return False

    return True


def extract_content_for_translation(document, file_path, texts):
    """æå–éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼Œä½¿ç”¨runåˆå¹¶ä¼˜åŒ–"""
    # æ­£æ–‡æ®µè½
    for paragraph in document.paragraphs:
        extract_paragraph_with_merge(paragraph, texts, "paragraph")
        
        # å¤„ç†è¶…é“¾æ¥ï¼ˆå•ç‹¬å¤„ç†ï¼Œé¿å…ä¸æ™®é€šrunæ··åˆï¼‰
        for hyperlink in paragraph.hyperlinks:
            for run in hyperlink.runs:
                if check_text(run.text):
                    texts.append({
                        "text": run.text,
                        "type": "run",
                        "run": run,
                        "complete": False
                    })

    # è¡¨æ ¼å†…å®¹
    for table in document.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    extract_paragraph_with_merge(paragraph, texts, "table_cell")
                    
                    # å¤„ç†è¡¨æ ¼ä¸­çš„è¶…é“¾æ¥
                    for hyperlink in paragraph.hyperlinks:
                        for run in hyperlink.runs:
                            if check_text(run.text):
                                texts.append({
                                    "text": run.text,
                                    "type": "run",
                                    "run": run,
                                    "complete": False
                                })

    # é¡µçœ‰é¡µè„š
    for section in document.sections:
        # å¤„ç†é¡µçœ‰
        for paragraph in section.header.paragraphs:
            extract_paragraph_with_merge(paragraph, texts, "header")
            
            # å¤„ç†é¡µçœ‰ä¸­çš„è¶…é“¾æ¥
            for hyperlink in paragraph.hyperlinks:
                for run in hyperlink.runs:
                    if check_text(run.text):
                        texts.append({
                            "text": run.text,
                            "type": "run",
                            "run": run,
                            "complete": False
                        })

        # å¤„ç†é¡µè„š
        for paragraph in section.footer.paragraphs:
            extract_paragraph_with_merge(paragraph, texts, "footer")
            
            # å¤„ç†é¡µè„šä¸­çš„è¶…é“¾æ¥
            for hyperlink in paragraph.hyperlinks:
                for run in hyperlink.runs:
                    if check_text(run.text):
                        texts.append({
                            "text": run.text,
                            "type": "run",
                            "run": run,
                            "complete": False
                        })

    # æ‰¹æ³¨å†…å®¹
    extract_comments(file_path, texts)


def extract_comments(file_path, texts):
    """æå–æ–‡æ¡£ä¸­çš„æ‰¹æ³¨"""
    try:
        with zipfile.ZipFile(file_path, 'r') as docx:
            if 'word/comments.xml' in docx.namelist():
                with docx.open('word/comments.xml') as f:
                    tree = ET.parse(f)
                    root = tree.getroot()

                    # å®šä¹‰å‘½åç©ºé—´
                    namespaces = {
                        'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                    }

                    # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ³¨
                    for comment in root.findall('.//w:comment', namespaces) or root.findall(
                            './/{*}comment'):
                        # å°è¯•ä¸åŒæ–¹å¼è·å–ID
                        comment_id = None
                        for attr_name in [
                            '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id',
                            'id']:
                            if attr_name in comment.attrib:
                                comment_id = comment.attrib[attr_name]
                                break

                        if not comment_id:
                            continue

                        # æ”¶é›†æ‰¹æ³¨ä¸­çš„æ‰€æœ‰æ–‡æœ¬
                        text_elements = comment.findall('.//w:t', namespaces) or comment.findall(
                            './/{*}t')
                        for t_elem in text_elements:
                            if t_elem.text and check_text(t_elem.text):
                                texts.append({
                                    "text": t_elem.text,
                                    "type": "comment",
                                    "comment_id": comment_id,
                                    "complete": False,
                                    "original_text": t_elem.text  # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºåŒ¹é…
                                })
    except Exception as e:
        print(f"æå–æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")


def run_translation(trans, texts, max_threads=30):
    # ç¡¬ç¼–ç çº¿ç¨‹æ•°ä¸º30ï¼Œå¿½ç•¥å‰ç«¯ä¼ å…¥çš„é…ç½®
    """æ‰§è¡Œå¤šçº¿ç¨‹ç¿»è¯‘"""
    if not texts:
        print("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„å†…å®¹")
        return

    event = threading.Event()
    run_index = 0
    active_count = threading.activeCount()
    
    # è¿›åº¦æ›´æ–°ç›¸å…³å˜é‡
    completed_count = 0
    total_count = len(texts)
    progress_lock = threading.Lock()
    
    def update_progress():
        """æ›´æ–°ç¿»è¯‘è¿›åº¦"""
        nonlocal completed_count
        with progress_lock:
            completed_count += 1
            progress_percentage = min((completed_count / total_count) * 100, 100.0)
            print(f"ç¿»è¯‘è¿›åº¦: {completed_count}/{total_count} ({progress_percentage:.1f}%)")
            
            # æ›´æ–°æ•°æ®åº“è¿›åº¦
            try:
                from .to_translate import db
                db.execute("update translate set process=%s where id=%s", 
                         str(format(progress_percentage, '.1f')), 
                         trans['id'])
                
            except Exception as e:
                print(f"æ›´æ–°è¿›åº¦å¤±è´¥: {str(e)}")

    print(f"å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

    while run_index < len(texts):
        if threading.activeCount() < max_threads + active_count and not event.is_set():
            thread = threading.Thread(
                target=to_translate.get,
                args=(trans, event, texts, run_index)
            )
            thread.start()
            print(f"å¯åŠ¨ç¿»è¯‘çº¿ç¨‹ {run_index}")
            run_index += 1
        time.sleep(0.1)

    # ç­‰å¾…ç¿»è¯‘å®Œæˆï¼Œå¹¶ç›‘æ§è¿›åº¦
    last_completed_count = 0
    while not all(t.get('complete') for t in texts) and not event.is_set():
        # æ£€æŸ¥å®Œæˆçš„æ–‡æœ¬æ•°é‡
        current_completed = sum(1 for t in texts if t.get('complete', False))
        if current_completed > last_completed_count:
            # æœ‰æ–°çš„æ–‡æœ¬å®Œæˆï¼Œæ›´æ–°è¿›åº¦
            completed_count = current_completed
            progress_percentage = min((completed_count / total_count) * 100, 100.0)
            print(f"ç¿»è¯‘è¿›åº¦: {completed_count}/{total_count} ({progress_percentage:.1f}%)")
            
            # æ›´æ–°æ•°æ®åº“è¿›åº¦
            try:
                from .to_translate import db
                db.execute("update translate set process=%s where id=%s", 
                         str(format(progress_percentage, '.1f')), 
                         trans['id'])
                
            except Exception as e:
                print(f"æ›´æ–°è¿›åº¦å¤±è´¥: {str(e)}")
            
            last_completed_count = current_completed
        
        time.sleep(1)

    print("æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡å·²å®Œæˆ")


def apply_translations(document, texts):
    """åº”ç”¨ç¿»è¯‘ç»“æœåˆ°æ–‡æ¡£ï¼Œå®Œå…¨ä¿ç•™åŸå§‹æ ¼å¼"""
    text_count = 0

    for item in texts:
        if not item.get('complete', False):
            continue

        if item['type'] == 'run':
            # ç›´æ¥æ›¿æ¢runæ–‡æœ¬ï¼Œä¿ç•™æ‰€æœ‰æ ¼å¼
            item['run'].text = item.get('text', item['run'].text)
            text_count += 1
        elif item['type'] == 'merged_run':
            # å¤„ç†åˆå¹¶çš„runï¼Œéœ€è¦å°†ç¿»è¯‘ç»“æœåˆ†é…å›åŸå§‹run
            merged_item = item['merged_item']
            translated_text = item.get('text', merged_item['text'])
            
            if merged_item['type'] == 'merged':
                # åˆå¹¶çš„runç»„ï¼Œéœ€è¦æ™ºèƒ½åˆ†é…ç¿»è¯‘ç»“æœ
                distribute_translation_to_runs(merged_item['runs'], translated_text)
            else:
                # å•ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
                merged_item['runs'][0].text = translated_text
            
            text_count += 1

    return text_count


def distribute_translation_to_runs(runs, translated_text):
    """å°†ç¿»è¯‘ç»“æœæ™ºèƒ½åˆ†é…å›åŸå§‹runï¼Œä¿æŒæ ¼å¼"""
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
    if len(runs) == 1:
        runs[0].text = translated_text
        return
    
    # è®¡ç®—åŸå§‹æ–‡æœ¬çš„æ€»é•¿åº¦
    original_total_length = sum(len(run.text) for run in runs)
    
    # å¦‚æœç¿»è¯‘åæ–‡æœ¬é•¿åº¦å˜åŒ–ä¸å¤§ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
    if abs(len(translated_text) - original_total_length) <= 2:
        # æŒ‰åŸå§‹æ¯”ä¾‹åˆ†é…
        current_pos = 0
        for run in runs:
            original_ratio = len(run.text) / original_total_length
            target_length = int(len(translated_text) * original_ratio)
            
            if current_pos < len(translated_text):
                end_pos = min(current_pos + target_length, len(translated_text))
                run.text = translated_text[current_pos:end_pos]
                current_pos = end_pos
            else:
                run.text = ""
    else:
        # é•¿åº¦å˜åŒ–è¾ƒå¤§ï¼Œå°è¯•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
        words = translated_text.split()
        if len(words) >= len(runs):
            # æŒ‰runæ•°é‡åˆ†é…å•è¯
            for i, run in enumerate(runs):
                if i < len(words):
                    run.text = words[i]
                else:
                    run.text = ""
        else:
            # å•è¯æ•°é‡å°‘äºrunæ•°é‡ï¼Œå¹³å‡åˆ†é…
            chars_per_run = len(translated_text) // len(runs)
            for i, run in enumerate(runs):
                start_pos = i * chars_per_run
                end_pos = start_pos + chars_per_run if i < len(runs) - 1 else len(translated_text)
                run.text = translated_text[start_pos:end_pos]


def update_special_elements(docx_path, texts):
    """æ›´æ–°æ‰¹æ³¨ç­‰ç‰¹æ®Šå…ƒç´ """
    # ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„æ‰¹æ³¨
    comment_texts = [t for t in texts if t.get('type') == 'comment' and t.get('complete')]
    if not comment_texts:
        return  # æ²¡æœ‰éœ€è¦å¤„ç†çš„æ‰¹æ³¨

    temp_path = f"temp_{int(time.time())}_{os.path.basename(docx_path)}"

    try:
        with zipfile.ZipFile(docx_path, 'r') as zin, \
                zipfile.ZipFile(temp_path, 'w') as zout:

            for item in zin.infolist():
                with zin.open(item) as file:
                    if item.filename == 'word/comments.xml':
                        try:
                            tree = ET.parse(file)
                            root = tree.getroot()

                            # å®šä¹‰å‘½åç©ºé—´
                            namespaces = {
                                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                            }

                            # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ³¨
                            for comment in root.findall('.//w:comment', namespaces) or root.findall(
                                    './/{*}comment'):
                                # å°è¯•ä¸åŒæ–¹å¼è·å–ID
                                comment_id = None
                                for attr_name in [
                                    '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id',
                                    'id']:
                                    if attr_name in comment.attrib:
                                        comment_id = comment.attrib[attr_name]
                                        break

                                if not comment_id:
                                    continue

                                # æŸ¥æ‰¾åŒ¹é…çš„ç¿»è¯‘ç»“æœ
                                matching_texts = [t for t in comment_texts if
                                                  t.get('comment_id') == comment_id]
                                if not matching_texts:
                                    continue

                                # æ›´æ–°æ‰¹æ³¨æ–‡æœ¬
                                text_elements = comment.findall('.//w:t',
                                                                namespaces) or comment.findall(
                                    './/{*}t')
                                for i, t_elem in enumerate(text_elements):
                                    if i < len(matching_texts):
                                        # æ‰¾åˆ°åŒ¹é…çš„åŸå§‹æ–‡æœ¬
                                        for match in matching_texts:
                                            if match.get('original_text') == t_elem.text:
                                                t_elem.text = match.get('text', t_elem.text)
                                                break

                            # å†™å…¥ä¿®æ”¹åçš„XML
                            modified_xml = ET.tostring(root, encoding='utf-8', xml_declaration=True)
                            zout.writestr(item.filename, modified_xml)
                        except Exception as e:
                            print(f"å¤„ç†æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")
                            # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥å¤åˆ¶åŸæ–‡ä»¶
                            file.seek(0)
                            zout.writestr(item.filename, file.read())
                    else:
                        # ç›´æ¥å¤åˆ¶å…¶ä»–æ–‡ä»¶
                        file.seek(0)
                        zout.writestr(item.filename, file.read())

        # æ›¿æ¢åŸå§‹æ–‡ä»¶
        os.replace(temp_path, docx_path)
    except Exception as e:
        print(f"æ›´æ–°æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass


def check_text(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”éçº¯æ ‡ç‚¹ï¼‰"""
    return text and len(text) > 0 and not common.is_all_punc(text)


def get_pdf_translate_method():
    """è·å–PDFç¿»è¯‘æ–¹æ³•è®¾ç½®"""
    try:
        from app.models.setting import Setting
        pdf_method_setting = Setting.query.filter_by(
            group='other_setting',
            alias='pdf_translate_method',
            deleted_flag='N'
        ).first()
        return pdf_method_setting.value if pdf_method_setting else 'direct'
    except Exception as e:
        logging.warning(f"è·å–PDFç¿»è¯‘æ–¹æ³•è®¾ç½®å¤±è´¥: {e}")
        return 'direct'  # é»˜è®¤ä½¿ç”¨ç›´æ¥ç¿»è¯‘


def start_direct_pdf_translation(trans):
    """ç›´æ¥PDFç¿»è¯‘æ–¹æ³•"""
    try:
        print("ğŸš€ å¼€å§‹ç›´æ¥PDFç¿»è¯‘æµç¨‹")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤„ç†ä¸­ï¼Œä½†ä¸è®¾ç½®åˆå§‹è¿›åº¦
        try:
            from .to_translate import db
            db.execute("update translate set status='process', process='0' where id=%s", trans['id'])
            print("âœ… å·²æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºprocessï¼Œè¿›åº¦0%ï¼ˆå¼€å§‹PDFå¤„ç†ï¼‰")
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        original_path = Path(trans['file_path'])
        if not original_path.exists():
            print(f"âŒ æºæ–‡ä»¶ä¸å­˜åœ¨: {trans['file_path']}")
            to_translate.error(trans['id'], "æ–‡ä»¶ä¸å­˜åœ¨: " + trans['file_path'])
            return False
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target_dir = os.path.dirname(trans['target_file'])
        os.makedirs(target_dir, exist_ok=True)
        
        # æ£€æµ‹PDFé¡µæ•°ï¼Œå†³å®šä½¿ç”¨å“ªç§ç¿»è¯‘æ–¹æ³•
        try:
            import fitz
            doc = fitz.open(str(original_path))
            total_pages = doc.page_count
            doc.close()
            print(f"ğŸ“„ PDFæ€»é¡µæ•°: {total_pages}")
            
            if total_pages > 25:
                print("ğŸ“Š æ£€æµ‹åˆ°å¤§æ–‡ä»¶ï¼ˆè¶…è¿‡25é¡µï¼‰ï¼Œä½¿ç”¨å¤šçº¿ç¨‹åˆ†æ‰¹å¤„ç†")
                return start_large_pdf_translation(trans, total_pages)
            else:
                print("ğŸ“Š æ£€æµ‹åˆ°å°æ–‡ä»¶ï¼ˆ25é¡µä»¥å†…ï¼‰ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†")
                return start_small_pdf_translation(trans)
                
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹PDFé¡µæ•°å¤±è´¥: {e}ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†")
            return start_small_pdf_translation(trans)
            
    except Exception as e:
        print(f"âŒ ç›´æ¥PDFç¿»è¯‘å¼‚å¸¸: {str(e)}")
        to_translate.error(trans['id'], "ç›´æ¥PDFç¿»è¯‘å¼‚å¸¸: " + str(e))
        return False


def start_small_pdf_translation(trans):
    """å°æ–‡ä»¶PDFç¿»è¯‘æ–¹æ³•ï¼ˆ25é¡µä»¥å†…ï¼‰"""
    try:
        print("ğŸ¯ ä½¿ç”¨å°æ–‡ä»¶ç¿»è¯‘æ–¹æ³•")
        
        original_path = Path(trans['file_path'])
        
        # åˆ›å»ºç¿»è¯‘å‡½æ•°
        def translate_func(text):
            """ç¿»è¯‘å‡½æ•°ï¼Œä½¿ç”¨ç°æœ‰çš„ç¿»è¯‘é€»è¾‘"""
            try:
                # ä½¿ç”¨ç°æœ‰çš„ç¿»è¯‘é€»è¾‘
                from .to_translate import translate_text
                return translate_text(trans, text)
            except Exception as e:
                logging.warning("ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡: " + str(e))
                return text
        
        # åˆ›å»ºç›´æ¥PDFç¿»è¯‘å™¨
        translator = DirectPDFTranslator(
            input_pdf_path=str(original_path),
            target_lang=trans.get('lang', 'zh'),  # ä½¿ç”¨ 'lang' å­—æ®µä¸ç¿»è¯‘å‡½æ•°ä¸€è‡´
            user_id=trans.get('user_id')  # ä¼ é€’ç”¨æˆ·IDç”¨äºä¸´æ—¶æ–‡ä»¶éš”ç¦»
        )
        
        # æ‰§è¡Œå®Œæ•´ç¿»è¯‘æµç¨‹
        result_file = translator.run_complete_translation(
            trans=trans,
            output_file=trans['target_file']
        )
        
        if result_file and os.path.exists(result_file):
            print(f"âœ… å°æ–‡ä»¶PDFç¿»è¯‘å®Œæˆ: {result_file}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            try:
                from .to_translate import db
                db.execute("update translate set status='done', process='100' where id=%s", trans['id'])
                print("âœ… å·²æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºdoneï¼Œè¿›åº¦100%")
            except Exception as e:
                print(f"âš ï¸ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
            
            return True
        else:
            print(f"âŒ å°æ–‡ä»¶PDFç¿»è¯‘å¤±è´¥")
            to_translate.error(trans['id'], "å°æ–‡ä»¶PDFç¿»è¯‘å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ å°æ–‡ä»¶PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: {str(e)}")
        traceback.print_exc()
        to_translate.error(trans['id'], "å°æ–‡ä»¶PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: " + str(e))
        return False


def start_large_pdf_translation(trans, total_pages):
    """å¤§æ–‡ä»¶PDFç¿»è¯‘æ–¹æ³•ï¼ˆè¶…è¿‡20é¡µï¼‰"""
    try:
        print("ğŸ¯ ä½¿ç”¨å¤§æ–‡ä»¶å¤šçº¿ç¨‹ç¿»è¯‘æ–¹æ³•")
        print(f"ğŸ“Š æ€»é¡µæ•°: {total_pages}")
        
        original_path = Path(trans['file_path'])
        
        # å¯¼å…¥å¤§æ–‡ä»¶ç¿»è¯‘å™¨
        from .large_pdf_translator import LargePDFTranslator
        
        # åˆ›å»ºå¤§æ–‡ä»¶ç¿»è¯‘å™¨ï¼Œä½¿ç”¨ä¸å°PDFç›¸åŒçš„çº¿ç¨‹é…ç½®
        translator = LargePDFTranslator(
            input_pdf_path=str(original_path),
            batch_size=5,  # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œé™ä½å†…å­˜å ç”¨
            max_workers=30,  # ä¸å°PDFä¿æŒä¸€è‡´ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤30çº¿ç¨‹
            target_lang=trans.get('lang', 'zh'),  # ä½¿ç”¨ 'lang' å­—æ®µä¸ç¿»è¯‘å‡½æ•°ä¸€è‡´
            user_id=trans.get('user_id')  # ä¼ é€’ç”¨æˆ·IDç”¨äºä¸´æ—¶æ–‡ä»¶éš”ç¦»
        )
        
        # æ‰§è¡Œå®Œæ•´ç¿»è¯‘æµç¨‹
        result_file = translator.run_complete_translation(
            trans=trans,
            output_file=trans['target_file']
        )
        
        if result_file and os.path.exists(result_file):
            print(f"âœ… å¤§æ–‡ä»¶PDFç¿»è¯‘å®Œæˆ: {result_file}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            try:
                from .to_translate import db
                db.execute("update translate set status='done', process='100' where id=%s", trans['id'])
                print("âœ… å·²æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºdoneï¼Œè¿›åº¦100%")
            except Exception as e:
                print(f"âš ï¸ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
            
            return True
        else:
            print("âŒ å¤§æ–‡ä»¶PDFç¿»è¯‘å¤±è´¥")
            to_translate.error(trans['id'], "å¤§æ–‡ä»¶PDFç¿»è¯‘å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ å¤§æ–‡ä»¶PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: {str(e)}")
        traceback.print_exc()
        to_translate.error(trans['id'], "å¤§æ–‡ä»¶PDFç¿»è¯‘è¿‡ç¨‹å‡ºé”™: " + str(e))
        return False


class DirectPDFTranslator:
    """
    ç›´æ¥PDFç¿»è¯‘å™¨ - æ”¯æŒä¸­æ–‡å­—ç¬¦æ˜¾ç¤ºï¼Œä¿æŒåŸå§‹æ ·å¼ï¼Œæ— èƒŒæ™¯è¦†ç›–
    åŸºäºpdf-translator-finalé¡¹ç›®é›†æˆ
    """
    
    def __init__(self, input_pdf_path, target_lang='zh', user_id=None):
        self.input_pdf_path = input_pdf_path
        self.target_lang = target_lang
        self.user_id = user_id
        self.doc = None
        self.extracted_texts = []
        
    def step1_split_pdf(self, output_dir):
        """æ­¥éª¤1: æ‹†åˆ†PDFä¸ºæ–‡æœ¬JSONå’Œæ— æ–‡æœ¬PDF"""
        print("=" * 60)
        print("æ­¥éª¤1: æ‹†åˆ†PDFä¸ºæ–‡æœ¬JSONå’Œæ— æ–‡æœ¬PDF")
        print("=" * 60)
        
        try:
            # 1. æ‰“å¼€PDF
            print("1. æ‰“å¼€PDF...")
            self.doc = fitz.open(self.input_pdf_path)
            print(f"   æ‰“å¼€äº† {self.doc.page_count} é¡µçš„PDF")
            
            # 2. æå–æ–‡æœ¬ä¿¡æ¯
            print("\n2. æå–æ–‡æœ¬ä¿¡æ¯...")
            self.extracted_texts = []
            
            for page_num in range(self.doc.page_count):
                page = self.doc[page_num]
                print(f"   å¤„ç†ç¬¬ {page_num + 1} é¡µ...")
                
                # æå–æ–‡æœ¬å—
                text_blocks = page.get_text("dict", flags=fitz.TEXTFLAGS_TEXT)["blocks"]
                page_texts = []
                
                for block in text_blocks:
                    if "lines" in block:
                        for line in block["lines"]:
                            for span in line["spans"]:
                                text = span["text"].strip()
                                if text:
                                    text_info = {
                                        "text": text,
                                        "bbox": span["bbox"],
                                        "size": span["size"],
                                        "color": span["color"],
                                        "font": span["font"]
                                    }
                                    page_texts.append(text_info)
                
                page_data = {
                    "page_number": page_num,
                    "texts": page_texts
                }
                self.extracted_texts.append(page_data)
                print(f"   æå–äº† {len(page_texts)} ä¸ªæ–‡æœ¬å—")
            
            # 3. ä¿å­˜æå–çš„æ–‡æœ¬
            print("\n3. ä¿å­˜æå–çš„æ–‡æœ¬...")
            extracted_texts_file = os.path.join(output_dir, "extracted_texts.json")
            with open(extracted_texts_file, 'w', encoding='utf-8') as f:
                json.dump(self.extracted_texts, f, ensure_ascii=False, indent=2)
            print(f"âœ… æå–çš„æ–‡æœ¬å·²ä¿å­˜åˆ°: {extracted_texts_file}")
            
            # 4. åˆ›å»ºæ— æ–‡æœ¬PDF
            print("\n4. åˆ›å»ºæ— æ–‡æœ¬PDF...")
            no_text_doc = fitz.open()
            
            for page_num in range(self.doc.page_count):
                page = self.doc[page_num]
                new_page = no_text_doc.new_page(width=page.rect.width, height=page.rect.height)
                
                # å¤åˆ¶é¡µé¢å†…å®¹ï¼ˆå›¾ç‰‡ã€èƒŒæ™¯ç­‰ï¼‰
                new_page.show_pdf_page(page.rect, self.doc, page_num)
                
                # ä½¿ç”¨ç²¾ç¡®æ–¹æ³•åˆ é™¤æ‰€æœ‰æ–‡æœ¬
                text_blocks = page.get_text("dict", flags=fitz.TEXTFLAGS_TEXT)["blocks"]
                text_count = 0
                
                for block in text_blocks:
                    if "lines" in block:
                        for line in block["lines"]:
                            for span in line["spans"]:
                                bbox = span["bbox"]
                                text = span["text"].strip()
                                if text:
                                    try:
                                        # ä½¿ç”¨add_redact_annotä½†ä¸å¡«å……ï¼Œé¿å…ç™½è‰²é®æŒ¡
                                        redact_annot = new_page.add_redact_annot(bbox, fill=None)
                                        text_count += 1
                                    except Exception as e:
                                        logging.warning(f"åˆ é™¤æ–‡æœ¬å¤±è´¥: {e}")
                                        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ç”¨é€æ˜å¡«å……
                                        try:
                                            redact_annot = new_page.add_redact_annot(bbox, fill=(0, 0, 0, 0))
                                            text_count += 1
                                        except Exception as e2:
                                            logging.warning(f"é€æ˜å¡«å……ä¹Ÿå¤±è´¥: {e2}")
                
                # åº”ç”¨åˆ é™¤æ“ä½œ
                try:
                    new_page.apply_redactions()
                    print(f"   ç¬¬ {page_num + 1} é¡µåˆ é™¤äº† {text_count} ä¸ªæ–‡æœ¬å—")
                except Exception as e:
                    logging.warning(f"åº”ç”¨åˆ é™¤æ“ä½œå¤±è´¥: {e}")
                    print(f"   âš ï¸ ç¬¬ {page_num + 1} é¡µåº”ç”¨åˆ é™¤æ“ä½œå¤±è´¥: {e}")
            
            no_text_pdf_file = os.path.join(output_dir, "no_text.pdf")
            no_text_doc.save(no_text_pdf_file)
            no_text_doc.close()
            print(f"âœ… æ— æ–‡æœ¬PDFå·²ä¿å­˜åˆ°: {no_text_pdf_file}")
            
            return extracted_texts_file, no_text_pdf_file
            
        except Exception as e:
            logging.error(f"æ‹†åˆ†PDFæ—¶å‡ºé”™: {e}")
            raise
    
    def step2_translate_texts(self, extracted_texts_file, trans, output_dir):
        """æ­¥éª¤2: ä½¿ç”¨å¤šçº¿ç¨‹ç¿»è¯‘JSONä¸­çš„æ–‡æœ¬"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤2: ä½¿ç”¨å¤šçº¿ç¨‹ç¿»è¯‘JSONä¸­çš„æ–‡æœ¬")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½æå–çš„æ–‡æœ¬
            print("1. åŠ è½½æå–çš„æ–‡æœ¬...")
            with open(extracted_texts_file, 'r', encoding='utf-8') as f:
                extracted_texts = json.load(f)
            
            print("   åŠ è½½äº† " + str(len(extracted_texts)) + " é¡µçš„æ–‡æœ¬æ•°æ®")
            
            # 2. å‡†å¤‡å¤šçº¿ç¨‹ç¿»è¯‘æ•°æ®
            print("\n2. å‡†å¤‡å¤šçº¿ç¨‹ç¿»è¯‘æ•°æ®...")
            texts_for_translation = []
            text_mapping = {}  # ç”¨äºæ˜ å°„ç¿»è¯‘ç»“æœå›åŸå§‹ä½ç½®
            
            for page_idx, page_data in enumerate(extracted_texts):
                page_num = page_data["page_number"]
                for text_idx, text_info in enumerate(page_data["texts"]):
                    original_text = text_info["text"]
                    if original_text and original_text.strip():
                        # åˆ›å»ºç¿»è¯‘ä»»åŠ¡
                        translation_task = {
                            'text': original_text,
                            'complete': False,
                            'page_idx': page_idx,
                            'text_idx': text_idx,
                            'original_info': text_info
                        }
                        texts_for_translation.append(translation_task)
                        text_mapping[(page_idx, text_idx)] = translation_task
            
            print("   å‡†å¤‡ç¿»è¯‘ " + str(len(texts_for_translation)) + " ä¸ªæ–‡æœ¬ç‰‡æ®µ")
            
            # 3. ä½¿ç”¨å¤šçº¿ç¨‹ç¿»è¯‘
            print("\n3. å¼€å§‹å¤šçº¿ç¨‹ç¿»è¯‘...")
            if texts_for_translation:
                # ä½¿ç”¨ç°æœ‰çš„å¤šçº¿ç¨‹ç¿»è¯‘ç³»ç»Ÿ
                run_translation(trans, texts_for_translation, max_threads=30)
                print("   å¤šçº¿ç¨‹ç¿»è¯‘å®Œæˆ")
            else:
                print("   æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬")
            
            # 4. é‡æ–°ç»„ç»‡ç¿»è¯‘ç»“æœ
            print("\n4. é‡æ–°ç»„ç»‡ç¿»è¯‘ç»“æœ...")
            translated_texts = []
            
            for page_data in extracted_texts:
                page_num = page_data["page_number"]
                translated_page_data = {"page_number": page_num, "texts": []}
                
                for text_idx, text_info in enumerate(page_data["texts"]):
                    original_text = text_info["text"]
                    if original_text and original_text.strip():
                        # è·å–ç¿»è¯‘ç»“æœ
                        translation_task = text_mapping.get((extracted_texts.index(page_data), text_idx))
                        if translation_task and translation_task.get('complete'):
                            translated_text = translation_task.get('text', original_text)
                            print("   âœ… ç¿»è¯‘: '" + original_text[:20] + "...' -> '" + translated_text[:20] + "...'")
                        else:
                            translated_text = original_text
                            print("   âš ï¸ ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡: '" + original_text[:20] + "...'")
                        
                        # åˆ›å»ºç¿»è¯‘åçš„æ–‡æœ¬ä¿¡æ¯
                        translated_text_info = text_info.copy()
                        translated_text_info["text"] = translated_text
                        translated_text_info["original_text"] = original_text
                        translated_page_data["texts"].append(translated_text_info)
                    else:
                        translated_page_data["texts"].append(text_info)
                
                translated_texts.append(translated_page_data)
            
            # 5. ä¿å­˜ç¿»è¯‘åçš„æ–‡æœ¬
            print("\n5. ä¿å­˜ç¿»è¯‘åçš„æ–‡æœ¬...")
            translated_texts_file = os.path.join(output_dir, "translated_texts.json")
            with open(translated_texts_file, 'w', encoding='utf-8') as f:
                json.dump(translated_texts, f, ensure_ascii=False, indent=2)
            print("âœ… ç¿»è¯‘åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°: " + translated_texts_file)
            
            return translated_texts_file
            
        except Exception as e:
            logging.error("ç¿»è¯‘æ–‡æœ¬æ—¶å‡ºé”™: " + str(e))
            raise
    
    def step3_fill_translated_texts(self, translated_texts_file, no_text_pdf_file, output_file):
        """æ­¥éª¤3: ä½¿ç”¨insert_htmlboxå›å¡«ç¿»è¯‘åçš„æ–‡æœ¬"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤3: ä½¿ç”¨insert_htmlboxå›å¡«ç¿»è¯‘åçš„æ–‡æœ¬")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½ç¿»è¯‘åçš„æ–‡æœ¬
            print("1. åŠ è½½ç¿»è¯‘åçš„æ–‡æœ¬...")
            with open(translated_texts_file, 'r', encoding='utf-8') as f:
                translated_texts = json.load(f)
            
            print(f"   åŠ è½½äº† {len(translated_texts)} é¡µçš„ç¿»è¯‘æ–‡æœ¬æ•°æ®")
            
            # 2. æ‰“å¼€æ— æ–‡æœ¬PDF
            print("\n2. æ‰“å¼€æ— æ–‡æœ¬PDF...")
            doc = fitz.open(no_text_pdf_file)
            print(f"   æ‰“å¼€äº† {doc.page_count} é¡µçš„PDF")
            
            # 3. å›å¡«ç¿»è¯‘æ–‡æœ¬
            print("\n3. å›å¡«ç¿»è¯‘æ–‡æœ¬...")
            
            for page_data in translated_texts:
                page_num = page_data["page_number"]
                page = doc[page_num]
                print(f"   å¤„ç†ç¬¬ {page_num + 1} é¡µ...")
                
                for text_info in page_data["texts"]:
                    text = text_info["text"]
                    if text and text.strip():
                        bbox = text_info["bbox"]
                        font_size = text_info["size"]
                        color = text_info["color"]
                        
                        # æ ‡å‡†åŒ–é¢œè‰²
                        if isinstance(color, (int, float)):
                            if color == 0:
                                color = (0, 0, 0)  # é»‘è‰²
                            elif color == 16777215:
                                color = (1, 1, 1)  # ç™½è‰²
                            else:
                                # è½¬æ¢ä¸ºRGB
                                r = ((color >> 16) & 0xFF) / 255.0
                                g = ((color >> 8) & 0xFF) / 255.0
                                b = (color & 0xFF) / 255.0
                                color = (r, g, b)
                        elif isinstance(color, (list, tuple)) and len(color) >= 3:
                            color = tuple(color[:3])
                        else:
                            color = (0, 0, 0)  # é»˜è®¤é»‘è‰²
                        
                        # ä½¿ç”¨insert_htmlboxæ–¹æ³•ï¼ˆæ”¯æŒä¸­æ–‡ï¼Œé¿å…èƒŒæ™¯è¦†ç›–ï¼‰
                        try:
                            # åˆ›å»ºæ–‡æœ¬æ¡†
                            textbox = fitz.Rect(bbox[0], bbox[1], bbox[2], bbox[3])
                            
                            # è®¡ç®—æ–‡æœ¬é•¿åº¦å’Œboxå®½åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦æ¢è¡Œ
                            box_width = bbox[2] - bbox[0]
                            box_height = bbox[3] - bbox[1]
                            
                            # ä¼°ç®—æ¯è¡Œå­—ç¬¦æ•°ï¼ˆæ ¹æ®å­—ä½“å¤§å°ï¼‰
                            chars_per_line = max(1, int(box_width / (font_size * 0.6)))  # 0.6æ˜¯ç»éªŒå€¼

                            wrapped_text = text
                            
                            # æ„å»ºHTMLæ–‡æœ¬ï¼Œä½¿ç”¨å®Œå…¨é€æ˜çš„èƒŒæ™¯
                            html_text = f"""
                            <div style="
                                font-size: {font_size}px;
                                color: rgb({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)});
                                font-family: sans-serif;
                                line-height: 1.0;
                                margin: 0;
                                padding: 0;
                                background: none;
                                background-color: transparent;
                                background-image: none;
                                border: none;
                                outline: none;
                                box-shadow: none;
                            ">
                                {wrapped_text}
                            </div>
                            """
                            
                            # ä½¿ç”¨insert_htmlboxæ’å…¥æ–‡æœ¬
                            page.insert_htmlbox(textbox, html_text)
                            logging.info(f"âœ… æ–‡æœ¬æ’å…¥æˆåŠŸ: '{text[:20]}...'")
                            print(f"   âœ… æ–‡æœ¬æ’å…¥æˆåŠŸ: '{text[:20]}...'")
                        except Exception as e:
                            logging.error(f"æ–‡æœ¬æ’å…¥å¤±è´¥: {e}")
                            print(f"   âŒ æ–‡æœ¬æ’å…¥å¤±è´¥: '{text[:20]}...' - {e}")
                    else:
                        logging.warning(f"æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡: '{text}'")
            
            # 4. ä¿å­˜æœ€ç»ˆPDF
            print("\n4. ä¿å­˜æœ€ç»ˆPDF...")
            doc.save(output_file)
            doc.close()
            print(f"âœ… æœ€ç»ˆç¿»è¯‘åçš„PDFå·²ä¿å­˜åˆ°: {output_file}")
            
            return output_file
            
        except Exception as e:
            logging.error(f"å›å¡«PDFæ—¶å‡ºé”™: {e}")
            raise
    
    def run_complete_translation(self, trans, output_file):
        """è¿è¡Œå®Œæ•´çš„ç¿»è¯‘æµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„PDFç¿»è¯‘æµç¨‹")
        print("=" * 60)
        
        # ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨äºæœ€åæ¸…ç†
        temp_files = []
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.dirname(output_file)
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¸ºæ¯ä¸ªç¿»è¯‘ä»»åŠ¡åˆ›å»ºå”¯ä¸€çš„ä¸´æ—¶ç›®å½•ï¼Œé¿å…æ‰¹é‡ç¿»è¯‘æ—¶æ–‡ä»¶å†²çª
            import uuid
            if self.user_id:
                # ä½¿ç”¨ç”¨æˆ·IDåˆ›å»ºéš”ç¦»ç›®å½•
                temp_dir_name = f"temp_user_{self.user_id}_{uuid.uuid4().hex[:8]}"
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·IDï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼
                temp_dir_name = f"temp_{uuid.uuid4().hex[:8]}"
            temp_dir = os.path.join(output_dir, temp_dir_name)
            os.makedirs(temp_dir, exist_ok=True)
            print(f"ğŸ“ åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_dir}")
            print(f"ğŸ‘¤ ç”¨æˆ·ID: {self.user_id if self.user_id else 'N/A'}")
            
            # æ­¥éª¤1: æ‹†åˆ†PDF
            extracted_texts_file, no_text_pdf_file = self.step1_split_pdf(temp_dir)
            temp_files.extend([extracted_texts_file, no_text_pdf_file])
            
            # æ­¥éª¤2: ç¿»è¯‘æ–‡æœ¬
            translated_texts_file = self.step2_translate_texts(extracted_texts_file, trans, temp_dir)
            temp_files.append(translated_texts_file)
            
            # æ­¥éª¤3: å›å¡«ç¿»è¯‘æ–‡æœ¬
            final_pdf_file = self.step3_fill_translated_texts(translated_texts_file, no_text_pdf_file, output_file)
            
            # æ­¥éª¤4: å‹ç¼©PDFæ–‡ä»¶
            print("\n" + "=" * 60)
            print("æ­¥éª¤4: å‹ç¼©PDFæ–‡ä»¶")
            print("=" * 60)
            
            # ç”Ÿæˆå‹ç¼©åçš„æ–‡ä»¶å
            base_name = os.path.splitext(output_file)[0]
            compressed_pdf_file = base_name + "_compressed.pdf"
            
            # å‹ç¼©PDF
            optimized_pdf_file = self._optimize_pdf_size(final_pdf_file, compressed_pdf_file)
            
            if optimized_pdf_file and os.path.exists(optimized_pdf_file):
                # åˆ é™¤åŸå§‹åˆæˆPDFï¼Œåªä¿ç•™å‹ç¼©åçš„PDF
                try:
                    os.remove(final_pdf_file)
                    print("âœ… å·²åˆ é™¤åŸå§‹åˆæˆPDF: " + os.path.basename(final_pdf_file))
                except Exception as e:
                    logging.warning("åˆ é™¤åŸå§‹PDFå¤±è´¥: " + str(e))
                    print("âš ï¸ åˆ é™¤åŸå§‹PDFå¤±è´¥: " + str(e))
                
                # å°†å‹ç¼©åçš„PDFé‡å‘½åä¸ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶
                try:
                    os.rename(optimized_pdf_file, output_file)
                    print("âœ… å‹ç¼©åçš„PDFå·²é‡å‘½åä¸ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶")
                    final_pdf_file = output_file
                except Exception as e:
                    logging.warning("é‡å‘½åå‹ç¼©PDFå¤±è´¥: " + str(e))
                    print("âš ï¸ é‡å‘½åå‹ç¼©PDFå¤±è´¥: " + str(e))
                    final_pdf_file = optimized_pdf_file
            else:
                print("âš ï¸ PDFå‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹PDF")
                final_pdf_file = final_pdf_file
            
            # åˆ é™¤åŸå§‹ä¸Šä¼ æ–‡ä»¶ï¼Œä¿ç•™ç¿»è¯‘åçš„æ–‡ä»¶ï¼ˆå¸¦UUIDåç¼€ï¼‰
            original_file = self.input_pdf_path
            if os.path.exists(original_file) and original_file != output_file:
                try:
                    os.remove(original_file)
                    print("âœ… å·²åˆ é™¤åŸå§‹ä¸Šä¼ æ–‡ä»¶: " + os.path.basename(original_file))
                except Exception as e:
                    logging.warning("åˆ é™¤åŸå§‹ä¸Šä¼ æ–‡ä»¶å¤±è´¥: " + str(e))
                    print("âš ï¸ åˆ é™¤åŸå§‹ä¸Šä¼ æ–‡ä»¶å¤±è´¥: " + str(e))
            
            # ç¡®ä¿è¾“å‡ºæ–‡ä»¶ä¿æŒUUIDåç¼€ï¼Œé¿å…å¤šæ¬¡ç¿»è¯‘å†²çª
            # ä¸é‡å‘½åæ–‡ä»¶ï¼Œä¿æŒæ•°æ®åº“è·¯å¾„ä¸å®é™…æ–‡ä»¶ä¸€è‡´
            
            print("\n" + "=" * 60)
            print("ğŸ‰ å®Œæ•´PDFç¿»è¯‘æµç¨‹å®Œæˆ!")
            print("=" * 60)
            print(f"ğŸ“„ è¾“å…¥æ–‡ä»¶: {self.input_pdf_path}")
            print(f"ğŸ“ æå–æ–‡æœ¬: {extracted_texts_file}")
            print(f"ğŸ”„ ç¿»è¯‘æ–‡æœ¬: {translated_texts_file}")
            print(f"ğŸ“„ æ— æ–‡æœ¬PDF: {no_text_pdf_file}")
            print(f"ğŸ¯ æœ€ç»ˆè¾“å‡º: {final_pdf_file}")
            print("=" * 60)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
            self._cleanup_temp_files(temp_files, temp_dir)
            
            return final_pdf_file
            
        except Exception as e:
            logging.error(f"å®Œæ•´ç¿»è¯‘æµç¨‹å¤±è´¥: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
            self._cleanup_temp_files(temp_files, temp_dir)
            raise
        finally:
            if self.doc:
                self.doc.close()
    
    def _optimize_pdf_size(self, input_pdf_path, output_pdf_path=None):
        """ä¼˜åŒ–PDFæ–‡ä»¶å¤§å°"""
        print("\n" + "=" * 60)
        print("PDFæ–‡ä»¶å¤§å°ä¼˜åŒ–")
        print("=" * 60)
        
        if not os.path.exists(input_pdf_path):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨: " + input_pdf_path)
            return None
        
        # è·å–åŸå§‹æ–‡ä»¶å¤§å°
        original_size = os.path.getsize(input_pdf_path)
        print("åŸå§‹æ–‡ä»¶å¤§å°: " + str(original_size) + " å­—èŠ‚ (" + str(original_size/1024/1024) + " MB)")
        
        doc = None
        doc2 = None
        try:
            # æ‰“å¼€PDF
            print("\n1. æ‰“å¼€PDF...")
            doc = fitz.open(input_pdf_path)
            print("   æ‰“å¼€äº† " + str(doc.page_count) + " é¡µçš„PDF")
            
            # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
            if output_pdf_path is None:
                base_name = os.path.splitext(input_pdf_path)[0]
                output_pdf_path = base_name + "_optimized.pdf"
            
            # ä¼˜åŒ–é€‰é¡¹
            print("\n2. åº”ç”¨ä¼˜åŒ–é€‰é¡¹...")
            
            # æ–¹æ³•1: ä½¿ç”¨å‹ç¼©é€‰é¡¹ä¿å­˜
            print("   æ–¹æ³•1: ä½¿ç”¨å‹ç¼©é€‰é¡¹...")
            doc.save(
                output_pdf_path,
                garbage=4,        # åƒåœ¾å›æ”¶
                deflate=True,     # å‹ç¼©
                clean=True,       # æ¸…ç†
                encryption=fitz.PDF_ENCRYPT_NONE  # æ— åŠ å¯†
            )
            
            # æ£€æŸ¥ä¼˜åŒ–åçš„æ–‡ä»¶å¤§å°
            optimized_size = os.path.getsize(output_pdf_path)
            print("   ä¼˜åŒ–åæ–‡ä»¶å¤§å°: " + str(optimized_size) + " å­—èŠ‚ (" + str(optimized_size/1024/1024) + " MB)")
            
            # è®¡ç®—å‹ç¼©ç‡
            compression_ratio = (1 - optimized_size / original_size) * 100
            print("   å‹ç¼©ç‡: " + str(compression_ratio) + "%")
            
            # æ–¹æ³•2: å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼Œå°è¯•æ›´æ¿€è¿›çš„ä¼˜åŒ–
            if optimized_size > 800000:  # å¦‚æœè¿˜æ˜¯è¶…è¿‡800KB
                print("\n   æ–¹æ³•2: åº”ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–...")
                aggressive_output = os.path.splitext(output_pdf_path)[0] + "_aggressive.pdf"
                
                # é‡æ–°æ‰“å¼€å¹¶åº”ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–
                doc2 = fitz.open(input_pdf_path)
                doc2.save(
                    aggressive_output,
                    garbage=4,
                    deflate=True,
                    clean=True,
                    encryption=fitz.PDF_ENCRYPT_NONE,
                    # æ›´æ¿€è¿›çš„ä¼˜åŒ–é€‰é¡¹
                    ascii=False,      # äºŒè¿›åˆ¶æ¨¡å¼
                    expand=0,         # ä¸å±•å¼€
                    no_new_id=True,   # ä¸ç”Ÿæˆæ–°ID
                )
                
                # æ£€æŸ¥æ¿€è¿›ä¼˜åŒ–åçš„æ–‡ä»¶å¤§å°
                aggressive_size = os.path.getsize(aggressive_output)
                print("   æ¿€è¿›ä¼˜åŒ–åæ–‡ä»¶å¤§å°: " + str(aggressive_size) + " å­—èŠ‚ (" + str(aggressive_size/1024/1024) + " MB)")
                
                aggressive_ratio = (1 - aggressive_size / original_size) * 100
                print("   æ¿€è¿›å‹ç¼©ç‡: " + str(aggressive_ratio) + "%")
                
                # é€‰æ‹©æ›´å°çš„æ–‡ä»¶
                if aggressive_size < optimized_size:
                    print("   âœ… æ¿€è¿›ä¼˜åŒ–æ•ˆæœæ›´å¥½ï¼Œä½¿ç”¨: " + aggressive_output)
                    output_pdf_path = aggressive_output
                    optimized_size = aggressive_size
            
            print("\nâœ… ä¼˜åŒ–å®Œæˆ! è¾“å‡ºæ–‡ä»¶: " + output_pdf_path)
            print("ğŸ“Š æ–‡ä»¶å¤§å°å¯¹æ¯”:")
            print("   åŸå§‹: " + str(original_size) + " å­—èŠ‚ (" + str(original_size/1024/1024) + " MB)")
            print("   ä¼˜åŒ–: " + str(optimized_size) + " å­—èŠ‚ (" + str(optimized_size/1024/1024) + " MB)")
            print("   èŠ‚çœ: " + str(original_size - optimized_size) + " å­—èŠ‚ (" + str((1 - optimized_size/original_size)*100) + "%)")
            
            return output_pdf_path
            
        except Exception as e:
            logging.error("ä¼˜åŒ–PDFæ—¶å‡ºé”™: " + str(e))
            print("âŒ ä¼˜åŒ–PDFæ—¶å‡ºé”™: " + str(e))
            raise
        finally:
            # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£å¯¹è±¡è¢«æ­£ç¡®å…³é—­
            if doc2 is not None:
                try:
                    doc2.close()
                    logging.debug("æ¿€è¿›ä¼˜åŒ–PDFæ–‡æ¡£å·²å…³é—­")
                except Exception as close_error:
                    logging.warning(f"å…³é—­æ¿€è¿›ä¼˜åŒ–PDFæ–‡æ¡£æ—¶å‡ºé”™: {close_error}")
            
            if doc is not None:
                try:
                    doc.close()
                    logging.debug("ä¼˜åŒ–PDFæ–‡æ¡£å·²å…³é—­")
                except Exception as close_error:
                    logging.warning(f"å…³é—­ä¼˜åŒ–PDFæ–‡æ¡£æ—¶å‡ºé”™: {close_error}")

    def _cleanup_temp_files(self, temp_files, temp_dir=None):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•"""
        print("\nğŸ§¹ å¼€å§‹æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        cleaned_count = 0
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in temp_files:
            if temp_file and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    print("âœ… å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: " + os.path.basename(temp_file))
                    cleaned_count += 1
                except Exception as e:
                    logging.warning("åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: " + temp_file + " - " + str(e))
                    print("âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: " + os.path.basename(temp_file) + " - " + str(e))
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir and os.path.exists(temp_dir):
            try:
                import shutil
                shutil.rmtree(temp_dir)
                print("âœ… å·²åˆ é™¤ä¸´æ—¶ç›®å½•: " + os.path.basename(temp_dir))
                cleaned_count += 1
            except Exception as e:
                logging.warning("åˆ é™¤ä¸´æ—¶ç›®å½•å¤±è´¥: " + temp_dir + " - " + str(e))
                print("âš ï¸ åˆ é™¤ä¸´æ—¶ç›®å½•å¤±è´¥: " + os.path.basename(temp_dir) + " - " + str(e))
        
        print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† " + str(cleaned_count) + " ä¸ªæ–‡ä»¶/ç›®å½•")
