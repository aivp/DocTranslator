#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§æ–‡ä»¶PDFç¿»è¯‘å™¨
åŸºäºbig_pdf_transé¡¹ç›®é›†æˆï¼Œæ”¯æŒå¤šçº¿ç¨‹ç¿»è¯‘ï¼Œæ¯5é¡µåˆ†æ‰¹å¤„ç†ï¼Œæœ€ååˆå¹¶æˆå®Œæ•´PDF
"""

import fitz
from app.utils.pymupdf_queue import (
    safe_fitz_open, safe_fitz_close, safe_fitz_save, 
    safe_fitz_new_document, safe_fitz_insert_pdf,
    safe_fitz_get_text_blocks, safe_fitz_insert_text,
    safe_fitz_insert_textbox, safe_fitz_new_rect,
    PyMuPDFContext
)
import os
import json
import logging
import tempfile
import shutil
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from multiprocessing import Process, Queue, cpu_count
from multiprocessing import set_start_method
import ctypes

# è®¾ç½®è¿›ç¨‹å¯åŠ¨æ–¹æ³•ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ä¸­è®¾ç½®ä¸€æ¬¡ï¼‰
try:
    set_start_method('spawn', force=True)
except RuntimeError:
    pass  # å·²ç»è®¾ç½®è¿‡äº†

logger = logging.getLogger(__name__)

def force_memory_release():
    """å¼ºåˆ¶é‡Šæ”¾å†…å­˜åˆ°æ“ä½œç³»ç»Ÿ"""
    try:
        import gc
        gc.collect()
        # å°è¯•è°ƒç”¨glibcçš„malloc_trimé‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
        try:
            libc = ctypes.CDLL("libc.so.6")
            libc.malloc_trim(0)
            logger.info("ğŸ§¹ å·²è°ƒç”¨malloc_trimé‡Šæ”¾å†…å­˜")
        except Exception as e:
            logger.debug(f"malloc_trimä¸å¯ç”¨: {e}")
    except Exception as e:
        logger.warning(f"å¼ºåˆ¶é‡Šæ”¾å†…å­˜å¤±è´¥: {e}")

# å…¨å±€è¿›ç¨‹æ± ç®¡ç†å™¨
_process_pool = None
_process_pool_lock = threading.Lock()

def get_process_pool():
    """è·å–å…¨å±€è¿›ç¨‹æ± ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _process_pool
    with _process_pool_lock:
        if _process_pool is None:
            # é™åˆ¶è¿›ç¨‹æ± å¤§å°ä¸ºCPUæ ¸å¿ƒæ•°ï¼Œé¿å…èµ„æºè€—å°½
            max_processes = min(cpu_count(), 4)  # æœ€å¤š4ä¸ªè¿›ç¨‹
            _process_pool = ThreadPoolExecutor(max_workers=max_processes)
            logger.info(f"åˆ›å»ºè¿›ç¨‹æ± ï¼Œæœ€å¤§è¿›ç¨‹æ•°: {max_processes}")
        return _process_pool

def shutdown_process_pool():
    """å…³é—­å…¨å±€è¿›ç¨‹æ± """
    global _process_pool
    with _process_pool_lock:
        if _process_pool is not None:
            try:
                _process_pool.shutdown(wait=True)
                logger.info("å…¨å±€è¿›ç¨‹æ± å·²å…³é—­")
            except Exception as e:
                logger.warning(f"å…³é—­è¿›ç¨‹æ± å¤±è´¥: {e}")
            finally:
                _process_pool = None


def _merge_pdfs_in_process(batch_results, output_path, input_pdf_path, temp_dir):
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­åˆå¹¶PDFï¼ˆé¿å…é˜»å¡ä¸»çº¿ç¨‹ï¼‰
    
    Args:
        batch_results: æ‰¹æ¬¡å¤„ç†ç»“æœåˆ—è¡¨
        output_path: æœ€ç»ˆè¾“å‡ºè·¯å¾„
        input_pdf_path: åŸå§‹PDFè·¯å¾„
        temp_dir: ä¸´æ—¶æ–‡ä»¶ç›®å½•
    """
    try:
        logger.info("è¿›ç¨‹å¼€å§‹åˆå¹¶ç¿»è¯‘åçš„PDF...")
        
        # è¿‡æ»¤æˆåŠŸçš„æ‰¹æ¬¡
        successful_batches = [r for r in batch_results if r["status"] == "success"]
        
        if not successful_batches:
            raise Exception("æ²¡æœ‰æˆåŠŸçš„æ‰¹æ¬¡å¯ä»¥åˆå¹¶")
        
        successful_batches.sort(key=lambda x: x["start_page"])
        
        # åˆ›å»ºåˆå¹¶æ–‡æ¡£
        merged_doc = safe_fitz_new_document()
        batch_docs = []
        
        # æµå¼åˆå¹¶ï¼Œé¿å…åŒæ—¶æ‰“å¼€æ‰€æœ‰PDF
        for batch_result in successful_batches:
            translated_pdf_path = batch_result["translated_pdf_path"]
            if os.path.exists(translated_pdf_path):
                batch_doc = safe_fitz_open(translated_pdf_path)
                batch_docs.append(batch_doc)
                safe_fitz_insert_pdf(merged_doc, batch_doc, 0, batch_doc.page_count)
                logger.info(f"åˆå¹¶æ‰¹æ¬¡ {batch_result['batch_num']}: é¡µé¢ {batch_result['start_page']}-{batch_result['end_page']-1}")
                
                # æ¯åˆå¹¶å‡ ä¸ªæ‰¹æ¬¡åå¼ºåˆ¶åƒåœ¾å›æ”¶
                if len(batch_docs) % 5 == 0:
                    import gc
                    gc.collect()
                    logger.debug(f"å·²åˆå¹¶ {len(batch_docs)} ä¸ªæ‰¹æ¬¡ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶")
        
        # ä¿å­˜åˆå¹¶åçš„PDF
        merged_doc.save(output_path)
        
        # åˆ é™¤åŸå§‹æ–‡ä»¶
        if os.path.exists(input_pdf_path) and input_pdf_path != output_path:
            try:
                os.remove(input_pdf_path)
                logger.info(f"å·²åˆ é™¤åŸå§‹æ–‡ä»¶: {input_pdf_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤åŸå§‹æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
        
        logger.info(f"è¿›ç¨‹å®ŒæˆPDFåˆå¹¶: {output_path}")
        
    except Exception as e:
        logger.error(f"è¿›ç¨‹PDFåˆå¹¶å¤±è´¥: {e}")
    finally:
        # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£å¯¹è±¡è¢«æ­£ç¡®å…³é—­
        if 'merged_doc' in locals() and merged_doc is not None:
            try:
                merged_doc.close()
            except Exception as e:
                logger.warning(f"å…³é—­åˆå¹¶PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")
        
        for batch_doc in batch_docs:
            try:
                safe_fitz_close(batch_doc)
            except Exception as e:
                logger.warning(f"å…³é—­æ‰¹æ¬¡PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")


def _compress_pdf_in_process(input_pdf_path, output_pdf_path):
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å‹ç¼©PDFï¼ˆé¿å…é˜»å¡ä¸»çº¿ç¨‹ï¼‰
    
    Args:
        input_pdf_path: è¾“å…¥PDFè·¯å¾„
        output_pdf_path: è¾“å‡ºPDFè·¯å¾„
    """
    doc = None
    try:
        original_size = os.path.getsize(input_pdf_path)
        
        doc = fitz.open(input_pdf_path)
        doc.save(
            output_pdf_path,
            garbage=4,
            deflate=True,
            clean=True,
            encryption=fitz.PDF_ENCRYPT_NONE
        )
        
        compressed_size = os.path.getsize(output_pdf_path)
        compression_ratio = (1 - compressed_size / original_size) * 100
        
        logger.info(f"è¿›ç¨‹å®ŒæˆPDFå‹ç¼©: {original_size:,} â†’ {compressed_size:,} å­—èŠ‚ (å‹ç¼©ç‡: {compression_ratio:.1f}%)")
        return True
        
    except Exception as e:
        logger.error(f"è¿›ç¨‹PDFå‹ç¼©å¤±è´¥: {e}")
        return False
    finally:
        if doc is not None:
            try:
                doc.close()
            except Exception as e:
                logger.warning(f"å…³é—­å‹ç¼©PDFæ–‡æ¡£æ—¶å‡ºé”™: {e}")


class LargePDFTranslator:
    def __init__(self, input_pdf_path, batch_size=5, max_workers=10, target_lang='zh', temp_dir=None, user_id=None):
        """
        åˆå§‹åŒ–å¤§æ–‡ä»¶PDFç¿»è¯‘å™¨
        
        Args:
            input_pdf_path: è¾“å…¥PDFæ–‡ä»¶è·¯å¾„
            batch_size: æ¯æ‰¹å¤„ç†çš„é¡µæ•°ï¼Œé»˜è®¤5é¡µ
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤10
            target_lang: ç›®æ ‡è¯­è¨€ï¼Œé»˜è®¤ä¸­æ–‡
            temp_dir: ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼Œé»˜è®¤ä¸ºç³»ç»Ÿä¸´æ—¶ç›®å½•
            user_id: ç”¨æˆ·IDï¼Œç”¨äºä¸´æ—¶æ–‡ä»¶éš”ç¦»
        """
        self.input_pdf_path = input_pdf_path
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.target_lang = target_lang
        self.user_id = user_id
        self.doc = None
        self.total_pages = 0
        self.processed_batches = []
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ï¼ŒæŒ‰ç”¨æˆ·IDéš”ç¦»
        if temp_dir is None:
            # è·å–uploadsåŸºç¡€ç›®å½•ï¼ˆé¿å…åœ¨ç”¨æˆ·çš„ä¸Šä¼ æ–‡ä»¶ç›®å½•ä¸­åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼‰
            # è¾“å…¥æ–‡ä»¶è·¯å¾„æ ¼å¼ï¼š/app/storage/uploads/tenant_1/user_2/2025-10-21/file.pdf
            # ä¸´æ—¶æ–‡ä»¶åº”åˆ›å»ºåœ¨ï¼š/app/storage/uploads/temp_user_2/translate_xxx/
            base_dir = os.path.dirname(input_pdf_path)  # /app/storage/uploads/tenant_1/user_2/2025-10-21
            date_dir = os.path.dirname(base_dir)  # /app/storage/uploads/tenant_1/user_2
            user_dir = os.path.dirname(date_dir)  # /app/storage/uploads/tenant_1
            uploads_base = os.path.dirname(user_dir)  # /app/storage/uploads
            
            if user_id:
                # ä½¿ç”¨ç”¨æˆ·IDåˆ›å»ºéš”ç¦»ç›®å½•
                temp_dir = os.path.join(uploads_base, f"temp_user_{user_id}", f"translate_{uuid.uuid4().hex[:8]}")
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·IDï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼
                temp_dir = os.path.join(uploads_base, f"temp_translate_{uuid.uuid4().hex[:8]}")
        
        self.temp_dir = temp_dir
        os.makedirs(self.temp_dir, exist_ok=True)
        logger.info(f"ä¸´æ—¶æ–‡ä»¶ç›®å½•: {self.temp_dir}")
        logger.info(f"ç”¨æˆ·ID: {user_id if user_id else 'N/A'}")
        logger.info(f"æ‰¹å¤„ç†å¤§å°: {self.batch_size} é¡µ/æ‰¹")
        logger.info(f"æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers}")
        
        # çº¿ç¨‹å®‰å…¨é”
        self.lock = threading.Lock()
    
    def __del__(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
            try:
                shutil.rmtree(self.temp_dir)
                logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
                
                # å¦‚æœç”¨æˆ·éš”ç¦»ç›®å½•ä¸ºç©ºï¼Œä¹Ÿæ¸…ç†æ‰
                if self.user_id:
                    user_temp_dir = os.path.dirname(self.temp_dir)
                    if os.path.exists(user_temp_dir) and not os.listdir(user_temp_dir):
                        shutil.rmtree(user_temp_dir)
                        logger.info(f"å·²æ¸…ç†ç©ºçš„ç”¨æˆ·éš”ç¦»ç›®å½•: {user_temp_dir}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
    
    def get_total_pages(self):
        """è·å–PDFæ€»é¡µæ•°"""
        try:
            doc = fitz.open(self.input_pdf_path)
            total_pages = doc.page_count
            doc.close()
            return total_pages
        except Exception as e:
            logger.error(f"è·å–PDFé¡µæ•°å¤±è´¥: {e}")
            raise
    
    def extract_texts_from_pages(self, start_page, end_page):
        """
        ä»æŒ‡å®šé¡µé¢èŒƒå›´æå–æ–‡æœ¬
        
        Args:
            start_page: èµ·å§‹é¡µï¼ˆä»0å¼€å§‹ï¼‰
            end_page: ç»“æŸé¡µï¼ˆä¸åŒ…å«ï¼‰
        
        Returns:
            list: æå–çš„æ–‡æœ¬æ•°æ®
        """
        doc = None
        try:
            doc = fitz.open(self.input_pdf_path)
            extracted_texts = []
            
            for page_num in range(start_page, min(end_page, doc.page_count)):
                page = doc[page_num]
                logger.info(f"æå–ç¬¬ {page_num + 1} é¡µæ–‡æœ¬...")
                
                # æå–æ–‡æœ¬å—
                text_blocks = page.get_text("dict", flags=fitz.TEXTFLAGS_TEXT)["blocks"]
                page_texts = []
                
                for block in text_blocks:
                    if "lines" in block:
                        for line in block["lines"]:
                            for span in line["spans"]:
                                text = span["text"].strip()
                                if text:
                                    text_info = {
                                        "text": text,
                                        "bbox": span["bbox"],
                                        "size": span["size"],
                                        "color": span["color"],
                                        "font": span["font"]
                                    }
                                    page_texts.append(text_info)
                
                page_data = {
                    "page_number": page_num,
                    "texts": page_texts
                }
                extracted_texts.append(page_data)
                logger.info(f"ç¬¬ {page_num + 1} é¡µæå–äº† {len(page_texts)} ä¸ªæ–‡æœ¬å—")
            
            return extracted_texts
            
        except Exception as e:
            logger.error(f"æå–æ–‡æœ¬å¤±è´¥: {e}")
            raise
        finally:
            # ç¡®ä¿æ–‡æ¡£å¯¹è±¡è¢«æ­£ç¡®å…³é—­
            if doc is not None:
                try:
                    doc.close()
                    logger.debug("PDFæ–‡æ¡£å·²å…³é—­")
                except Exception as close_error:
                    logger.warning(f"å…³é—­PDFæ–‡æ¡£æ—¶å‡ºé”™: {close_error}")
    
    def translate_texts_batch(self, extracted_texts, trans):
        """
        å¤šçº¿ç¨‹ç¿»è¯‘æ–‡æœ¬æ‰¹æ¬¡
        
        Args:
            extracted_texts: æå–çš„æ–‡æœ¬æ•°æ®
            trans: ç¿»è¯‘ä»»åŠ¡ä¿¡æ¯
        
        Returns:
            list: ç¿»è¯‘åçš„æ–‡æœ¬æ•°æ®
        """
        try:
            translated_texts = []
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼Œå»é‡
            unique_texts = []
            text_to_index = {}
            
            for page_data in extracted_texts:
                page_num = page_data["page_number"]
                for text_info in page_data["texts"]:
                    original_text = text_info["text"]
                    if original_text and original_text.strip():
                        if original_text not in text_to_index:
                            text_to_index[original_text] = len(unique_texts)
                            unique_texts.append(original_text)
            
            logger.info(f"æ‰¹æ¬¡ä¸­å…±æœ‰ {len(unique_texts)} ä¸ªå”¯ä¸€æ–‡æœ¬éœ€è¦ç¿»è¯‘")
            
            # å¤šçº¿ç¨‹ç¿»è¯‘å”¯ä¸€æ–‡æœ¬
            translated_results = {}
            if unique_texts:
                # è®°å½•ç¿»è¯‘å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # ä½¿ç”¨é…ç½®çš„çº¿ç¨‹æ•°
                actual_workers = min(self.max_workers, len(unique_texts))
                logger.info(f"ä½¿ç”¨ {actual_workers} ä¸ªçº¿ç¨‹è¿›è¡Œç¿»è¯‘")
                
                # ä½¿ç”¨ä¸å°PDFå®Œå…¨ç›¸åŒçš„å¹¶å‘æ–¹å¼ï¼Œä½†é¿å…è¿›åº¦å†²çª
                import threading
                from .to_translate import get
                
                # æ ‡è®°ä¸ºå¤§PDFç¿»è¯‘ï¼Œé¿å… to_translate.py ä¸­çš„è¿›åº¦æ›´æ–°
                trans['is_large_pdf'] = True
                
                # åˆ›å»ºæ–‡æœ¬æ•°ç»„ï¼Œæ ¼å¼ä¸å°PDFä¸€è‡´
                texts = []
                for text in unique_texts:
                    texts.append({'text': text, 'complete': False})
                
                # çº¿ç¨‹å¯åŠ¨æ§åˆ¶å˜é‡ï¼Œä¸å°PDFä¿æŒä¸€è‡´
                event = threading.Event()
                run_index = 0
                active_count = threading.activeCount()
                max_threads = actual_workers
                
                logger.info(f"å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µï¼Œä½¿ç”¨ {max_threads} ä¸ªçº¿ç¨‹")
                
                # å¯åŠ¨çº¿ç¨‹ï¼Œä¸å°PDFä¿æŒå®Œå…¨ä¸€è‡´çš„æ–¹å¼
                while run_index < len(texts):
                    if threading.activeCount() < max_threads + active_count and not event.is_set():
                        thread = threading.Thread(
                            target=get,
                            args=(trans, event, texts, run_index)
                        )
                        thread.start()
                        logger.info(f"å¯åŠ¨ç¿»è¯‘çº¿ç¨‹ {run_index}")
                        run_index += 1
                    time.sleep(0.1)  # ä¸å°PDFä¿æŒç›¸åŒçš„å»¶è¿Ÿ
                
                # ç­‰å¾…ç¿»è¯‘å®Œæˆï¼Œä¸å°PDFä¿æŒä¸€è‡´
                while not all(t.get('complete') for t in texts) and not event.is_set():
                    time.sleep(0.1)
                
                # æ”¶é›†ç¿»è¯‘ç»“æœ
                for i, text_item in enumerate(texts):
                    original_text = unique_texts[i]
                    translated_text = text_item['text']
                    translated_results[original_text] = translated_text
                    
                    # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
                    if original_text != translated_text:
                        logger.info(f"ç¿»è¯‘æˆåŠŸ ({i+1}/{len(texts)}): '{original_text[:20]}...' -> '{translated_text[:20]}...'")
                    else:
                        logger.warning(f"ç¿»è¯‘ç»“æœä¸åŸæ–‡ç›¸åŒ ({i+1}/{len(texts)}): '{original_text[:20]}...'")
                
                # æ³¨æ„ï¼šè¿™é‡Œä¸è°ƒç”¨ to_translate.py çš„ process å‡½æ•°ï¼Œé¿å…è¿›åº¦å†²çª
                # å¤§PDFç¿»è¯‘æœ‰è‡ªå·±çš„è¿›åº¦æ›´æ–°æœºåˆ¶
                
                total_time = time.time() - start_time
                logger.info(f"æ‰¹æ¬¡ç¿»è¯‘å®Œæˆï¼Œå…± {len(unique_texts)} ä¸ªæ–‡æœ¬ï¼Œæ€»ç”¨æ—¶: {total_time:.1f}s")
            
            # é‡æ–°ç»„ç»‡ç¿»è¯‘ç»“æœ
            for page_data in extracted_texts:
                page_num = page_data["page_number"]
                translated_page_data = {"page_number": page_num, "texts": []}
                
                for text_info in page_data["texts"]:
                    original_text = text_info["text"]
                    if original_text and original_text.strip():
                        # è·å–ç¿»è¯‘ç»“æœ
                        translated_text = translated_results.get(original_text, original_text)
                        
                        # æ·»åŠ è°ƒè¯•æ—¥å¿—
                        if original_text != translated_text:
                            logger.debug(f"ç¿»è¯‘æ˜ å°„: '{original_text[:30]}...' -> '{translated_text[:30]}...'")
                        else:
                            logger.warning(f"æœªæ‰¾åˆ°ç¿»è¯‘ç»“æœï¼Œä½¿ç”¨åŸæ–‡: '{original_text[:30]}...'")
                        
                        # åˆ›å»ºç¿»è¯‘åçš„æ–‡æœ¬ä¿¡æ¯
                        translated_text_info = text_info.copy()
                        translated_text_info["text"] = translated_text
                        translated_text_info["original_text"] = original_text
                        translated_page_data["texts"].append(translated_text_info)
                    else:
                        translated_page_data["texts"].append(text_info)
                
                translated_texts.append(translated_page_data)
            
            return translated_texts
            
        except Exception as e:
            logger.error(f"ç¿»è¯‘æ–‡æœ¬å¤±è´¥: {e}")
            raise
    
    def translate_single_text_with_delay(self, text, trans, delay):
        """
        å¸¦å»¶è¿Ÿçš„ç¿»è¯‘å•ä¸ªæ–‡æœ¬
        
        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            trans: ç¿»è¯‘ä»»åŠ¡ä¿¡æ¯
            delay: å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            str: ç¿»è¯‘åçš„æ–‡æœ¬
        """
        thread_id = threading.current_thread().ident
        logger.debug(f"[çº¿ç¨‹{thread_id}] å»¶è¿Ÿ {delay} ç§’åå¼€å§‹ç¿»è¯‘: '{text[:30]}...'")
        
        # å»¶è¿Ÿå¯åŠ¨
        if delay > 0:
            time.sleep(delay)
        
        # è¿™ä¸ªæ–¹æ³•å·²ç»ä¸å†ä½¿ç”¨ï¼Œç›´æ¥è¿”å›åŸæ–‡
        return text
    
    
    def create_no_text_pdf(self, start_page, end_page, output_path):
        """
        åˆ›å»ºæ— æ–‡æœ¬PDF
        
        Args:
            start_page: èµ·å§‹é¡µï¼ˆä»0å¼€å§‹ï¼‰
            end_page: ç»“æŸé¡µï¼ˆä¸åŒ…å«ï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        doc = None
        no_text_doc = None
        try:
            doc = fitz.open(self.input_pdf_path)
            no_text_doc = fitz.open()
            
            for page_num in range(start_page, min(end_page, doc.page_count)):
                page = doc[page_num]
                new_page = no_text_doc.new_page(width=page.rect.width, height=page.rect.height)
                
                # å¤åˆ¶é¡µé¢å†…å®¹ï¼ˆå›¾ç‰‡ã€èƒŒæ™¯ç­‰ï¼‰- ä¿ç•™åŸæ–‡æœ¬ï¼Œä¸åˆ é™¤
                # è¿™æ ·å¯ä»¥é¿å…é«˜å†…å­˜æ¶ˆè€—çš„redactionæ“ä½œï¼Œé™ä½å´©æºƒé£é™©
                new_page.show_pdf_page(page.rect, doc, page_num)
                
                # æ³¨æ„ï¼šä¸å†æ‰§è¡Œåˆ é™¤æ–‡æœ¬æ“ä½œï¼ˆredactionï¼‰ï¼Œå› ä¸ºï¼š
                # 1. redactionæ“ä½œä¼šæ¶ˆè€—å¤§é‡å†…å­˜
                # 2. å¯èƒ½å¯¼è‡´PyMuPDFåº•å±‚å´©æºƒ
                # 3. ä¿ç•™åŸæ–‡æœ¬ä¸ä¼šå½±å“ç¿»è¯‘ç»“æœï¼ˆç¿»è¯‘æ–‡æœ¬ä¼šè¦†ç›–åœ¨åŸæ–‡ä¸Šæ–¹ï¼‰
                logger.debug(f"ç¬¬ {page_num + 1} é¡µå·²å¤åˆ¶ï¼ˆä¿ç•™åŸæ–‡æœ¬ï¼Œä¸åˆ é™¤ï¼‰")
            
            no_text_doc.save(output_path)
            logger.info(f"[no_text] æ— æ–‡æœ¬PDFä¿å­˜å®Œæˆ: {output_path}")
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦æ­£å¸¸
            if not os.path.exists(output_path):
                logger.error(f"[no_text] æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {output_path}")
                return False
            
            file_size = os.path.getsize(output_path)
            logger.info(f"[no_text] ä¿å­˜åæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            
            # å°è¯•éªŒè¯PDFæ˜¯å¦å¯ä»¥æ­£å¸¸æ‰“å¼€ï¼ˆä½†ä¸ä¿ç•™å¼•ç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼ï¼‰
            test_doc = None
            try:
                test_doc = fitz.open(output_path)
                test_page_count = test_doc.page_count
                logger.info(f"[no_text] PDFéªŒè¯æˆåŠŸï¼Œé¡µæ•°: {test_page_count}")
            except Exception as verify_error:
                logger.error(f"[no_text] PDFéªŒè¯å¤±è´¥ï¼Œæ–‡ä»¶å¯èƒ½æŸå: {verify_error}")
                return False
            finally:
                # ç«‹å³å…³é—­éªŒè¯æ–‡æ¡£ï¼Œé‡Šæ”¾å†…å­˜
                if test_doc is not None:
                    try:
                        test_doc.close()
                    except:
                        pass
                # éªŒè¯åç«‹å³æ¸…ç†å†…å­˜
                import gc
                gc.collect()
            
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ— æ–‡æœ¬PDFå¤±è´¥: {e}")
            return False
        finally:
            # ç¡®ä¿æ‰€æœ‰æ–‡æ¡£å¯¹è±¡è¢«æ­£ç¡®å…³é—­
            if no_text_doc is not None:
                try:
                    no_text_doc.close()
                    logger.debug("æ— æ–‡æœ¬PDFæ–‡æ¡£å·²å…³é—­")
                except Exception as close_error:
                    logger.warning(f"å…³é—­æ— æ–‡æœ¬PDFæ–‡æ¡£æ—¶å‡ºé”™: {close_error}")
            
            if doc is not None:
                try:
                    doc.close()
                    logger.debug("åŸå§‹PDFæ–‡æ¡£å·²å…³é—­")
                except Exception as close_error:
                    logger.warning(f"å…³é—­åŸå§‹PDFæ–‡æ¡£æ—¶å‡ºé”™: {close_error}")
    
    def fill_translated_texts_to_pdf(self, translated_texts, no_text_pdf_path, output_path):
        """
        å°†ç¿»è¯‘åçš„æ–‡æœ¬å¡«å……åˆ°PDF
        
        Args:
            translated_texts: ç¿»è¯‘åçš„æ–‡æœ¬æ•°æ®
            no_text_pdf_path: æ— æ–‡æœ¬PDFè·¯å¾„
            output_path: è¾“å‡ºPDFè·¯å¾„
        """
        doc = None
        try:
            # æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼Œå®šä½å´©æºƒç‚¹
            logger.info(f"[fill] å‡†å¤‡æ‰“å¼€æ— æ–‡æœ¬PDF: {no_text_pdf_path}")
            logger.info(f"[fill] æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(no_text_pdf_path)}")
            if os.path.exists(no_text_pdf_path):
                file_size = os.path.getsize(no_text_pdf_path)
                logger.info(f"[fill] æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            
            doc = fitz.open(no_text_pdf_path)
            logger.info(f"[fill] PDFæ‰“å¼€æˆåŠŸï¼Œé¡µæ•°: {doc.page_count}")
            
            # åˆ›å»ºé¡µé¢ç´¢å¼•æ˜ å°„ï¼šåŸå§‹é¡µé¢å· -> æ‰¹æ¬¡å†…é¡µé¢ç´¢å¼•
            page_index_map = {}
            for i, page_data in enumerate(translated_texts):
                original_page_num = page_data["page_number"]
                page_index_map[original_page_num] = i
            
            # æ‰¹é‡å¤„ç†æ–‡æœ¬æ’å…¥ï¼Œæ¯å¤„ç†ä¸€å®šæ•°é‡åæ¸…ç†å†…å­˜
            text_insert_count = 0
            for page_data in translated_texts:
                original_page_num = page_data["page_number"]
                batch_page_index = page_index_map[original_page_num]
                page = doc[batch_page_index]
                logger.info(f"å¡«å……ç¬¬ {original_page_num + 1} é¡µç¿»è¯‘æ–‡æœ¬ (æ‰¹æ¬¡å†…ç´¢å¼•: {batch_page_index})...")
                
                # å…ˆæ”¶é›†è¿™ä¸€é¡µæ‰€æœ‰éœ€è¦åˆ é™¤çš„æ–‡æœ¬åŒºåŸŸå’Œè¦æ’å…¥çš„æ–‡æœ¬
                redact_rects = []
                text_insertions = []
                
                for text_info in page_data["texts"]:
                    # ä½¿ç”¨ç¿»è¯‘åçš„æ–‡æœ¬
                    text = text_info["text"]
                    if text and text.strip():
                        bbox = text_info["bbox"]
                        font_size = text_info["size"]
                        color = text_info["color"]
                        
                        # æ ‡å‡†åŒ–é¢œè‰²
                        if isinstance(color, (int, float)):
                            if color == 0:
                                color = (0, 0, 0)  # é»‘è‰²
                            elif color == 16777215:
                                color = (1, 1, 1)  # ç™½è‰²
                            else:
                                # è½¬æ¢ä¸ºRGB
                                r = ((color >> 16) & 0xFF) / 255.0
                                g = ((color >> 8) & 0xFF) / 255.0
                                b = (color & 0xFF) / 255.0
                                color = (r, g, b)
                        elif isinstance(color, (list, tuple)) and len(color) >= 3:
                            color = tuple(color[:3])
                        else:
                            color = (0, 0, 0)  # é»˜è®¤é»‘è‰²
                        
                        # åˆ›å»ºæ–‡æœ¬æ¡†çŸ©å½¢
                        textbox = fitz.Rect(bbox[0], bbox[1], bbox[2], bbox[3])
                        
                        # æ”¶é›†éœ€è¦åˆ é™¤çš„åŒºåŸŸï¼ˆé¿å…æ–‡æœ¬é‡å ï¼‰
                        redact_rects.append(textbox)
                        
                        # å‡†å¤‡HTMLæ–‡æœ¬
                        html_text = f"""
                        <div style="
                            font-size: {font_size}px;
                            color: rgb({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)});
                            font-family: sans-serif;
                            line-height: 1.0;
                            margin: 0;
                            padding: 0;
                            background: none;
                            background-color: transparent;
                            background-image: none;
                            border: none;
                            outline: none;
                            box-shadow: none;
                        ">
                            {text}
                        </div>
                        """
                        
                        text_insertions.append({
                            'textbox': textbox,
                            'html_text': html_text,
                            'text': text
                        })
                
                # å…ˆåˆ é™¤åŸå§‹æ–‡æœ¬ï¼Œé¿å…é‡å 
                if redact_rects:
                    try:
                        # ä¸ºæ¯ä¸ªæ–‡æœ¬åŒºåŸŸæ·»åŠ redactionæ ‡æ³¨
                        for rect in redact_rects:
                            try:
                                # ä½¿ç”¨é€æ˜å¡«å……ï¼Œé¿å…ç™½è‰²é®æŒ¡
                                page.add_redact_annot(rect, fill=None)
                            except Exception as e:
                                logger.debug(f"æ·»åŠ redactionæ ‡æ³¨å¤±è´¥ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰: {e}")
                                # å¦‚æœå¤±è´¥ï¼Œå°è¯•é€æ˜å¡«å……
                                try:
                                    page.add_redact_annot(rect, fill=(0, 0, 0, 0))
                                except Exception as e2:
                                    logger.debug(f"é€æ˜å¡«å……redactionä¹Ÿå¤±è´¥: {e2}")
                        
                        # åº”ç”¨redactionï¼ŒçœŸæ­£åˆ é™¤æ–‡æœ¬
                        page.apply_redactions()
                        logger.debug(f"ç¬¬ {original_page_num + 1} é¡µå·²åˆ é™¤ {len(redact_rects)} ä¸ªåŸå§‹æ–‡æœ¬åŒºåŸŸ")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤åŸå§‹æ–‡æœ¬å¤±è´¥ï¼Œç»§ç»­æ’å…¥ç¿»è¯‘æ–‡æœ¬: {e}")
                
                # ç„¶åæ’å…¥ç¿»è¯‘åçš„æ–‡æœ¬
                for insertion in text_insertions:
                    try:
                        page.insert_htmlbox(insertion['textbox'], insertion['html_text'])
                        text_insert_count += 1
                        # æ¯æ’å…¥50ä¸ªæ–‡æœ¬æ¸…ç†ä¸€æ¬¡å†…å­˜ï¼Œé¿å…å†…å­˜è¿‡è½½
                        if text_insert_count % 50 == 0:
                            import gc
                            gc.collect()
                            logger.debug(f"[fill] å·²æ’å…¥{text_insert_count}ä¸ªæ–‡æœ¬ï¼Œæ‰§è¡Œå†…å­˜æ¸…ç†")
                        logger.info(f"æ–‡æœ¬æ’å…¥æˆåŠŸ: '{insertion['text'][:20]}...'")
                    except Exception as e:
                        logger.error(f"æ–‡æœ¬æ’å…¥å¤±è´¥: {e}")
                
                # æ¯é¡µå¤„ç†å®Œåæ¸…ç†ä¸´æ—¶æ•°æ®ï¼Œé¿å…å†…å­˜ç´¯ç§¯
                redact_rects.clear()
                text_insertions.clear()
                import gc
                gc.collect()
                logger.debug(f"[fill] ç¬¬ {original_page_num + 1} é¡µå¤„ç†å®Œæˆï¼Œå·²æ¸…ç†å†…å­˜")
            
            # ä¿å­˜å‰æ·»åŠ æ—¥å¿—å’Œå†…å­˜æ£€æŸ¥
            logger.info(f"[fill] å‡†å¤‡ä¿å­˜PDFåˆ°: {output_path}")
            import gc
            gc.collect()  # ä¿å­˜å‰æ¸…ç†å†…å­˜
            
            doc.save(output_path)
            logger.info(f"[fill] PDFä¿å­˜æˆåŠŸ: {output_path}")
            logger.info(f"[fill] ä¿å­˜åæ–‡ä»¶å¤§å°: {os.path.getsize(output_path) if os.path.exists(output_path) else 0} å­—èŠ‚")
            return True
            
        except Exception as e:
            logger.error(f"å¡«å……ç¿»è¯‘æ–‡æœ¬å¤±è´¥: {e}")
            return False
        finally:
            # ç¡®ä¿æ–‡æ¡£å¯¹è±¡è¢«æ­£ç¡®å…³é—­
            if doc is not None:
                try:
                    doc.close()
                    logger.debug("å¡«å……PDFæ–‡æ¡£å·²å…³é—­")
                except Exception as close_error:
                    logger.warning(f"å…³é—­å¡«å……PDFæ–‡æ¡£æ—¶å‡ºé”™: {close_error}")
    
    def compress_pdf(self, input_pdf_path, output_pdf_path, cancel_event=None):
        """
        å‹ç¼©PDFæ–‡ä»¶ï¼ˆä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡å…¶ä»–ä»»åŠ¡ï¼‰
        
        Args:
            input_pdf_path: è¾“å…¥PDFè·¯å¾„
            output_pdf_path: è¾“å‡ºPDFè·¯å¾„
            cancel_event: å–æ¶ˆäº‹ä»¶
        
        Returns:
            bool: å‹ç¼©æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹å‹ç¼©PDFï¼ˆä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹ï¼‰...")
            
            # æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
            if cancel_event and cancel_event.is_set():
                logger.info("PDFå‹ç¼©è¢«å–æ¶ˆ")
                return False
            
            # ä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œå‹ç¼©æ“ä½œï¼Œé¿å…é˜»å¡å…¶ä»–ç¿»è¯‘ä»»åŠ¡
            # è™½ç„¶å½“å‰ä»»åŠ¡ä¼šç­‰å¾…å‹ç¼©å®Œæˆï¼Œä½†å…¶ä»–ä»»åŠ¡ä¸ä¼šå—å½±å“
            process = Process(
                target=_compress_pdf_in_process,
                args=(input_pdf_path, output_pdf_path)
            )
            process.start()
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆï¼Œä½†å®šæœŸæ£€æŸ¥å–æ¶ˆäº‹ä»¶
            while process.is_alive():
                if cancel_event and cancel_event.is_set():
                    logger.info("PDFå‹ç¼©è¢«å–æ¶ˆï¼Œç»ˆæ­¢è¿›ç¨‹")
                    process.terminate()
                    process.join(timeout=5)  # ç­‰å¾…5ç§’è®©è¿›ç¨‹ä¼˜é›…é€€å‡º
                    if process.is_alive():
                        process.kill()  # å¼ºåˆ¶æ€æ­»è¿›ç¨‹
                    return False
                time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
            
            process.join()  # ç¡®ä¿è¿›ç¨‹å®Œå…¨ç»“æŸ
            
            if process.exitcode == 0:
                logger.info("PDFå‹ç¼©å®Œæˆ")
                return True
            else:
                logger.error(f"PDFå‹ç¼©è¿›ç¨‹å¤±è´¥ï¼Œé€€å‡ºç : {process.exitcode}")
                return False
            
        except Exception as e:
            logger.error(f"PDFå‹ç¼©å¤±è´¥: {e}")
            return False
    
    def process_batch_with_delay(self, batch_num, start_page, end_page, trans, delay):
        """
        å¸¦å»¶è¿Ÿçš„æ‰¹æ¬¡å¤„ç†
        
        Args:
            batch_num: æ‰¹æ¬¡å·
            start_page: èµ·å§‹é¡µ
            end_page: ç»“æŸé¡µ
            trans: ç¿»è¯‘ä»»åŠ¡ä¿¡æ¯
            delay: å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        thread_id = threading.current_thread().ident
        logger.info(f"[çº¿ç¨‹{thread_id}] æ‰¹æ¬¡ {batch_num} å»¶è¿Ÿ {delay} ç§’åå¼€å§‹å¤„ç†")
        
        # å»¶è¿Ÿå¯åŠ¨
        if delay > 0:
            time.sleep(delay)
        
        return self.process_batch(batch_num, start_page, end_page, trans)
    
    def process_batch(self, batch_num, start_page, end_page, trans):
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡
        
        Args:
            batch_num: æ‰¹æ¬¡å·
            start_page: èµ·å§‹é¡µ
            end_page: ç»“æŸé¡µ
            trans: ç¿»è¯‘ä»»åŠ¡ä¿¡æ¯
        
        Returns:
            dict: å¤„ç†ç»“æœ
        """
        try:
            thread_id = threading.current_thread().ident
            logger.info(f"[çº¿ç¨‹{thread_id}] å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num}: é¡µé¢ {start_page}-{end_page-1}")
            
            # 1. æå–æ–‡æœ¬
            extracted_texts = self.extract_texts_from_pages(start_page, end_page)
            
            # 2. å¤šçº¿ç¨‹ç¿»è¯‘æ–‡æœ¬
            translated_texts = self.translate_texts_batch(extracted_texts, trans)
            
            # 3. åˆ›å»ºæ— æ–‡æœ¬PDF
            no_text_pdf_path = os.path.join(self.temp_dir, f"batch_{batch_num}_no_text.pdf")
            if not self.create_no_text_pdf(start_page, end_page, no_text_pdf_path):
                raise Exception("åˆ›å»ºæ— æ–‡æœ¬PDFå¤±è´¥")
            
            # 4. å¡«å……ç¿»è¯‘æ–‡æœ¬
            translated_pdf_path = os.path.join(self.temp_dir, f"batch_{batch_num}_translated.pdf")
            if not self.fill_translated_texts_to_pdf(translated_texts, no_text_pdf_path, translated_pdf_path):
                raise Exception("å¡«å……ç¿»è¯‘æ–‡æœ¬å¤±è´¥")
            
            # 5. å‹ç¼©å•ä¸ªæ‰¹æ¬¡PDF
            compressed_pdf_path = os.path.join(self.temp_dir, f"batch_{batch_num}_compressed.pdf")
            if not self.compress_pdf(translated_pdf_path, compressed_pdf_path, trans.get('cancel_event')):
                logger.warning(f"æ‰¹æ¬¡ {batch_num} å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡ä»¶")
                compressed_pdf_path = translated_pdf_path
            
            # 6. åˆ é™¤ä¸­é—´æ–‡ä»¶ï¼Œåªä¿ç•™å‹ç¼©æ–‡ä»¶
            try:
                # åˆ é™¤æ— æ–‡æœ¬PDF
                if os.path.exists(no_text_pdf_path):
                    os.remove(no_text_pdf_path)
                    logger.debug(f"å·²åˆ é™¤æ— æ–‡æœ¬PDF: {no_text_pdf_path}")
                
                # åˆ é™¤ç¿»è¯‘PDF
                if os.path.exists(translated_pdf_path) and compressed_pdf_path != translated_pdf_path:
                    os.remove(translated_pdf_path)
                    logger.debug(f"å·²åˆ é™¤ç¿»è¯‘PDF: {translated_pdf_path}")
                
                logger.info(f"æ‰¹æ¬¡ {batch_num} ä¸­é—´æ–‡ä»¶å·²æ¸…ç†ï¼Œä¿ç•™å‹ç¼©æ–‡ä»¶")
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸­é—´æ–‡ä»¶å¤±è´¥: {e}")
            
            result = {
                "batch_num": batch_num,
                "start_page": start_page,
                "end_page": end_page,
                "translated_pdf_path": compressed_pdf_path,
                "status": "success",
                "thread_id": thread_id
            }
            
            logger.info(f"[çº¿ç¨‹{thread_id}] æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            thread_id = threading.current_thread().ident
            logger.error(f"[çº¿ç¨‹{thread_id}] æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
            return {
                "batch_num": batch_num,
                "start_page": start_page,
                "end_page": end_page,
                "status": "failed",
                "error": str(e),
                "thread_id": thread_id
            }
    
    def merge_translated_pdfs(self, batch_results, output_path, cancel_event=None):
        """
        åˆå¹¶æ‰€æœ‰ç¿»è¯‘åçš„PDFï¼ˆä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡å…¶ä»–ä»»åŠ¡ï¼‰
        
        Args:
            batch_results: æ‰¹æ¬¡å¤„ç†ç»“æœåˆ—è¡¨
            output_path: æœ€ç»ˆè¾“å‡ºè·¯å¾„
            cancel_event: å–æ¶ˆäº‹ä»¶
        """
        try:
            logger.info("å¼€å§‹åˆå¹¶ç¿»è¯‘åçš„PDFï¼ˆä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹ï¼‰...")
            
            # æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
            if cancel_event and cancel_event.is_set():
                logger.info("PDFåˆå¹¶è¢«å–æ¶ˆ")
                return False
            
            # è¿‡æ»¤æˆåŠŸçš„æ‰¹æ¬¡
            successful_batches = [r for r in batch_results if r["status"] == "success"]
            
            if not successful_batches:
                raise Exception("æ²¡æœ‰æˆåŠŸçš„æ‰¹æ¬¡å¯ä»¥åˆå¹¶")
            
            # ä½¿ç”¨ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œåˆå¹¶æ“ä½œï¼Œé¿å…é˜»å¡å…¶ä»–ç¿»è¯‘ä»»åŠ¡
            # è™½ç„¶å½“å‰ä»»åŠ¡ä¼šç­‰å¾…åˆå¹¶å®Œæˆï¼Œä½†å…¶ä»–ä»»åŠ¡ä¸ä¼šå—å½±å“
            process = Process(
                target=_merge_pdfs_in_process,
                args=(successful_batches, output_path, self.input_pdf_path, self.temp_dir)
            )
            process.start()
            
            # ç­‰å¾…è¿›ç¨‹å®Œæˆï¼Œä½†å®šæœŸæ£€æŸ¥å–æ¶ˆäº‹ä»¶
            while process.is_alive():
                if cancel_event and cancel_event.is_set():
                    logger.info("PDFåˆå¹¶è¢«å–æ¶ˆï¼Œç»ˆæ­¢è¿›ç¨‹")
                    process.terminate()
                    process.join(timeout=5)  # ç­‰å¾…5ç§’è®©è¿›ç¨‹ä¼˜é›…é€€å‡º
                    if process.is_alive():
                        process.kill()  # å¼ºåˆ¶æ€æ­»è¿›ç¨‹
                    return False
                time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
            
            process.join()  # ç¡®ä¿è¿›ç¨‹å®Œå…¨ç»“æŸ
            
            if process.exitcode == 0:
                logger.info(f"PDFåˆå¹¶å®Œæˆ: {output_path}")
                return output_path
            else:
                logger.error(f"PDFåˆå¹¶è¿›ç¨‹å¤±è´¥ï¼Œé€€å‡ºç : {process.exitcode}")
                return False
            
        except Exception as e:
            logger.error(f"PDFåˆå¹¶å¤±è´¥: {e}")
            return False
    
    def _cleanup_temp_files(self, temp_files=None, temp_dir=None):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•ï¼Œä¸å°PDFä¿æŒä¸€è‡´"""
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        cleaned_count = 0
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_files:
            for temp_file in temp_files:
                if temp_file and os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                        logger.info(f"âœ… å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {os.path.basename(temp_file)}")
                        cleaned_count += 1
                    except Exception as e:
                        logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_file} - {e}")
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        cleanup_dir = temp_dir or (hasattr(self, 'temp_dir') and self.temp_dir)
        if cleanup_dir and os.path.exists(cleanup_dir):
            try:
                import shutil
                shutil.rmtree(cleanup_dir)
                logger.info(f"âœ… å·²åˆ é™¤ä¸´æ—¶ç›®å½•: {os.path.basename(cleanup_dir)}")
                cleaned_count += 1
                
                # å¦‚æœç”¨æˆ·ä¸´æ—¶ç›®å½•ä¸ºç©ºï¼Œä¹Ÿåˆ é™¤
                if hasattr(self, 'user_id') and self.user_id:
                    user_temp_dir = os.path.dirname(cleanup_dir)
                    if os.path.exists(user_temp_dir) and not os.listdir(user_temp_dir):
                        shutil.rmtree(user_temp_dir)
                        logger.info(f"âœ… å·²æ¸…ç†ç©ºç”¨æˆ·ä¸´æ—¶ç›®å½•: {os.path.basename(user_temp_dir)}")
                        cleaned_count += 1
                        
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶ç›®å½•å¤±è´¥: {cleanup_dir} - {e}")
        
        logger.info(f"ğŸ§¹ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_count} ä¸ªæ–‡ä»¶/ç›®å½•")
    
    def update_progress(self, trans, progress_percentage):
        """æ›´æ–°ç¿»è¯‘è¿›åº¦"""
        try:
            from .to_translate import db
            db.execute("update translate set process=%s where id=%s", 
                     str(format(progress_percentage, '.1f')), 
                     trans['id'])
            logger.info(f"è¿›åº¦æ›´æ–°: {progress_percentage:.1f}%")
        except Exception as e:
            logger.error(f"æ›´æ–°è¿›åº¦å¤±è´¥: {str(e)}")
    
    def run_complete_translation(self, trans, output_file):
        """
        è¿è¡Œå®Œæ•´çš„å¤šçº¿ç¨‹ç¿»è¯‘æµç¨‹
        
        Args:
            trans: ç¿»è¯‘ä»»åŠ¡ä¿¡æ¯
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
        Returns:
            str: æœ€ç»ˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            print("ğŸš€ å¼€å§‹å¤§æ–‡ä»¶å¤šçº¿ç¨‹PDFç¿»è¯‘")
            print("=" * 60)
            
            # è·å–å–æ¶ˆäº‹ä»¶
            cancel_event = trans.get('cancel_event')
            
            # æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
            if cancel_event and cancel_event.is_set():
                print("ç¿»è¯‘ä»»åŠ¡å·²è¢«å–æ¶ˆ")
                return False
            
            # è·å–æ€»é¡µæ•°
            self.total_pages = self.get_total_pages()
            print(f"PDFæ€»é¡µæ•°: {self.total_pages}")
            print(f"æ‰¹å¤„ç†å¤§å°: {self.batch_size} é¡µ/æ‰¹")
            print(f"æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers}")
            
            # è®¡ç®—æ‰¹æ¬¡æ•°
            total_batches = (self.total_pages + self.batch_size - 1) // self.batch_size
            print(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
            
            # è®¡ç®—æ¯ä¸ªæ‰¹æ¬¡åº”è¯¥å çš„ç™¾åˆ†æ¯”ï¼ˆåˆå¹¶å‰å 90%ï¼‰
            batch_progress_percentage = 90.0 / total_batches
            print(f"æ¯ä¸ªæ‰¹æ¬¡è¿›åº¦: {batch_progress_percentage:.1f}%")
            print("=" * 60)
            
            # é¡ºåºå¤„ç†æ¯ä¸ªæ‰¹æ¬¡ï¼Œé¿å…æ‰¹æ¬¡é—´å¹¶å‘
            batch_results = []
            start_time = time.time()
            successful_pages = 0
            
            # é€ä¸ªå¤„ç†æ‰¹æ¬¡ï¼Œç¡®ä¿ä¸€ä¸ªæ‰¹æ¬¡å®Œå…¨å¤„ç†å®Œåå†å¤„ç†ä¸‹ä¸€ä¸ª
            for batch_num in range(total_batches):
                # æ£€æŸ¥æ˜¯å¦å·²è¢«å–æ¶ˆ
                if cancel_event and cancel_event.is_set():
                    print("ç¿»è¯‘ä»»åŠ¡å·²è¢«å–æ¶ˆï¼Œåœæ­¢å¤„ç†")
                    return False
                
                start_page = batch_num * self.batch_size
                end_page = min(start_page + self.batch_size, self.total_pages)
                
                print(f"\nğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches} (ç¬¬{start_page + 1}-{end_page}é¡µ)")
                print("=" * 60)
                
                try:
                    # å¤„ç†å½“å‰æ‰¹æ¬¡
                    batch_result = self.process_single_batch(
                        batch_num, 
                        start_page, 
                        end_page,
                        trans
                    )
                    
                    if batch_result:
                        batch_results.append(batch_result)
                        successful_pages += (end_page - start_page)
                        print(f"âœ… æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å®Œæˆ")
                        
                        # æ˜¾ç¤ºè¿›åº¦å¹¶æ›´æ–°æ•°æ®åº“ï¼ˆåˆå¹¶å‰æœ€å¤š90%ï¼‰
                        # ä½¿ç”¨é¢„å…ˆè®¡ç®—çš„æ¯ä¸ªæ‰¹æ¬¡ç™¾åˆ†æ¯”
                        completed_batches = len(batch_results)
                        batch_progress = min(completed_batches * batch_progress_percentage, 90.0)
                        elapsed_time = time.time() - start_time
                        
                        print(f"ğŸ“Š è¿›åº¦è®¡ç®—: {completed_batches} Ã— {batch_progress_percentage:.1f}% = {batch_progress:.1f}% - å·²ç”¨æ—¶: {elapsed_time:.1f}s")
                        
                        # æ›´æ–°æ•°æ®åº“è¿›åº¦
                        self.update_progress(trans, batch_progress)
                    else:
                        print(f"âŒ æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å‡ºé”™: {str(e)}")
                    logger.error(f"æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å‡ºé”™: {str(e)}")
                    continue
            
            # åˆå¹¶æ‰€æœ‰ç¿»è¯‘åçš„PDF
            print(f"\nğŸ”„ åˆå¹¶æ‰€æœ‰ç¿»è¯‘åçš„PDF...")
            # åˆå¹¶å¼€å§‹ï¼Œè¿›åº¦åº”è¯¥æ˜¯90%ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼‰
            print(f"ğŸ“Š è¿›åº¦: åˆå¹¶ä¸­ (90.0%)")
            
            # åœ¨åˆå¹¶è¿‡ç¨‹ä¸­æ›´æ–°è¿›åº¦ä¸º95%ï¼Œè®©ç”¨æˆ·çŸ¥é“ç³»ç»Ÿè¿˜åœ¨å·¥ä½œ
            self.update_progress(trans, 95.0)
            print(f"ğŸ“Š è¿›åº¦: åˆå¹¶ä¸­ (95.0%)")
            
            final_output_file = self.merge_translated_pdfs(batch_results, output_file, cancel_event)
            
            # å†…å­˜ä¼˜åŒ–ï¼šåˆå¹¶åç«‹å³æ¸…ç†batch_results
            try:
                successful_batches_count = len(batch_results)
                batch_results.clear()
                del batch_results
                import gc
                gc.collect()
                logger.info("ğŸ§¹ åˆå¹¶åç«‹å³æ¸…ç†batch_results")
            except Exception as cleanup_error:
                logger.warning(f"æ¸…ç†batch_resultsæ—¶å‡ºé”™: {cleanup_error}")
                successful_batches_count = 0
            
            if final_output_file:
                # åˆå¹¶å®Œæˆï¼Œæ›´æ–°ä¸º100%
                self.update_progress(trans, 100.0)
                total_time = time.time() - start_time
                print(f"âœ… å¤§æ–‡ä»¶å¤šçº¿ç¨‹ç¿»è¯‘å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {final_output_file}")
                print(f"ğŸ“Š è¿›åº¦: å®Œæˆ (100.0%)")
                print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")
                
                # ç»Ÿè®¡ç»“æœ
                successful_batches = successful_batches_count
                failed_batches = total_batches - successful_batches
                
                print("\n" + "=" * 60)
                print("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
                print(f"   æ€»æ‰¹æ¬¡æ•°: {total_batches}")
                print(f"   æˆåŠŸæ‰¹æ¬¡: {successful_batches}")
                print(f"   å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
                print(f"   æˆåŠŸç‡: {successful_batches/total_batches*100:.1f}%")
                print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")
                print(f"   å¹³å‡æ¯æ‰¹æ—¶é—´: {total_time/total_batches:.2f} ç§’")
                print(f"   ä½¿ç”¨çº¿ç¨‹æ•°: {self.max_workers}")
                print("=" * 60)
                
                return final_output_file
            else:
                raise Exception("PDFåˆå¹¶å¤±è´¥")
                
        except Exception as e:
            logger.error(f"å¤§æ–‡ä»¶å¤šçº¿ç¨‹ç¿»è¯‘å¤±è´¥: {e}")
            raise
        finally:
            # æ¸…ç†èµ„æº
            if self.doc:
                self.doc.close()
            
            # å†…å­˜ä¼˜åŒ–ï¼šç¿»è¯‘å®Œæˆåå½»åº•æ¸…ç†æ‰€æœ‰æ•°æ®ç»“æ„
            try:
                # æ¸…ç†å·²å¤„ç†æ‰¹æ¬¡åˆ—è¡¨
                if hasattr(self, 'processed_batches'):
                    self.processed_batches.clear()
                    del self.processed_batches
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                # å¼ºåˆ¶é‡Šæ”¾å†…å­˜åˆ°æ“ä½œç³»ç»Ÿ
                force_memory_release()
                
                logger.info("ğŸ§¹ ç¿»è¯‘å®Œæˆåå†…å­˜å·²å½»åº•æ¸…ç†")
            except Exception as cleanup_error:
                logger.warning(f"æ¸…ç†ç¿»è¯‘å®Œæˆåçš„å†…å­˜æ—¶å‡ºé”™: {cleanup_error}")
    
    def process_single_batch(self, batch_num, start_page, end_page, trans):
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼šæå–æ–‡æœ¬ -> ç¿»è¯‘ -> ç”ŸæˆPDF -> å‹ç¼© -> åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        
        Args:
            batch_num: æ‰¹æ¬¡å·
            start_page: èµ·å§‹é¡µ
            end_page: ç»“æŸé¡µ
            trans: ç¿»è¯‘ä»»åŠ¡ä¿¡æ¯
            
        Returns:
            dict: æ‰¹æ¬¡å¤„ç†ç»“æœ
        """
        try:
            print(f"ğŸ“„ æå–ç¬¬{start_page+1}-{end_page}é¡µæ–‡æœ¬...")
            
            # 1. æå–æ–‡æœ¬
            extracted_texts = self.extract_texts_from_pages(start_page, end_page)
            if not extracted_texts:
                print(f"âš ï¸ ç¬¬{start_page+1}-{end_page}é¡µæ²¡æœ‰æå–åˆ°æ–‡æœ¬")
                return None
            
            # 2. ç¿»è¯‘æ–‡æœ¬
            print(f"ğŸ”„ å¼€å§‹ç¿»è¯‘ç¬¬{start_page+1}-{end_page}é¡µ...")
            translated_texts = self.translate_texts_batch(extracted_texts, trans)
            
            # 3. ç”Ÿæˆç¿»è¯‘åçš„PDF
            batch_output_path = os.path.join(self.temp_dir, f"batch_{batch_num}_translated.pdf")
            print(f"ğŸ“ ç”Ÿæˆç¿»è¯‘åçš„PDF: {batch_output_path}")
            
            # å…ˆåˆ›å»ºæ— æ–‡æœ¬PDF
            no_text_pdf_path = os.path.join(self.temp_dir, f"batch_{batch_num}_no_text.pdf")
            logger.info(f"[batch_{batch_num}] å¼€å§‹åˆ›å»ºé™„åŠ æ–‡æœ¬PDF...")
            if not self.create_no_text_pdf(start_page, end_page, no_text_pdf_path):
                raise Exception("åˆ›å»ºæ— æ–‡æœ¬PDFå¤±è´¥")
            
            # åˆ›å»ºå®Œæˆåç«‹å³æ¸…ç†å†…å­˜ï¼Œé¿å…åç»­æ­¥éª¤å†…å­˜ä¸è¶³
            logger.info(f"[batch_{batch_num}] æ— æ–‡æœ¬PDFåˆ›å»ºå®Œæˆï¼Œæ¸…ç†å†…å­˜...")
            import gc
            gc.collect()
            logger.info(f"[batch_{batch_num}] å†…å­˜æ¸…ç†å®Œæˆï¼Œå¼€å§‹å¡«å……ç¿»è¯‘æ–‡æœ¬...")
            
            # å¡«å……ç¿»è¯‘æ–‡æœ¬
            if not self.fill_translated_texts_to_pdf(translated_texts, no_text_pdf_path, batch_output_path):
                raise Exception("å¡«å……ç¿»è¯‘æ–‡æœ¬å¤±è´¥")
            
            logger.info(f"[batch_{batch_num}] ç¿»è¯‘æ–‡æœ¬å¡«å……å®Œæˆ")
            
            # 4. å‹ç¼©PDF
            compressed_path = batch_output_path.replace('.pdf', '_compressed.pdf')
            print(f"ğŸ—œï¸ å‹ç¼©PDF: {compressed_path}")
            
            self.compress_pdf(batch_output_path, compressed_path, trans.get('cancel_event'))
            
            # 5. åˆ é™¤æœªå‹ç¼©çš„ä¸´æ—¶æ–‡ä»¶ï¼Œåªä¿ç•™å‹ç¼©æ–‡ä»¶
            if os.path.exists(batch_output_path):
                os.remove(batch_output_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æœªå‹ç¼©çš„ä¸´æ—¶æ–‡ä»¶: {batch_output_path}")
            
            # åˆ é™¤æ— æ–‡æœ¬PDFæ–‡ä»¶
            if os.path.exists(no_text_pdf_path):
                os.remove(no_text_pdf_path)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ— æ–‡æœ¬PDFæ–‡ä»¶: {no_text_pdf_path}")
            
            # å†…å­˜ä¼˜åŒ–ï¼šç«‹å³æ¸…ç†æ‰¹æ¬¡å¤„ç†ä¸­çš„ä¸´æ—¶æ•°æ®
            try:
                # æ¸…ç†æ–‡æœ¬æ•°æ®
                extracted_texts.clear()
                del extracted_texts
                translated_texts.clear()
                del translated_texts
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                print(f"ğŸ§¹ æ‰¹æ¬¡ {batch_num + 1} å†…å­˜å·²æ¸…ç†")
            except Exception as cleanup_error:
                print(f"âš ï¸ æ¸…ç†æ‰¹æ¬¡å†…å­˜æ—¶å‡ºé”™: {cleanup_error}")
            
            print(f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œä¿ç•™å‹ç¼©æ–‡ä»¶: {compressed_path}")
            
            print(f"âœ… æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å®Œæˆ: {compressed_path}")
            
            return {
                "batch_num": batch_num,
                "start_page": start_page,
                "end_page": end_page,
                "translated_pdf_path": compressed_path,  # ä½¿ç”¨ä¸big_pdf_transä¸€è‡´çš„å­—æ®µå
                "status": "success"
            }
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å¤±è´¥: {str(e)}")
            logger.error(f"æ‰¹æ¬¡ {batch_num + 1} å¤„ç†å¤±è´¥: {str(e)}")
            return {
                "batch_num": batch_num,
                "start_page": start_page,
                "end_page": end_page,
                "status": "failed",
                "error": str(e)
            }
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
                try:
                    shutil.rmtree(self.temp_dir)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
