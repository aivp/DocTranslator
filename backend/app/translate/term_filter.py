"""
æœ¯è¯­ç­›é€‰æ¨¡å—

æ ¹æ®æ–‡æœ¬å†…å®¹ç­›é€‰æœ€ç›¸å…³çš„æœ¯è¯­ï¼Œè§£å†³API Tokené™åˆ¶é—®é¢˜ã€‚
ä¿ç•™ç°æœ‰æœ¯è¯­åº“æ ¼å¼ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„æœ¯è¯­ã€‚

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼š2.0.0 - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ”¯æŒå€’æ’ç´¢å¼•ï¼‰
"""

import re
import time
import logging
import hashlib
from typing import Dict, List, Tuple, Optional
from difflib import SequenceMatcher
from functools import lru_cache

logger = logging.getLogger(__name__)

# å…¨å±€å€’æ’ç´¢å¼•ç¼“å­˜ï¼š{comparison_id: inverted_index}
_inverted_index_cache: Dict[str, Dict[str, List[Tuple[str, str]]]] = {}
# å€’æ’ç´¢å¼•ç¼“å­˜è®¿é—®æ—¶é—´ï¼š{comparison_id: last_access_time}
_inverted_index_cache_time: Dict[str, float] = {}

# ç»“æœç¼“å­˜ï¼š{text_hash: filtered_terms}
_result_cache: Dict[str, List[Dict[str, str]]] = {}
# ç»“æœç¼“å­˜è®¿é—®æ—¶é—´ï¼š{text_hash: last_access_time}
_result_cache_time: Dict[str, float] = {}
_max_cache_size = 1000  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°

# ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼š1å°æ—¶æœªä½¿ç”¨åˆ™è¿‡æœŸ
_cache_expire_time = 3600  # 1å°æ—¶ = 3600ç§’

def calculate_similarity(text: str, term_source: str) -> float:
    """
    è®¡ç®—æ–‡æœ¬ä¸æœ¯è¯­çš„ç›¸ä¼¼åº¦åˆ†æ•°
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        term_source: æœ¯è¯­åŸæ–‡
        
    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if not text or not term_source:
        return 0.0
    
    # 1. ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
    if term_source in text:
        logger.debug(f"ç²¾ç¡®åŒ¹é…: '{term_source}' in '{text[:50]}...'")
        return 100.0
    
    # 2. å¿½ç•¥å¤§å°å†™çš„ç²¾ç¡®åŒ¹é…
    if term_source.lower() in text.lower():
        logger.debug(f"å¿½ç•¥å¤§å°å†™åŒ¹é…: '{term_source}' in '{text[:50]}...'")
        return 95.0
    
    # 3. æ­£åˆ™åŒ¹é…
    try:
        if re.search(re.escape(term_source), text, re.IGNORECASE):
            logger.debug(f"æ­£åˆ™åŒ¹é…: '{term_source}' in '{text[:50]}...'")
            return 90.0
    except re.error:
        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æœ‰é”™è¯¯ï¼Œè·³è¿‡æ­£åˆ™åŒ¹é…
        pass
    
    # 4. è¯é¢‘åŒ¹é…ï¼ˆè®¡ç®—å…±åŒè¯æ±‡ï¼‰
    text_words = set(text.lower().split())
    term_words = set(term_source.lower().split())
    common_words = text_words & term_words
    
    if common_words:
        # è®¡ç®—è¯æ±‡é‡å åº¦
        overlap_ratio = len(common_words) / len(term_words)
        score = overlap_ratio * 70.0  # æœ€é«˜70åˆ†
        logger.debug(f"è¯æ±‡åŒ¹é…: '{term_source}' ä¸ '{text[:50]}...' é‡å åº¦ {overlap_ratio:.2f}")
        return score
    
    # 5. åºåˆ—ç›¸ä¼¼åº¦ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
    similarity = SequenceMatcher(None, text.lower(), term_source.lower()).ratio()
    if similarity > 0.3:  # åªè€ƒè™‘ç›¸ä¼¼åº¦å¤§äº30%çš„
        score = similarity * 50.0  # æœ€é«˜50åˆ†
        logger.debug(f"åºåˆ—ç›¸ä¼¼åº¦: '{term_source}' ä¸ '{text[:50]}...' ç›¸ä¼¼åº¦ {similarity:.2f}")
        return score
    
    return 0.0

def _cleanup_expired_cache():
    """
    æ¸…ç†è¿‡æœŸçš„ç¼“å­˜ï¼ˆå€’æ’ç´¢å¼•å’Œç»“æœç¼“å­˜ï¼‰
    """
    global _inverted_index_cache, _inverted_index_cache_time, _result_cache, _result_cache_time
    
    current_time = time.time()
    
    # æ¸…ç†è¿‡æœŸçš„å€’æ’ç´¢å¼•ç¼“å­˜
    expired_keys = []
    for key, last_access in _inverted_index_cache_time.items():
        if current_time - last_access > _cache_expire_time:
            expired_keys.append(key)
    
    for key in expired_keys:
        if key in _inverted_index_cache:
            del _inverted_index_cache[key]
        if key in _inverted_index_cache_time:
            del _inverted_index_cache_time[key]
    
    if expired_keys:
        logger.info(f"æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸçš„å€’æ’ç´¢å¼•ç¼“å­˜")
    
    # æ¸…ç†è¿‡æœŸçš„ç»“æœç¼“å­˜
    expired_result_keys = []
    for key, last_access in _result_cache_time.items():
        if current_time - last_access > _cache_expire_time:
            expired_result_keys.append(key)
    
    for key in expired_result_keys:
        if key in _result_cache:
            del _result_cache[key]
        if key in _result_cache_time:
            del _result_cache_time[key]
    
    if expired_result_keys:
        logger.info(f"æ¸…ç†äº† {len(expired_result_keys)} ä¸ªè¿‡æœŸçš„ç»“æœç¼“å­˜")


def build_inverted_index(all_terms: Dict[str, str], comparison_id: Optional[str] = None) -> Dict[str, List[Tuple[str, str]]]:
    """
    ä¸ºæœ¯è¯­åº“å»ºç«‹å€’æ’ç´¢å¼•
    
    Args:
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        comparison_id: æœ¯è¯­åº“IDï¼ˆç”¨äºç¼“å­˜ï¼‰
        
    Returns:
        Dict: å€’æ’ç´¢å¼• {word: [(source, target), ...]}
    """
    if not all_terms:
        return {}
    
    # å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆæ¯10æ¬¡è°ƒç”¨æ¸…ç†ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹æ¸…ç†ï¼‰
    if len(_inverted_index_cache) > 0 and len(_inverted_index_cache) % 10 == 0:
        _cleanup_expired_cache()
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = comparison_id or str(id(all_terms))
    if cache_key in _inverted_index_cache:
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if cache_key in _inverted_index_cache_time:
            current_time = time.time()
            if current_time - _inverted_index_cache_time[cache_key] <= _cache_expire_time:
                # æ›´æ–°è®¿é—®æ—¶é—´
                _inverted_index_cache_time[cache_key] = current_time
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„å€’æ’ç´¢å¼•ï¼Œæœ¯è¯­åº“å¤§å°: {len(all_terms)}")
                return _inverted_index_cache[cache_key]
            else:
                # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤
                logger.debug(f"å€’æ’ç´¢å¼•ç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°æ„å»º")
                del _inverted_index_cache[cache_key]
                del _inverted_index_cache_time[cache_key]
    
    logger.info(f"å¼€å§‹å»ºç«‹å€’æ’ç´¢å¼•ï¼Œæœ¯è¯­åº“å¤§å°: {len(all_terms)}")
    start_time = time.time()
    
    inverted_index: Dict[str, List[Tuple[str, str]]] = {}
    
    for source, target in all_terms.items():
        # å¯¹æœ¯è¯­åŸæ–‡è¿›è¡Œåˆ†è¯
        words = re.findall(r'\b\w+\b', source.lower())
        
        for word in words:
            if len(word) < 2:  # è·³è¿‡å¤ªçŸ­çš„è¯
                continue
            
            if word not in inverted_index:
                inverted_index[word] = []
            
            # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æœ¯è¯­
            term_pair = (source, target)
            if term_pair not in inverted_index[word]:
                inverted_index[word].append(term_pair)
    
    # ç¼“å­˜ç´¢å¼•
    if cache_key:
        _inverted_index_cache[cache_key] = inverted_index
        _inverted_index_cache_time[cache_key] = time.time()  # è®°å½•ç¼“å­˜æ—¶é—´
    
    elapsed = time.time() - start_time
    logger.info(f"å€’æ’ç´¢å¼•å»ºç«‹å®Œæˆï¼Œç”¨æ—¶: {elapsed:.3f}ç§’, ç´¢å¼•è¯æ±‡æ•°: {len(inverted_index)}")
    
    return inverted_index


def filter_relevant_terms(text: str, all_terms: Dict[str, str], max_terms: int = 10, 
                         comparison_id: Optional[str] = None, use_index: bool = True) -> List[Dict[str, str]]:
    """
    æ ¹æ®æ–‡æœ¬å†…å®¹ç­›é€‰æœ€ç›¸å…³çš„æœ¯è¯­ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå€’æ’ç´¢å¼•ï¼‰
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        max_terms: æœ€å¤§æœ¯è¯­æ•°é‡ï¼ˆé»˜è®¤50ä¸ªï¼‰
        comparison_id: æœ¯è¯­åº“IDï¼ˆç”¨äºç¼“å­˜ç´¢å¼•ï¼‰
        use_index: æ˜¯å¦ä½¿ç”¨å€’æ’ç´¢å¼•ï¼ˆé»˜è®¤Trueï¼‰
        
    Returns:
        List[Dict]: ç­›é€‰åçš„æœ¯è¯­åˆ—è¡¨ [{"source": "...", "target": "..."}]
    """
    if not text or not all_terms:
        logger.debug("æ–‡æœ¬æˆ–æœ¯è¯­åº“ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    # æ£€æŸ¥ç»“æœç¼“å­˜
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
    cache_key = f"{comparison_id or ''}_{text_hash}"
    if cache_key in _result_cache:
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if cache_key in _result_cache_time:
            current_time = time.time()
            if current_time - _result_cache_time[cache_key] <= _cache_expire_time:
                # æ›´æ–°è®¿é—®æ—¶é—´
                _result_cache_time[cache_key] = current_time
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„ç­›é€‰ç»“æœ")
                return _result_cache[cache_key]
            else:
                # ç¼“å­˜å·²è¿‡æœŸï¼Œåˆ é™¤
                logger.debug(f"ç»“æœç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°ç­›é€‰")
                del _result_cache[cache_key]
                del _result_cache_time[cache_key]
    
    term_count = len(all_terms)
    logger.debug(f"å¼€å§‹ç­›é€‰æœ¯è¯­ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}, æœ¯è¯­åº“å¤§å°: {term_count}")
    
    start_time = time.time()
    
    # åˆ†è¯å¤„ç†
    words = re.findall(r'\b\w+\b', text.lower())
    words = [w for w in words if len(w) >= 2]  # è¿‡æ»¤å¤ªçŸ­çš„è¯
    
    if not words:
        return []
    
    # ä½¿ç”¨å€’æ’ç´¢å¼•ä¼˜åŒ–ï¼ˆå½“æœ¯è¯­åº“å¤§äº1000æ¡æ—¶ï¼‰
    if use_index and term_count > 1000:
        inverted_index = build_inverted_index(all_terms, comparison_id)
        
        # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆæå‰ç»ˆæ­¢ä¼˜åŒ–ï¼‰
        exact_matches = []
        text_lower = text.lower()
        
        for source, target in all_terms.items():
            source_lower = source.lower()
            # ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if source in text or source_lower in text_lower:
                exact_matches.append({
                    'source': source,
                    'target': target,
                    'score': 100.0 if source in text else 95.0
                })
                # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿå¤šçš„ç²¾ç¡®åŒ¹é…ï¼Œå¯ä»¥æå‰ç»ˆæ­¢
                if len(exact_matches) >= max_terms:
                    logger.debug(f"æ‰¾åˆ° {len(exact_matches)} ä¸ªç²¾ç¡®åŒ¹é…ï¼Œæå‰ç»ˆæ­¢")
                    result = [
                        {'source': term['source'], 'target': term['target']}
                        for term in exact_matches[:max_terms]
                    ]
                    # ç¼“å­˜ç»“æœ
                    if len(_result_cache) >= _max_cache_size:
                        keys_to_remove = list(_result_cache.keys())[:_max_cache_size // 2]
                        for key in keys_to_remove:
                            if key in _result_cache:
                                del _result_cache[key]
                            if key in _result_cache_time:
                                del _result_cache_time[key]
                    _result_cache[cache_key] = result
                    _result_cache_time[cache_key] = time.time()  # è®°å½•ç¼“å­˜æ—¶é—´
                    return result
        
        # é€šè¿‡ç´¢å¼•å¿«é€ŸæŸ¥æ‰¾ç›¸å…³æœ¯è¯­
        candidate_terms = {}  # {source: (target, max_score)}
        
        # å¦‚æœå·²æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä¼˜å…ˆä¿ç•™
        for match in exact_matches:
            candidate_terms[match['source']] = (match['target'], match['score'])
        
        # é€šè¿‡ç´¢å¼•æŸ¥æ‰¾å…¶ä»–ç›¸å…³æœ¯è¯­
        for word in words:
            if word in inverted_index:
                # ä»ç´¢å¼•ä¸­è·å–åŒ…å«è¯¥è¯æ±‡çš„æœ¯è¯­
                for source, target in inverted_index[word]:
                    # è·³è¿‡å·²æ‰¾åˆ°çš„ç²¾ç¡®åŒ¹é…
                    if source in candidate_terms and candidate_terms[source][1] >= 95.0:
                        continue
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                    score = calculate_word_similarity(word, source)
                    if score > 0:
                        term_key = source
                        if term_key not in candidate_terms or score > candidate_terms[term_key][1]:
                            candidate_terms[term_key] = (target, score)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        scored_terms = [
            {'source': source, 'target': target, 'score': score}
            for source, (target, score) in candidate_terms.items()
        ]
        scored_terms.sort(key=lambda x: x['score'], reverse=True)
        
        # é™åˆ¶æ•°é‡
        selected_terms = scored_terms[:max_terms]
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        result = [
            {'source': term['source'], 'target': term['target']}
            for term in selected_terms
        ]
        
    else:
        # å°æœ¯è¯­åº“æˆ–ç¦ç”¨ç´¢å¼•æ—¶ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        logger.debug("ä½¿ç”¨åŸå§‹æ–¹æ³•ç­›é€‰æœ¯è¯­ï¼ˆæœ¯è¯­åº“è¾ƒå°æˆ–ç¦ç”¨ç´¢å¼•ï¼‰")
        
        # ä¸ºæ¯ä¸ªè¯æ±‡æ‰¾åˆ°æœ€ç›¸å…³çš„æœ¯è¯­
        word_term_mapping = {}  # {word: [terms]}
        
        for word in words:
            word_terms = []
            for source, target in all_terms.items():
                score = calculate_word_similarity(word, source)
                if score > 0:
                    word_terms.append({
                        'source': source,
                        'target': target,
                        'score': score,
                        'matched_word': word
                    })
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œæ¯ä¸ªè¯æ±‡æœ€å¤šå–5ä¸ªæœ€ç›¸å…³çš„æœ¯è¯­
            word_terms.sort(key=lambda x: x['score'], reverse=True)
            word_term_mapping[word] = word_terms[:5]
        
        # åˆå¹¶æ‰€æœ‰è¯æ±‡çš„æœ¯è¯­ï¼Œå»é‡
        all_scored_terms = {}
        for word, terms in word_term_mapping.items():
            for term in terms:
                term_key = f"{term['source']}:{term['target']}"
                if term_key not in all_scored_terms:
                    all_scored_terms[term_key] = term
                else:
                    if term['score'] > all_scored_terms[term_key]['score']:
                        all_scored_terms[term_key] = term
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        scored_terms = list(all_scored_terms.values())
        scored_terms.sort(key=lambda x: x['score'], reverse=True)
        
        # é™åˆ¶æ€»æ•°ä¸è¶…è¿‡max_terms
        selected_terms = scored_terms[:max_terms]
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        result = [
            {'source': term['source'], 'target': term['target']}
            for term in selected_terms
        ]
    
    # ç¼“å­˜ç»“æœ
    if len(_result_cache) >= _max_cache_size:
        # æ¸…é™¤æœ€æ—§çš„ç¼“å­˜ï¼ˆç®€å•ç­–ç•¥ï¼šæ¸…é™¤ä¸€åŠï¼‰
        keys_to_remove = list(_result_cache.keys())[:_max_cache_size // 2]
        for key in keys_to_remove:
            if key in _result_cache:
                del _result_cache[key]
            if key in _result_cache_time:
                del _result_cache_time[key]
    
    _result_cache[cache_key] = result
    _result_cache_time[cache_key] = time.time()  # è®°å½•ç¼“å­˜æ—¶é—´
    
    elapsed = time.time() - start_time
    logger.debug(f"æœ¯è¯­ç­›é€‰å®Œæˆ: {term_count} -> {len(result)} ä¸ªæœ¯è¯­, ç”¨æ—¶: {elapsed:.3f}ç§’")
    
    return result

def calculate_word_similarity(word: str, term_source: str) -> float:
    """
    è®¡ç®—å•ä¸ªè¯æ±‡ä¸æœ¯è¯­çš„ç›¸ä¼¼åº¦
    
    Args:
        word: å•ä¸ªè¯æ±‡
        term_source: æœ¯è¯­åŸæ–‡
        
    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    """
    if not word or not term_source:
        return 0.0
    
    # 1. ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜åˆ†ï¼‰
    if word == term_source.lower():
        logger.debug(f"è¯æ±‡ç²¾ç¡®åŒ¹é…: '{word}' == '{term_source}'")
        return 100.0
    
    # 2. è¯æ±‡åŒ…å«å…³ç³»
    if word in term_source.lower():
        logger.debug(f"è¯æ±‡åŒ…å«åŒ¹é…: '{word}' in '{term_source}'")
        return 90.0
    
    # 3. æœ¯è¯­åŒ…å«è¯æ±‡
    if term_source.lower() in word:
        logger.debug(f"æœ¯è¯­åŒ…å«è¯æ±‡: '{term_source}' in '{word}'")
        return 85.0
    
    # 4. è¯æ±‡é‡å åº¦
    word_chars = set(word)
    term_chars = set(term_source.lower())
    common_chars = word_chars & term_chars
    
    if common_chars:
        overlap_ratio = len(common_chars) / max(len(word_chars), len(term_chars))
        if overlap_ratio > 0.5:  # å­—ç¬¦é‡å åº¦å¤§äº50%
            score = overlap_ratio * 70.0
            logger.debug(f"å­—ç¬¦é‡å åŒ¹é…: '{word}' ä¸ '{term_source}' é‡å åº¦ {overlap_ratio:.2f}")
            return score
    
    # 5. åºåˆ—ç›¸ä¼¼åº¦ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
    similarity = SequenceMatcher(None, word, term_source.lower()).ratio()
    if similarity > 0.6:  # ç›¸ä¼¼åº¦å¤§äº60%
        score = similarity * 60.0
        logger.debug(f"åºåˆ—ç›¸ä¼¼åº¦: '{word}' ä¸ '{term_source}' ç›¸ä¼¼åº¦ {similarity:.2f}")
        return score
    
    return 0.0

def optimize_terms_for_api(text: str, all_terms: Dict[str, str], max_terms: int = 10, 
                           comparison_id: Optional[str] = None) -> List[Dict[str, str]]:
    """
    ä¸ºAPIè°ƒç”¨ä¼˜åŒ–æœ¯è¯­åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå€’æ’ç´¢å¼•å’Œç¼“å­˜ï¼‰
    ä¸¥æ ¼é™åˆ¶æœ¯è¯­æ•°é‡å’Œå­—ç¬¦æ•°ï¼Œé¿å…APIè¶…æ—¶
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        max_terms: æœ€å¤§æœ¯è¯­æ•°é‡ï¼ˆé»˜è®¤50ï¼Œä½†ä¼šæ ¹æ®æœ¯è¯­åº“å¤§å°å’Œå­—ç¬¦æ•°è¿›ä¸€æ­¥é™åˆ¶ï¼‰
        comparison_id: æœ¯è¯­åº“IDï¼ˆç”¨äºç¼“å­˜ç´¢å¼•å’Œç»“æœï¼‰
        
    Returns:
        List[Dict]: ä¼˜åŒ–åçš„æœ¯è¯­åˆ—è¡¨ï¼Œæ ¼å¼ä¸Qwen APIå…¼å®¹
    """
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ä¸¥æ ¼é™åˆ¶ï¼šé¿å…ä¼ å…¥è¿‡å¤šæœ¯è¯­å¯¼è‡´APIè¶…æ—¶
    # ç»Ÿä¸€é™åˆ¶ï¼šæœ€å¤š10ä¸ªæœ¯è¯­ï¼ˆæ— è®ºæœ¯è¯­åº“å¤§å°ï¼‰
    strict_max_terms = min(10, max_terms)
    if len(all_terms) > 1000:
        logger.info(f"æœ¯è¯­åº“æ£€æµ‹ï¼ˆ{len(all_terms)}æ¡ï¼‰ï¼Œä½¿ç”¨ä¸¥æ ¼ç­›é€‰ï¼šæœ€å¤š{strict_max_terms}ä¸ªæœ¯è¯­")
    
    # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„ç­›é€‰å‡½æ•°ï¼ˆè‡ªåŠ¨ä½¿ç”¨å€’æ’ç´¢å¼•ï¼‰
    relevant_terms = filter_relevant_terms(text, all_terms, strict_max_terms, comparison_id, use_index=True)
    
    # æ£€æŸ¥æœ¯è¯­åº“å¤§å°å’Œå­—ç¬¦æ•°ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ä»¥é¿å…APIè¶…æ—¶
    # ç»Ÿä¸€é™åˆ¶ï¼šæœ€å¤š10ä¸ªæœ¯è¯­ï¼Œæœ€å¤š1000å­—ç¬¦
    MAX_TERMS_COUNT = 10  # æœ€å¤š10ä¸ªæœ¯è¯­
    MAX_CHARS_LIMIT = 1000  # æœ€å¤š1000å­—ç¬¦
    
    # å…ˆæŒ‰ç›¸ä¼¼åº¦é‡æ–°è¯„åˆ†å¹¶æ’åº
    scored_terms = []
    for term in relevant_terms:
        score = calculate_similarity(text, term['source'])
        scored_terms.append((term, score))
    
    scored_terms.sort(key=lambda x: x[1], reverse=True)
    
    # æ ¹æ®å­—ç¬¦æ•°å’Œæ•°é‡é™åˆ¶ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„æœ¯è¯­
    optimized_terms = []
    current_chars = 0
    term_count = 0
    
    for term, score in scored_terms:
        # æ£€æŸ¥æ•°é‡é™åˆ¶
        if term_count >= MAX_TERMS_COUNT:
            logger.debug(f"è¾¾åˆ°æœ€å¤§æœ¯è¯­æ•°é‡é™åˆ¶ï¼ˆ{MAX_TERMS_COUNT}ä¸ªï¼‰ï¼Œåœæ­¢æ·»åŠ ")
            break
        
        term_chars = len(term['source']) + len(term['target'])
        
        # æ£€æŸ¥å­—ç¬¦æ•°é™åˆ¶
        if current_chars + term_chars > MAX_CHARS_LIMIT:
            logger.debug(f"è¾¾åˆ°æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼ˆ{MAX_CHARS_LIMIT}å­—ç¬¦ï¼‰ï¼Œå½“å‰: {current_chars}ï¼Œåœæ­¢æ·»åŠ ")
            break
        
        optimized_terms.append(term)
        current_chars += term_chars
        term_count += 1
    
    # è®¡ç®—æ€»ç”¨æ—¶
    end_time = time.time()
    duration = end_time - start_time
    
    if len(optimized_terms) < len(relevant_terms):
        logger.info(f"ğŸ“š æœ¯è¯­åº“ä¼˜åŒ–å®Œæˆ: {len(relevant_terms)} -> {len(optimized_terms)} ä¸ªæœ¯è¯­, å­—ç¬¦æ•°: {current_chars}, ç”¨æ—¶: {duration:.3f}ç§’")
    else:
        logger.debug(f"ğŸ“š æœ¯è¯­ç­›é€‰å®Œæˆ: {len(optimized_terms)} ä¸ªæœ¯è¯­, å­—ç¬¦æ•°: {current_chars}, ç”¨æ—¶: {duration:.3f}ç§’")
    
    return optimized_terms
    
    # å¦‚æœç­›é€‰åçš„æœ¯è¯­ä»ç„¶å¤ªå¤šï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–
    if len(relevant_terms) > 15:
        # é‡æ–°è¯„åˆ†å¹¶åªä¿ç•™æœ€ç›¸å…³çš„15ä¸ª
        scored_terms = []
        for term in relevant_terms:
            score = calculate_similarity(text, term['source'])
            scored_terms.append((term, score))
        
        scored_terms.sort(key=lambda x: x[1], reverse=True)
        relevant_terms = [term for term, _ in scored_terms[:15]]
        logger.debug(f"è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šé™åˆ¶ä¸ºæœ€ç›¸å…³çš„15ä¸ªæœ¯è¯­")
    
    # æ£€æŸ¥å­—ç¬¦æ•°é™åˆ¶ï¼ˆ1500å­—ç¬¦ï¼‰
    total_chars = sum(len(term['source']) + len(term['target']) for term in relevant_terms)
    if total_chars > 1500:
        # æŒ‰åˆ†æ•°æ’åºï¼Œä¼˜å…ˆä¿ç•™é«˜åˆ†æœ¯è¯­
        scored_terms = []
        for term in relevant_terms:
            score = calculate_similarity(text, term['source'])
            scored_terms.append((term, score))
        
        scored_terms.sort(key=lambda x: x[1], reverse=True)
        
        # é€æ­¥å‡å°‘æœ¯è¯­æ•°é‡ï¼Œç›´åˆ°æ»¡è¶³å­—ç¬¦é™åˆ¶
        optimized_terms = []
        current_chars = 0
        for term, score in scored_terms:
            term_chars = len(term['source']) + len(term['target'])
            if current_chars + term_chars <= 1500:
                optimized_terms.append(term)
                current_chars += term_chars
            else:
                break
        
        logger.debug(f"å­—ç¬¦æ•°ä¼˜åŒ–ï¼š{len(relevant_terms)} -> {len(optimized_terms)} ä¸ªæœ¯è¯­, {total_chars} -> {current_chars} å­—ç¬¦")
        relevant_terms = optimized_terms
    
    # è®¡ç®—æ€»ç”¨æ—¶
    end_time = time.time()
    duration = end_time - start_time
    logger.debug(f"ğŸ“š æœ¯è¯­ç­›é€‰ç®—æ³•ç”¨æ—¶: {duration:.3f}ç§’, ç­›é€‰ç»“æœ: {len(relevant_terms)}ä¸ªæœ¯è¯­")
    
    return relevant_terms


def clear_term_cache(comparison_id: Optional[str] = None):
    """
    æ¸…é™¤æœ¯è¯­åº“ç¼“å­˜
    
    Args:
        comparison_id: æœ¯è¯­åº“IDï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
    """
    global _inverted_index_cache, _inverted_index_cache_time, _result_cache, _result_cache_time
    
    if comparison_id:
        if comparison_id in _inverted_index_cache:
            del _inverted_index_cache[comparison_id]
            logger.info(f"å·²æ¸…é™¤æœ¯è¯­åº“ {comparison_id} çš„å€’æ’ç´¢å¼•ç¼“å­˜")
        if comparison_id in _inverted_index_cache_time:
            del _inverted_index_cache_time[comparison_id]
        
        # æ¸…é™¤ç›¸å…³çš„ç»“æœç¼“å­˜
        keys_to_remove = [k for k in _result_cache.keys() if k.startswith(f"{comparison_id}_")]
        for key in keys_to_remove:
            if key in _result_cache:
                del _result_cache[key]
            if key in _result_cache_time:
                del _result_cache_time[key]
        logger.info(f"å·²æ¸…é™¤æœ¯è¯­åº“ {comparison_id} çš„ç»“æœç¼“å­˜ï¼Œå…± {len(keys_to_remove)} æ¡")
    else:
        _inverted_index_cache.clear()
        _inverted_index_cache_time.clear()
        _result_cache.clear()
        _result_cache_time.clear()
        logger.info("å·²æ¸…é™¤æ‰€æœ‰æœ¯è¯­åº“ç¼“å­˜")

def batch_filter_terms(texts: List[str], all_terms: Dict[str, str], max_terms: int = 10) -> List[List[Dict[str, str]]]:
    """
    æ‰¹é‡ç­›é€‰æœ¯è¯­ï¼ˆç”¨äºå¤šæ–‡æœ¬ç¿»è¯‘ï¼‰
    
    Args:
        texts: è¦ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
        all_terms: æ‰€æœ‰æœ¯è¯­å­—å…¸ {source: target}
        max_terms: æ¯ä¸ªæ–‡æœ¬çš„æœ€å¤§æœ¯è¯­æ•°é‡
        
    Returns:
        List[List[Dict]]: æ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„æœ¯è¯­åˆ—è¡¨
    """
    results = []
    for i, text in enumerate(texts):
        logger.debug(f"å¤„ç†ç¬¬ {i+1}/{len(texts)} ä¸ªæ–‡æœ¬")
        terms = optimize_terms_for_api(text, all_terms, max_terms)
        results.append(terms)
    
    return results 