import threading
import subprocess
from docx import Document
from docx.oxml.ns import qn
from . import to_translate
from . import common
import os
import time
import datetime
import zipfile
import xml.etree.ElementTree as ET
from threading import Lock
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import tempfile
import shutil
import logging
from typing import List, Dict, Any, Optional
from ..utils.word_run_optimizer import SafeRunMerger, quick_optimize
from docx.text.paragraph import Paragraph
from docx.oxml import OxmlElement

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# çº¿ç¨‹å®‰å…¨æ‰“å°é”
print_lock = Lock()

def cleanup_temp_file(temp_path: str):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        if temp_path and os.path.exists(temp_path) and 'optimized_' in os.path.basename(temp_path):
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)
            # åˆ é™¤ä¸´æ—¶ç›®å½•
            temp_dir = os.path.dirname(temp_path)
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)
    except Exception as e:
        logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

# ç‰¹æ®Šç¬¦å·å’Œæ•°å­¦ç¬¦å·çš„æ­£åˆ™è¡¨è¾¾å¼
SPECIAL_SYMBOLS_PATTERN = re.compile(
    r'^[â˜…â˜†â™¥â™¦â™£â™ â™€â™‚â˜¼â˜¾â˜½â™ªâ™«â™¬â˜‘â˜’âœ“âœ”âœ•âœ–âœ—âœ˜âŠ•âŠ—âˆâˆ‘âˆÏ€Î Â±Ã—Ã·âˆšâˆ›âˆœâˆ«âˆ®âˆ‡âˆ‚âˆ†âˆâˆ‘âˆšâˆâˆâˆŸâˆ âˆ¡âˆ¢âˆ£âˆ¤âˆ¥âˆ¦âˆ§âˆ¨âˆ©âˆªâˆ«âˆ¬âˆ­âˆ®âˆ¯âˆ°âˆ±âˆ²âˆ³âˆ´âˆµâˆ¶âˆ·âˆ¸âˆ¹âˆºâˆ»âˆ¼âˆ½âˆ¾âˆ¿â‰€â‰â‰‚â‰ƒâ‰„â‰…â‰†â‰‡â‰ˆâ‰‰â‰Šâ‰‹â‰Œâ‰â‰â‰â‰â‰‘â‰’â‰“â‰”â‰•â‰–â‰—â‰˜â‰™â‰šâ‰›â‰œâ‰â‰â‰Ÿâ‰ â‰¡â‰¢â‰£â‰¤â‰¥â‰¦â‰§â‰¨â‰©â‰ªâ‰«â‰¬â‰­â‰®â‰¯â‰°â‰±â‰²â‰³â‰´â‰µâ‰¶â‰·â‰¸â‰¹â‰ºâ‰»â‰¼â‰½â‰¾â‰¿âŠ€âŠâŠ‚âŠƒâŠ„âŠ…âŠ†âŠ‡âŠˆâŠ‰âŠŠâŠ‹âŠŒâŠâŠâŠâŠâŠ‘âŠ’âŠ“âŠ”âŠ•âŠ–âŠ—âŠ˜âŠ™âŠšâŠ›âŠœâŠâŠâŠŸâŠ âŠ¡âŠ¢âŠ£âŠ¤âŠ¥âŠ¦âŠ§âŠ¨âŠ©âŠªâŠ«âŠ¬âŠ­âŠ®âŠ¯âŠ°âŠ±âŠ²âŠ³âŠ´âŠµâŠ¶âŠ·âŠ¸âŠ¹âŠºâŠ»âŠ¼âŠ½âŠ¾âŠ¿â‹€â‹â‹‚â‹ƒâ‹„â‹…â‹†â‹‡â‹ˆâ‹‰â‹Šâ‹‹â‹Œâ‹â‹â‹â‹â‹‘â‹’â‹“â‹”â‹•â‹–â‹—â‹˜â‹™â‹šâ‹›â‹œâ‹â‹â‹Ÿâ‹ â‹¡â‹¢â‹£â‹¤â‹¥â‹¦â‹§â‹¨â‹©â‹ªâ‹«â‹¬â‹­â‹®â‹¯â‹°â‹±â‹²â‹³â‹´â‹µâ‹¶â‹·â‹¸â‹¹â‹ºâ‹»â‹¼â‹½â‹¾â‹¿]+$')

# çº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹çš„æ­£åˆ™è¡¨è¾¾å¼
NUMBERS_PATTERN = re.compile(r'^[\d\s\.,\-\+\*\/\(\)\[\]\{\}]+$')
TITLE_NUMBER_PATTERN = re.compile(r'^(\d+(?:\.\d+)*\.?)\s*$')  # åŒ¹é…æ ‡é¢˜ç¼–å·å¦‚ "1."ã€"1.1"ã€"1.1.1"ç­‰

# æ·»åŠ ç›®å½•é¡µç æ¨¡å¼
TOC_PAGE_PATTERN = re.compile(r'[\s\-â€”_]+\d+$')


def check_if_image(run):
    """æ£€æŸ¥runæ˜¯å¦åŒ…å«å›¾ç‰‡"""
    namespaces = run.element.nsmap
    drawing = run.element.find('.//w:drawing', namespaces)
    if drawing is not None:
        blip = drawing.find('.//a:blip', {**namespaces, 'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'})
        if blip is not None:
            return True
    if run.element.find('.//w:object', namespaces) is not None:
        return True
    if run.element.find('.//w:pict', namespaces) is not None:
        return True
    return False

def check_if_textbox(run):
    """æ£€æŸ¥runæ˜¯å¦åŒ…å«æ–‡æœ¬æ¡†"""
    namespaces = {**run.element.nsmap, 'v': 'urn:schemas-microsoft-com:vml'}
    # æ£€æŸ¥ DrawingML æ–‡æœ¬æ¡†
    drawing = run.element.find('.//w:drawing', namespaces)
    if drawing is not None:
        txbx = drawing.find('.//w:txbxContent', {**namespaces, 'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'})
        if txbx is not None:
            return True
    
    # æ£€æŸ¥ VML æ–‡æœ¬æ¡†
    vtextbox = run.element.find('.//v:textbox', namespaces)
    if vtextbox is not None:
        # VML æ–‡æœ¬æ¡†å†…éƒ¨é€šå¸¸æœ‰ w:txbxContent
        txbx = vtextbox.find('.//w:txbxContent', namespaces)
        if txbx is not None:
            return True
    
    return False


def clear_image(paragraph):
    """æ¸…é™¤æ®µè½ä¸­çš„å›¾ç‰‡ï¼Œä½†ä¿ç•™æ–‡æœ¬å†…å®¹"""
    runs_to_remove = []
    for run in paragraph.runs:
        if check_if_image(run):
            runs_to_remove.append(run)
    
    # ä»åå¾€å‰åˆ é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–
    for run in reversed(runs_to_remove):
        try:
            run._element.getparent().remove(run._element)
        except:
            pass



def start(trans):
    """ä¸»å…¥å£å‡½æ•°ï¼Œå¤„ç†Wordæ–‡æ¡£ç¿»è¯‘"""
    # ç¡¬ç¼–ç çº¿ç¨‹æ•°ä¸º30ï¼Œå¿½ç•¥å‰ç«¯ä¼ å…¥çš„é…ç½®
    max_threads = 30
    start_time = datetime.datetime.now()

    # ============== æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Okapiæ–¹æ¡ˆ ==============
    use_okapi = trans.get('use_okapi', True)  # é»˜è®¤ä½¿ç”¨Okapi
    
    logger.info(f"ğŸ” ç¿»è¯‘æ–¹æ³•é€‰æ‹©è°ƒè¯•:")
    logger.info(f"  é…ç½®çš„ use_okapi: {use_okapi}")
    logger.info(f"  é…ç½®çš„ threads: {trans.get('threads', 'æœªè®¾ç½®')}")
    logger.info(f"  å®é™…ä½¿ç”¨çº¿ç¨‹æ•°: {max_threads}")
    
    if use_okapi:
        logger.info("ğŸ”„ ä½¿ç”¨ Okapi Framework è¿›è¡Œç¿»è¯‘ï¼ˆè§£å†³runåˆ‡å‰²é—®é¢˜ï¼‰")
        return start_with_okapi(trans, start_time)
    else:
        logger.info("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œç¿»è¯‘")
        return start_traditional(trans, start_time, max_threads)


def start_with_okapi(trans, start_time):
    """ä½¿ç”¨ Okapi Framework è¿›è¡Œç¿»è¯‘"""
    try:
        # å¯¼å…¥ Okapi é›†æˆæ¨¡å—
        from .okapi_integration import OkapiWordTranslator, verify_okapi_installation
        
        # éªŒè¯ Okapi å®‰è£…
        if not verify_okapi_installation():
            logger.error("âŒ Okapi å®‰è£…éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            # ç¡¬ç¼–ç çº¿ç¨‹æ•°ä¸º30ï¼Œå¿½ç•¥å‰ç«¯ä¼ å…¥çš„é…ç½®
            max_threads = 30
            return start_traditional(trans, start_time, max_threads)
        
        # å¦‚æœç”¨æˆ·é€‰æ‹©äº†qwen-mt-plusï¼Œè®¾ç½®serverä¸ºqwen
        if trans.get('model') == 'qwen-mt-plus':
            trans['server'] = 'qwen'
            logger.info("âœ… è®¾ç½®ç¿»è¯‘æœåŠ¡ä¸º Qwen")
        
        # é¢„åŠ è½½æœ¯è¯­åº“
        comparison_id = trans.get('comparison_id')
        if comparison_id:
            logger.info(f"ğŸ“š å¼€å§‹é¢„åŠ è½½æœ¯è¯­åº“: {comparison_id}")
            from .main import get_comparison
            preloaded_terms = get_comparison(comparison_id)
            if preloaded_terms:
                logger.info(f"ğŸ“š æœ¯è¯­åº“é¢„åŠ è½½æˆåŠŸ: {len(preloaded_terms)} ä¸ªæœ¯è¯­")
                # å°†é¢„åŠ è½½çš„æœ¯è¯­åº“æ·»åŠ åˆ°transä¸­
                trans['preloaded_terms'] = preloaded_terms
            else:
                logger.warning(f"ğŸ“š æœ¯è¯­åº“é¢„åŠ è½½å¤±è´¥: {comparison_id}")
        
        # åˆ›å»º Okapi ç¿»è¯‘å™¨
        translator = OkapiWordTranslator()
        logger.info("âœ… Okapi ç¿»è¯‘å™¨åˆ›å»ºæˆåŠŸ")
        
        # è®¾ç½®ç¿»è¯‘æœåŠ¡
        class OkapiTranslationService:
            def __init__(self, trans):
                self.trans = trans
            
            def batch_translate(self, texts, source_lang, target_lang):
                """æ‰¹é‡ç¿»è¯‘æ–‡æœ¬ - ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæ”¯æŒæœ¯è¯­åº“ç­›é€‰"""
                from concurrent.futures import ThreadPoolExecutor, as_completed
                import threading
                
                translated_texts = [None] * len(texts)  # é¢„åˆ†é…ç»“æœæ•°ç»„
                # ä»å‰ç«¯é…ç½®è·å–æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º10
                # ç¡¬ç¼–ç çº¿ç¨‹æ•°ä¸º30ï¼Œå¿½ç•¥å‰ç«¯ä¼ å…¥çš„é…ç½®
                max_workers = min(30, len(texts))
                
                logger.info(f"å¼€å§‹å¹¶è¡Œç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹ï¼ˆç¡¬ç¼–ç 30ï¼‰")
                
                # è¿›åº¦æ›´æ–°ç›¸å…³å˜é‡
                completed_count = 0
                total_count = len(texts)
                progress_lock = threading.Lock()
                
                def update_progress():
                    """æ›´æ–°ç¿»è¯‘è¿›åº¦"""
                    nonlocal completed_count
                    with progress_lock:
                        completed_count += 1
                        progress_percentage = min((completed_count / total_count) * 100, 100.0)
                        logger.info(f"ç¿»è¯‘è¿›åº¦: {completed_count}/{total_count} ({progress_percentage:.1f}%)")
                        
                        # æ›´æ–°æ•°æ®åº“è¿›åº¦
                        try:
                            from .to_translate import db
                            db.execute("update translate set process=%s where id=%s", 
                                     str(format(progress_percentage, '.1f')), 
                                     self.trans['id'])
                            
                            # å¦‚æœè¿›åº¦è¾¾åˆ°100%ï¼Œç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
                            if progress_percentage >= 100.0:
                                from datetime import datetime
                                import pytz
                                end_time = datetime.now(pytz.timezone('Asia/Shanghai'))
                                db.execute(
                                    "update translate set status='done',end_at=%s,process=100 where id=%s",
                                    end_time, self.trans['id']
                                )
                                logger.info("âœ… ç¿»è¯‘å®Œæˆï¼ŒçŠ¶æ€å·²æ›´æ–°ä¸ºå·²å®Œæˆ")
                                
                        except Exception as e:
                            logger.error(f"æ›´æ–°è¿›åº¦å¤±è´¥: {str(e)}")
                
                def translate_single_text(index, text):
                    """ç¿»è¯‘å•ä¸ªæ–‡æœ¬ï¼Œæ”¯æŒæœ¯è¯­åº“ç­›é€‰"""
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ¯è¯­åº“é…ç½®
                        comparison_id = self.trans.get('comparison_id')
                        if comparison_id:
                            logger.debug(f"æ–‡æœ¬ {index} ä½¿ç”¨æœ¯è¯­åº“ç­›é€‰: {comparison_id}")
                            
                            # ä½¿ç”¨é¢„åŠ è½½çš„æœ¯è¯­åº“è¿›è¡Œç­›é€‰
                            preloaded_terms = self.trans.get('preloaded_terms')
                            if preloaded_terms:
                                # è®°å½•æœ¯è¯­åº“å¤„ç†å¼€å§‹æ—¶é—´
                                term_start_time = time.time()
                                from .term_filter import optimize_terms_for_api
                                filtered_terms = optimize_terms_for_api(text, preloaded_terms, max_terms=50)
                                term_end_time = time.time()
                                term_duration = term_end_time - term_start_time
                                
                                logger.info(f"ğŸ“š æœ¯è¯­åº“ç­›é€‰ç”¨æ—¶: {term_duration:.3f}ç§’, æ‰¾åˆ°æœ¯è¯­æ•°: {len(filtered_terms) if filtered_terms else 0}")
                            else:
                                # å¦‚æœæ²¡æœ‰é¢„åŠ è½½çš„æœ¯è¯­åº“ï¼Œå›é€€åˆ°åŸæ¥çš„æ–¹æ³•
                                logger.warning(f"æ–‡æœ¬ {index} æ²¡æœ‰é¢„åŠ è½½çš„æœ¯è¯­åº“ï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢")
                                from .main import get_filtered_terms_for_text
                                filtered_terms = get_filtered_terms_for_text(text, comparison_id, max_terms=50)
                            
                            if filtered_terms:
                                logger.debug(f"æ–‡æœ¬ {index} ä½¿ç”¨æœ¯è¯­åº“")
                                # åˆ›å»ºä¸´æ—¶ç¿»è¯‘é…ç½®ï¼ŒåŒ…å«ç­›é€‰åçš„æœ¯è¯­åº“
                                temp_trans = self.trans.copy()
                                temp_trans['filtered_terms'] = filtered_terms
                                translated = to_translate.translate_text(
                                    temp_trans, text, source_lang, target_lang
                                )
                            else:
                                logger.debug(f"æ–‡æœ¬ {index} æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æœ¯è¯­")
                                translated = to_translate.translate_text(
                                    self.trans, text, source_lang, target_lang
                                )
                        else:
                            logger.debug(f"æ–‡æœ¬ {index} æœªä½¿ç”¨æœ¯è¯­åº“")
                            translated = to_translate.translate_text(
                                self.trans, text, source_lang, target_lang
                            )
                        
                        logger.debug(f"æ–‡æœ¬ {index} ç¿»è¯‘å®Œæˆ: {text[:50]}... -> {translated[:50]}...")
                        return index, translated, None
                    except Exception as e:
                        logger.error(f"æ–‡æœ¬ {index} ç¿»è¯‘å¤±è´¥: {e}")
                        return index, text, str(e)  # å¤±è´¥æ—¶ä¿æŒåŸæ–‡
                
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œç¿»è¯‘
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
                    future_to_index = {
                        executor.submit(translate_single_text, i, text): i 
                        for i, text in enumerate(texts)
                    }
                    
                    # æ”¶é›†ç»“æœ
                    completed_count = 0
                    for future in as_completed(future_to_index):
                        index, translated_text, error = future.result()
                        translated_texts[index] = translated_text
                        completed_count += 1
                        
                        update_progress()
                
                logger.info(f"å¹¶è¡Œç¿»è¯‘å®Œæˆï¼Œå…±ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬")
                return translated_texts
        
        translator.set_translation_service(OkapiTranslationService(trans))
        logger.info("âœ… ç¿»è¯‘æœåŠ¡è®¾ç½®æˆåŠŸ")
        
        # è¯­è¨€æ˜ å°„ï¼šå°†ä¸­æ–‡è¯­è¨€åç§°è½¬æ¢ä¸ºè‹±æ–‡å…¨æ‹¼
        def map_language_to_qwen_format(lang_name):
            """å°†ä¸­æ–‡è¯­è¨€åç§°æ˜ å°„ä¸ºQwen APIéœ€è¦çš„è‹±æ–‡å…¨æ‹¼æ ¼å¼"""
            # å¤„ç†ç©ºå€¼å’ŒNoneçš„æƒ…å†µ
            if not lang_name or lang_name.strip() == '':
                return 'auto'  # æºè¯­è¨€ä¸ºç©ºæ—¶è¿”å›auto
                
            language_mapping = {
                # ä¸­æ–‡åç§°åˆ°è‹±æ–‡å…¨æ‹¼
                'ä¸­æ–‡': 'Chinese',
                'ç®€ä½“ä¸­æ–‡': 'Chinese',
                'ç¹ä½“ä¸­æ–‡': 'Traditional Chinese',
                'è‹±è¯­': 'English',
                'è‹±æ–‡': 'English',
                'ä¿„è¯­': 'Russian',
                'æ—¥è¯­': 'Japanese',
                'éŸ©è¯­': 'Korean',
                'è¥¿ç­ç‰™è¯­': 'Spanish',
                'æ³•è¯­': 'French',
                'è‘¡è„ç‰™è¯­': 'Portuguese',
                'å¾·è¯­': 'German',
                'æ„å¤§åˆ©è¯­': 'Italian',
                'æ³°è¯­': 'Thai',
                'è¶Šå—è¯­': 'Vietnamese',
                'å°åº¦å°¼è¥¿äºšè¯­': 'Indonesian',
                'é©¬æ¥è¯­': 'Malay',
                'é˜¿æ‹‰ä¼¯è¯­': 'Arabic',
                'å°åœ°è¯­': 'Hindi',
                'å¸Œä¼¯æ¥è¯­': 'Hebrew',
                'ç¼…ç”¸è¯­': 'Burmese',
                'æ³°ç±³å°”è¯­': 'Tamil',
                'ä¹Œå°”éƒ½è¯­': 'Urdu',
                'å­ŸåŠ æ‹‰è¯­': 'Bengali',
                'æ³¢å…°è¯­': 'Polish',
                'è·å…°è¯­': 'Dutch',
                'ç½—é©¬å°¼äºšè¯­': 'Romanian',
                'åœŸè€³å…¶è¯­': 'Turkish',
                'é«˜æ£‰è¯­': 'Khmer',
                'è€æŒè¯­': 'Lao',
                'ç²¤è¯­': 'Cantonese',
                'æŸ¬åŸ”å¯¨è¯­': 'Khmer',
                'æŸ¬åŸ”å¯¨è¯­ï¼ˆé«˜æ£‰è¯­ï¼‰': 'Khmer',
                'å°å°¼è¯­/é©¬æ¥è¯­': 'Indonesian',
                'è²å¾‹å®¾è¯­ï¼ˆä»–åŠ ç¦„è¯­ï¼‰': 'Tagalog',
                'è²å¾‹å®¾è¯­': 'Tagalog',
                'ä»–åŠ ç¦„è¯­': 'Tagalog',
                # è‹±æ–‡å…¨æ‹¼åˆ°è‡ªèº« (ç¡®ä¿è‹±æ–‡å…¨æ‹¼æ˜ å°„åˆ°è‡ªèº«)
                'Chinese': 'Chinese',
                'English': 'English',
                'Russian': 'Russian',
                'Japanese': 'Japanese',
                'Korean': 'Korean',
                'Spanish': 'Spanish',
                'French': 'French',
                'Portuguese': 'Portuguese',
                'German': 'German',
                'Italian': 'Italian',
                'Thai': 'Thai',
                'Vietnamese': 'Vietnamese',
                'Indonesian': 'Indonesian',
                'Malay': 'Malay',
                'Arabic': 'Arabic',
                'Hindi': 'Hindi',
                'Hebrew': 'Hebrew',
                'Burmese': 'Burmese',
                'Tamil': 'Tamil',
                'Urdu': 'Urdu',
                'Bengali': 'Bengali',
                'Polish': 'Polish',
                'Dutch': 'Dutch',
                'Romanian': 'Romanian',
                'Turkish': 'Turkish',
                'Khmer': 'Khmer',
                'Lao': 'Lao',
                'Cantonese': 'Cantonese',
                'Tagalog': 'Tagalog',
                # è¯­ç§ç¼–ç åˆ°è‹±æ–‡å…¨æ‹¼
                'zh': 'Chinese',
                'en': 'English',
                'ja': 'Japanese',
                'ko': 'Korean',
                'fr': 'French',
                'de': 'German',
                'es': 'Spanish',
                'ru': 'Russian',
                'it': 'Italian',
                'ar': 'Arabic',
                'th': 'Thai',
                'vi': 'Vietnamese',
                'id': 'Indonesian',
                'ms': 'Malay',
                'tl': 'Tagalog',
                'my': 'Burmese',
                'km': 'Khmer',
                'lo': 'Lao',
                'pt': 'Portuguese',
                'hi': 'Hindi',
                'he': 'Hebrew',
                'ta': 'Tamil',
                'ur': 'Urdu',
                'bn': 'Bengali',
                'pl': 'Polish',
                'nl': 'Dutch',
                'ro': 'Romanian',
                'tr': 'Turkish',
                'yue': 'Cantonese',
            }
            return language_mapping.get(lang_name.strip(), lang_name.strip())
        
        # è·å–å¹¶æ˜ å°„è¯­è¨€
        source_lang = map_language_to_qwen_format(trans.get('source_lang', ''))
        target_lang = map_language_to_qwen_format(trans.get('target_lang', 'è‹±è¯­'))
        
        logger.info(f"ğŸ” è¯­è¨€æ˜ å°„è°ƒè¯•:")
        logger.info(f"  åŸå§‹æºè¯­è¨€: {trans.get('source_lang', 'zh')}")
        logger.info(f"  åŸå§‹ç›®æ ‡è¯­è¨€: {trans.get('target_lang', 'en')}")
        logger.info(f"  æ˜ å°„åæºè¯­è¨€: {source_lang}")
        logger.info(f"  æ˜ å°„åç›®æ ‡è¯­è¨€: {target_lang}")
        
        # æ‰§è¡Œç¿»è¯‘
        success = translator.translate_document(
            trans['file_path'],
            trans['target_file'],
            source_lang,
            target_lang
        )
        
        if success:
            # å®Œæˆå¤„ç†
            end_time = datetime.datetime.now()
            spend_time = common.display_spend(start_time, end_time)
            
            # ç»Ÿè®¡ç¿»è¯‘çš„æ–‡æœ¬æ•°é‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            text_count = 1  # Okapi ä»¥æ–‡æ¡£ä¸ºå•ä½å¤„ç†
            
            if trans['run_complete']:
                to_translate.complete(trans, text_count, spend_time)
            
            logger.info(f"âœ… Okapi ç¿»è¯‘å®Œæˆï¼Œç”¨æ—¶: {spend_time}")
            return True
        else:
            logger.error("âŒ Okapi ç¿»è¯‘å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            return start_traditional(trans, start_time, 10)
            
    except Exception as e:
        logger.error(f"âŒ Okapi ç¿»è¯‘å‡ºé”™: {e}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
        return start_traditional(trans, start_time, 10)


def start_traditional(trans, start_time, max_threads):
    """ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
    # ============== Wordæ–‡æ¡£ç¿»è¯‘é…ç½® ==============
    logger.info(f"Wordæ–‡æ¡£ç¿»è¯‘ï¼šä½¿ç”¨ {trans.get('model', 'unknown')} æ¨¡å‹")
    
    # å¦‚æœç”¨æˆ·é€‰æ‹©äº†qwen-mt-plusï¼Œæ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    if trans.get('model') == 'qwen-mt-plus':
        try:
            trans['server'] = 'qwen'
            # å»ºè®®çº¿ç¨‹æ•°ï¼ˆQwenå¹¶å‘å·²æå‡åˆ°1000æ¬¡/åˆ†é’Ÿï¼‰
            if max_threads > 30:
                logger.info(f"å»ºè®®: å½“å‰çº¿ç¨‹æ•° {max_threads}ï¼Œå»ºè®®è®¾ç½®ä¸º 10-30 ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
            elif max_threads < 5:
                logger.info(f"å»ºè®®: å½“å‰çº¿ç¨‹æ•° {max_threads}ï¼Œå¯ä»¥é€‚å½“å¢åŠ åˆ° 10-30 ä»¥æå‡ç¿»è¯‘é€Ÿåº¦")
            from .qwen_translate import check_qwen_availability
            qwen_available, qwen_message = check_qwen_availability()
            logger.info(f"QwenæœåŠ¡æ£€æŸ¥: {qwen_message}")
            if not qwen_available:
                logger.warning("è­¦å‘Š: QwenæœåŠ¡ä¸å¯ç”¨ï¼Œä½†å°†ç»§ç»­å°è¯•ä½¿ç”¨")
        except ImportError:
            logger.warning("è­¦å‘Š: Qwenæ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤ç¿»è¯‘æœåŠ¡")
    


    # ============== è¯‘æ–‡å½¢å¼å¤„ç† ==============
    trans_type = trans.get('type', 'trans_text_only_inherit')  # é»˜è®¤ç»§æ‰¿åŸç‰ˆé¢
    logger.info(f"è¯‘æ–‡å½¢å¼: {trans_type}")
    # ===========================================

    # ============== Wordæ–‡æ¡£é¢„å¤„ç† ==============
    logger.info("å¼€å§‹Wordæ–‡æ¡£é¢„å¤„ç†...")
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
    temp_dir = tempfile.mkdtemp()
    base_name = os.path.basename(trans['file_path'])
    optimized_path = os.path.join(temp_dir, f"optimized_{base_name}")
    
    try:
        # ä½¿ç”¨word-run-optimizerè¿›è¡Œé¢„å¤„ç†
        stats = quick_optimize(trans['file_path'], optimized_path)
        logger.info(f"Wordé¢„å¤„ç†å®Œæˆ: {stats.get('merged_runs', 0)} ä¸ªrunsè¢«åˆå¹¶")
        optimized_doc_path = optimized_path
    except Exception as e:
        logger.warning(f"Wordé¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡æ¡£: {str(e)}")
        optimized_doc_path = trans['file_path']
    # ===========================================
    # optimized_doc_path = trans['file_path'] #å¯ç”¨ä»¥è·³è¿‡é¢„å¤„ç†

    # åŠ è½½Wordæ–‡æ¡£
    try:
        document = Document(optimized_doc_path)
    except Exception as e:
        to_translate.error(trans['id'], f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        cleanup_temp_file(optimized_doc_path)
        return False

    # æå–éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
    texts = []
    
    try:
        # ç›´æ¥ä½¿ç”¨å¤šçº¿ç¨‹æå–ï¼Œç®€åŒ–é…ç½®
        # extract_threads = min(4, max_threads)  # æ–‡æœ¬æå–ä½¿ç”¨è¾ƒå°‘çš„çº¿ç¨‹
        extract_threads = max_threads
        extract_content_for_translation(document, trans['file_path'], texts, extract_threads)
    except Exception as e:
        to_translate.error(trans['id'], f"æ–‡æœ¬æå–å¤±è´¥: {str(e)}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        cleanup_temp_file(optimized_doc_path)
        return False

    # è¿‡æ»¤æ‰ç‰¹æ®Šç¬¦å·å’Œçº¯æ•°å­—
    filtered_texts = []
    for i, item in enumerate(texts):
        if should_translate(item['text']):
            filtered_texts.append(item)
        else:
            # å¯¹äºä¸éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼Œæ ‡è®°ä¸ºå·²å®Œæˆ
            item['complete'] = True
            with print_lock:
                logger.info(f"è·³è¿‡ç¿»è¯‘: {item['text'][:30]}..." if len(
                    item['text']) > 30 else f"è·³è¿‡ç¿»è¯‘: {item['text']}")

    # å¤šçº¿ç¨‹ç¿»è¯‘
    run_translation(trans, filtered_texts, max_threads)

    # ============== å†™å…¥ç¿»è¯‘ç»“æœ ==============
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªé€‚åº”æ ·å¼
    use_adaptive_styles = trans.get('adaptive_styles', False)  # é»˜è®¤ä¸å¯ç”¨
    
    if use_adaptive_styles:
        logger.info("å¯ç”¨æ ·å¼è‡ªé€‚åº”åŠŸèƒ½ï¼šå­—ä½“å¤§å°å’Œè¡Œé—´è·å°†æ ¹æ®ç¿»è¯‘åæ–‡æœ¬é•¿åº¦è‡ªåŠ¨è°ƒæ•´")
        text_count = apply_translations_with_adaptive_styles(document, texts)
    else:
        logger.info("ä½¿ç”¨åŸå§‹æ ·å¼ï¼šä¿æŒåŸå§‹å­—ä½“å¤§å°å’Œè¡Œé—´è·")
        text_count = apply_translations(document, texts)
    # ===========================================

    # ä¿å­˜æ–‡æ¡£
    docx_path = trans['target_file']
    document.save(docx_path)

    # æ™ºèƒ½runæ‹¼æ¥å·²ç»åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç†
    
    # å•ä¸ªrunçš„ç©ºæ ¼å¤„ç†å·²ç»åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å®Œæˆï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç†

    # å¤„ç†æ‰¹æ³¨ç­‰ç‰¹æ®Šå…ƒç´ 
    update_special_elements(docx_path, texts)

    # æ›´æ–°æ–‡æ¡£ç›®å½• - ä½¿ç”¨LibreOfficeæ›´æ–°
    try:
        logger.info("å¼€å§‹ä½¿ç”¨LibreOfficeæ›´æ–°æ–‡æ¡£ç›®å½•...")
        
        # æ£€æŸ¥LibreOfficeæ˜¯å¦å¯ç”¨
        soffice_path = '/usr/bin/soffice'
        if not os.path.exists(soffice_path):
            logger.warning("LibreOfficeæœªå®‰è£…ï¼Œè·³è¿‡ç›®å½•æ›´æ–°")
            return
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp()
        temp_docx = os.path.join(temp_dir, "temp_doc.docx")
        
        try:
            # å¤åˆ¶åŸæ–‡æ¡£åˆ°ä¸´æ—¶ç›®å½•
            shutil.copy2(docx_path, temp_docx)
            
            # ä½¿ç”¨LibreOfficeçš„å­—æ®µæ›´æ–°åŠŸèƒ½
            logger.info("ä½¿ç”¨LibreOfficeè½¬æ¢æ›´æ–°ç›®å½•...")
            cmd = [
                soffice_path, 
                '--headless', 
                '--norestore', 
                '--invisible',
                '--convert-to', 'docx:MS Word 2007 XML',
                '--outdir', temp_dir,
                temp_docx
            ]
            
            result = subprocess.run(cmd, capture_output=True, timeout=60)
            
            if result.returncode == 0:
                # æŸ¥æ‰¾ç”Ÿæˆçš„æ–‡ä»¶
                generated_files = [f for f in os.listdir(temp_dir) if f.endswith('.docx') and f != 'temp_doc.docx']
                
                if generated_files:
                    generated_path = os.path.join(temp_dir, generated_files[0])
                    
                    # å¤‡ä»½åŸæ–‡ä»¶
                    backup_path = docx_path + ".backup"
                    if os.path.exists(docx_path):
                        shutil.copy2(docx_path, backup_path)
                        logger.info(f"åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
                    
                    # æ›¿æ¢åŸæ–‡ä»¶
                    shutil.move(generated_path, docx_path)
                    logger.info("æ–‡æ¡£ç›®å½•æ›´æ–°æˆåŠŸï¼ˆLibreOfficeè½¬æ¢æ–¹æ³•ï¼‰")
                    
                    # æ¸…ç†å¤‡ä»½æ–‡ä»¶
                    if os.path.exists(backup_path):
                        os.remove(backup_path)
                else:
                    logger.warning("LibreOfficeæœªç”Ÿæˆæ–°æ–‡ä»¶")
            else:
                logger.warning(f"LibreOfficeè½¬æ¢å¤±è´¥: {result.stderr.decode()}")
                
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
        
        logger.info("LibreOfficeç›®å½•æ›´æ–°å®Œæˆ")
        
    except Exception as e:
        logger.error(f"ä½¿ç”¨LibreOfficeæ›´æ–°æ–‡æ¡£ç›®å½•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.info("åŸå§‹æ–‡æ¡£ä¿æŒä¸å˜")

    # å®Œæˆå¤„ç†
    end_time = datetime.datetime.now()
    spend_time = common.display_spend(start_time, end_time)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    cleanup_temp_file(optimized_doc_path)
    
    if trans['run_complete']:
        to_translate.complete(trans, text_count, spend_time)
    return True


def should_translate(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åº”è¯¥è¢«ç¿»è¯‘"""
    if not text or not text.strip():
        return False

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜ç¼–å·ï¼ˆå¦‚ "1."ã€"2."ã€"3."ï¼‰
    if TITLE_NUMBER_PATTERN.match(text.strip()):
        return True  # æ ‡é¢˜ç¼–å·éœ€è¦å¤„ç†ï¼Œä½†ä¸ç¿»è¯‘
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—å’Œç®€å•æ ‡ç‚¹
    if NUMBERS_PATTERN.match(text.strip()):
        return False

    # æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šç¬¦å·
    if SPECIAL_SYMBOLS_PATTERN.match(text.strip()):
        return False

    return True


def extract_hyperlink_with_merge(hyperlink, texts):
    """æå–è¶…é“¾æ¥å†…å®¹ï¼Œä½¿ç”¨ä¿å®ˆrunåˆå¹¶"""
    hyperlink_runs = list(hyperlink.runs)
    
    # æš‚æ—¶æ³¨é‡Šæ‰runåˆå¹¶ï¼Œè®©æ¯ä¸ªè¶…é“¾æ¥ä¿æŒç‹¬ç«‹
    # ä½¿ç”¨ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥
    # merged_runs = conservative_run_merge(hyperlink_runs)
    
    # ç›´æ¥ä½¿ç”¨åŸå§‹runï¼Œä¸åˆå¹¶
    merged_runs = []
    for run in hyperlink_runs:
        if check_text(run.text):
            merged_runs.append({
                'text': run.text,
                'type': 'merged',  # æ”¹ä¸ºmergedç±»å‹ï¼Œè¿™æ ·apply_translationsæ‰èƒ½å¤„ç†
                'runs': [run]
            })
    
    for merged_item in merged_runs:
        if not check_text(merged_item['text']):
            continue
        
        texts.append({
            "text": merged_item['text'],
            "type": "merged_run",
            "merged_item": merged_item,
            "complete": False,
            "context_type": "hyperlink"  # æ ‡è®°ä¸ºè¶…é“¾æ¥
        })


def extract_paragraph_with_merge(paragraph, texts, context_type, paragraph_index=None, total_paragraphs=None):
    """æå–æ®µè½å†…å®¹ï¼Œä½¿ç”¨ä¿å®ˆrunåˆå¹¶ï¼ˆä¸æ·»åŠ ä¸Šä¸‹æ–‡ï¼‰"""
    paragraph_runs = list(paragraph.runs)
    
    # æš‚æ—¶æ³¨é‡Šæ‰ä¸»æ ‡é¢˜è¯†åˆ«ï¼Œé¿å…è¡¨æ ¼ä¸­çš„æ®µè½è¢«é”™è¯¯è¯†åˆ«
    # æ£€æŸ¥æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªæ®µè½ï¼ˆé€šå¸¸æ˜¯ä¸»æ ‡é¢˜ï¼‰
    # is_main_title = paragraph_index == 0
    is_main_title = False  # æš‚æ—¶ç¦ç”¨ä¸»æ ‡é¢˜è¯†åˆ«
    
    # æš‚æ—¶æ³¨é‡Šæ‰runåˆå¹¶ï¼Œè®©æ¯ä¸ªæ®µè½ä¿æŒç‹¬ç«‹
    # ä½¿ç”¨ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥ï¼Œä¼ é€’is_main_titleå‚æ•°
    # merged_runs = conservative_run_merge(paragraph_runs, is_main_title=is_main_title)
    
    # ç›´æ¥ä½¿ç”¨åŸå§‹runï¼Œä¸åˆå¹¶
    merged_runs = []
    for run in paragraph_runs:
        if check_text(run.text):
            merged_runs.append({
                'text': run.text,
                'type': 'merged',  # æ”¹ä¸ºmergedç±»å‹ï¼Œè¿™æ ·apply_translationsæ‰èƒ½å¤„ç†
                'runs': [run]
            })
    
    # ç®€åŒ–æ—¥å¿—è¾“å‡ºï¼Œåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug(f"æ®µè½å¤„ç†: ç´¢å¼•{paragraph_index}/{total_paragraphs}, ç±»å‹{context_type}, runæ•°é‡{len(paragraph_runs)}")
    
    # ä¸»æ ‡é¢˜å¤„ç†å·²ç»è¢«ç¦ç”¨ï¼Œç›´æ¥å¤„ç†æ‰€æœ‰æ®µè½
    for merged_item in merged_runs:
        if not check_text(merged_item['text']):
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
        has_image = False
        for run in merged_item['runs']:
            if check_if_image(run):
                has_image = True
                break
        
        if has_image:
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜ç¼–å·ï¼ˆå¦‚ "1."ã€"2."ã€"3."ï¼‰
        if TITLE_NUMBER_PATTERN.match(merged_item['text'].strip()):
            # æ ‡é¢˜ç¼–å·ä¸ç¿»è¯‘ï¼Œä½†ä¿ç•™åœ¨æ–‡æ¡£ä¸­
            texts.append({
                "text": merged_item['text'],
                "type": "merged_run",
                "merged_item": merged_item,
                "complete": True,  # æ ‡è®°ä¸ºå·²å®Œæˆï¼Œä¸éœ€è¦ç¿»è¯‘
                "context_type": context_type,
                "is_title_number": True  # æ ‡è®°ä¸ºæ ‡é¢˜ç¼–å·
            })
            continue
        
        # è¿‡æ»¤çº¯æ•°å­—
        if NUMBERS_PATTERN.match(merged_item['text'].strip()):
            continue
        
        # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯ç›®å½•é¡¹ï¼ˆä»¥ -æ•°å­— æˆ– â€”æ•°å­— ç»“å°¾ï¼‰
        match = TOC_PAGE_PATTERN.search(merged_item['text'])
        if match and context_type == "body":  # å‡è®¾ç›®å½•åœ¨body
            page_num = match.group(0)
            # ä½¿ç”¨strip()å»é™¤å‰åç©ºæ ¼ï¼Œç¡®ä¿æ–‡æœ¬å®Œæ•´æ€§
            main_text = merged_item['text'][:match.start()].strip()
            
            if main_text and should_translate(main_text):
                # åˆ›å»ºæ–‡æœ¬é¡¹
                texts.append({
                    "text": main_text,
                    "type": "merged_run",
                    "merged_item": merged_item,  # å…±äº«runsï¼Œç¿»è¯‘åéœ€å°å¿ƒæ›¿æ¢
                    "complete": False,
                    "context_type": context_type,
                    "is_toc_text": True,
                    "original_page_num": page_num
                })
            
                # åˆ›å»ºé¡µç é¡¹ï¼ˆä¸ç¿»è¯‘ï¼‰
                if should_translate(page_num):  # é¢å¤–æ£€æŸ¥ï¼Œä½†çº¯æ•°å­—åº”False
                    texts.append({
                        "text": page_num,
                        "type": "merged_run",
                        "merged_item": {'text': page_num, 'runs': merged_item['runs'][-1:], 'type': 'single'},  # å–æœ€åä¸€ä¸ªrunä½œä¸ºé¡µç 
                        "complete": False,  # å¦‚æœéœ€è¦ç¿»è¯‘
                        "context_type": context_type,
                        "is_toc_page": True
                    })
                else:
                    texts.append({
                        "text": page_num,
                        "type": "merged_run",
                        "merged_item": {'text': page_num, 'runs': merged_item['runs'][-1:], 'type': 'single'},
                        "complete": True,
                        "context_type": context_type,
                        "is_toc_page": True
                    })
        else:
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ®µè½ï¼Œæ ‡è®°ä¸ºä¸»æ ‡é¢˜
            if is_main_title:
                context_type = "main_title"
                logger.info(f"æ£€æµ‹åˆ°ä¸»æ ‡é¢˜æ®µè½: '{merged_item['text']}'")
            
            texts.append({
                "text": merged_item['text'],
                "type": "merged_run",
                "merged_item": merged_item,
                "complete": False,
                "context_type": context_type,  # æ ‡è®°ç±»å‹
                "is_main_title": is_main_title  # æ·»åŠ ä¸»æ ‡é¢˜æ ‡è®°
            })
    
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug(f"æ®µè½å¤„ç†å®Œæˆ: ç´¢å¼•{paragraph_index}")


def extract_content_for_translation(document, file_path, texts, max_threads=4):
    """æå–éœ€è¦ç¿»è¯‘çš„å†…å®¹ï¼Œä½¿ç”¨å®‰å…¨çš„å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†"""
    
    # çº¿ç¨‹å®‰å…¨çš„æ–‡æœ¬åˆ—è¡¨
    from threading import Lock
    texts_lock = Lock()
    
    def add_text_safe(text_item):
        """çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ æ–‡æœ¬é¡¹"""
        with texts_lock:
            texts.append(text_item)
    
    def process_paragraphs_parallel():
        """å¹¶è¡Œå¤„ç†æ®µè½ï¼ˆä¿æŒé¡ºåºï¼‰"""
        # å°†æ®µè½åˆ†ç»„ï¼Œæ¯ç»„åŒ…å«è¿ç»­çš„æ®µè½
        paragraph_groups = []
        current_group = []
        
        for i, paragraph in enumerate(document.paragraphs):
            current_group.append((i, paragraph))
            
            # æ¯10ä¸ªæ®µè½ä¸ºä¸€ç»„ï¼Œæˆ–è€…é‡åˆ°è¡¨æ ¼æ—¶åˆ†ç»„
            if len(current_group) >= 10 or (i < len(document.paragraphs) - 1 and 
                hasattr(document.paragraphs[i+1], '_element') and 
                document.paragraphs[i+1]._element.getnext() is not None):
                paragraph_groups.append(current_group)
                current_group = []
        
        if current_group:
            paragraph_groups.append(current_group)
        
        def process_paragraph_group(group):
            """å¤„ç†ä¸€ç»„è¿ç»­çš„æ®µè½"""
            local_texts = []
            
            for index, paragraph in group:
                # å¤„ç†æ­£æ–‡æ®µè½ï¼ˆè·³è¿‡è¶…é“¾æ¥runï¼‰
                # ç›´æ¥è°ƒç”¨extract_paragraph_with_mergeï¼Œä¼ é€’æ®µè½ç´¢å¼•ä¿¡æ¯
                extract_paragraph_with_merge(paragraph, local_texts, "body", index, len(document.paragraphs))
                
                # å¤„ç†è¶…é“¾æ¥
                for hyperlink in paragraph.hyperlinks:
                    extract_hyperlink_with_merge(hyperlink, local_texts)
                
                # å¤„ç†æ®µè½ä¸­çš„æ–‡æœ¬æ¡†
                for run in paragraph.runs:
                    if check_if_textbox(run):
                        # å¤„ç† DrawingML
                        drawing_elem = run.element.find('.//w:drawing', run.element.nsmap)
                        if drawing_elem is not None:
                            txbx_elem = drawing_elem.find('.//w:txbxContent', drawing_elem.nsmap)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", index, len(document.paragraphs))
                        
                        # å¤„ç† VML
                        namespaces = {**run.element.nsmap, 'v': 'urn:schemas-microsoft-com:vml'}
                        vtextbox_elem = run.element.find('.//v:textbox', namespaces)
                        if vtextbox_elem is not None:
                            txbx_elem = vtextbox_elem.find('.//w:txbxContent', namespaces)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", index, len(document.paragraphs))
            
            # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ä¸»åˆ—è¡¨
            for text_item in local_texts:
                add_text_safe(text_item)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ®µè½ç»„
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(process_paragraph_group, group) 
                      for group in paragraph_groups]
            for future in as_completed(futures):
                future.result()
    
    def process_tables_parallel():
        """å¹¶è¡Œå¤„ç†è¡¨æ ¼ï¼ˆè¡¨æ ¼ä¹‹é—´æ— ä¾èµ–ï¼‰"""
        def process_table(table):
            local_texts = []
            # ä½¿ç”¨æ–°çš„è¡¨æ ¼å¤„ç†å‡½æ•°ï¼ŒåŒ…å«å¸ƒå±€è°ƒæ•´
            process_table_with_layout_adjustment(table, local_texts)
            
            # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ä¸»åˆ—è¡¨
            for text_item in local_texts:
                add_text_safe(text_item)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è¡¨æ ¼
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(process_table, table) 
                      for table in document.tables]
            for future in as_completed(futures):
                future.result()
    
    def process_sections_parallel():
        """å¹¶è¡Œå¤„ç†é¡µçœ‰é¡µè„šï¼ˆsectionä¹‹é—´æ— ä¾èµ–ï¼‰"""
        def process_section(section):
            local_texts = []
            
            # å¤„ç†é¡µçœ‰
            for paragraph in section.header.paragraphs:
                extract_paragraph_with_merge(paragraph, local_texts, "header", 0, 1)  # é¡µçœ‰æ®µè½ç´¢å¼•ä¸º0
                # å¤„ç†é¡µçœ‰ä¸­çš„è¶…é“¾æ¥
                for hyperlink in paragraph.hyperlinks:
                    extract_hyperlink_with_merge(hyperlink, local_texts)
                # å¤„ç†é¡µçœ‰ä¸­çš„æ–‡æœ¬æ¡†
                for run in paragraph.runs:
                    if check_if_textbox(run):
                        # å¤„ç† DrawingML
                        drawing_elem = run.element.find('.//w:drawing', run.element.nsmap)
                        if drawing_elem is not None:
                            txbx_elem = drawing_elem.find('.//w:txbxContent', drawing_elem.nsmap)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", 0, 1)
                        
                        # å¤„ç† VML
                        namespaces = {**run.element.nsmap, 'v': 'urn:schemas-microsoft-com:vml'}
                        vtextbox_elem = run.element.find('.//v:textbox', namespaces)
                        if vtextbox_elem is not None:
                            txbx_elem = vtextbox_elem.find('.//w:txbxContent', namespaces)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", 0, 1)

            # å¤„ç†é¡µè„š
            for paragraph in section.footer.paragraphs:
                extract_paragraph_with_merge(paragraph, local_texts, "footer", 0, 1)  # é¡µè„šæ®µè½ç´¢å¼•ä¸º0
                # å¤„ç†é¡µè„šä¸­çš„è¶…é“¾æ¥
                for hyperlink in paragraph.hyperlinks:
                    extract_hyperlink_with_merge(hyperlink, local_texts)
                # å¤„ç†é¡µè„šä¸­çš„æ–‡æœ¬æ¡†
                for run in paragraph.runs:
                    if check_if_textbox(run):
                        # å¤„ç† DrawingML
                        drawing_elem = run.element.find('.//w:drawing', run.element.nsmap)
                        if drawing_elem is not None:
                            txbx_elem = drawing_elem.find('.//w:txbxContent', drawing_elem.nsmap)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", 0, 1)
                        
                        # å¤„ç† VML
                        namespaces = {**run.element.nsmap, 'v': 'urn:schemas-microsoft-com:vml'}
                        vtextbox_elem = run.element.find('.//v:textbox', namespaces)
                        if vtextbox_elem is not None:
                            txbx_elem = vtextbox_elem.find('.//w:txbxContent', namespaces)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", 0, 1)
            
            # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ä¸»åˆ—è¡¨
            for text_item in local_texts:
                add_text_safe(text_item)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†sections
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(process_section, section) 
                      for section in document.sections]
            for future in as_completed(futures):
                future.result()
    
    def process_inline_shapes_parallel():
        """å¹¶è¡Œå¤„ç†å†…åµŒå½¢çŠ¶ï¼ˆinline shapesï¼‰"""
        def process_shape(shape):
            local_texts = []
            try:
                if hasattr(shape, 'has_text_frame') and shape.has_text_frame:
                    for paragraph in shape.text_frame.paragraphs:
                        extract_paragraph_with_merge(paragraph, local_texts, "textbox", 0, 1)  # å½¢çŠ¶æ®µè½ç´¢å¼•ä¸º0
                        # å¤„ç†å½¢çŠ¶ä¸­çš„è¶…é“¾æ¥
                        for hyperlink in paragraph.hyperlinks:
                            extract_hyperlink_with_merge(hyperlink, local_texts)
            except Exception as e:
                logger.error(f"å¤„ç†å†…åµŒå½¢çŠ¶æ—¶å‡ºé”™: {str(e)}")
            
            # çº¿ç¨‹å®‰å…¨åœ°æ·»åŠ åˆ°ä¸»åˆ—è¡¨
            for text_item in local_texts:
                add_text_safe(text_item)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†å½¢çŠ¶
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(process_shape, shape) 
                       for shape in document.inline_shapes]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"å†…åµŒå½¢çŠ¶çº¿ç¨‹å¼‚å¸¸: {str(e)}")
    
    # æŒ‰é¡ºåºæ‰§è¡Œå„ä¸ªéƒ¨åˆ†ï¼Œé¿å…ä¾èµ–é—®é¢˜
    logger.info("å¼€å§‹å®‰å…¨çš„å¤šçº¿ç¨‹æ–‡æœ¬æå–...")
    
    # 1. å¹¶è¡Œå¤„ç†æ®µè½ï¼ˆåˆ†ç»„å¤„ç†ï¼Œä¿æŒé¡ºåºï¼‰
    process_paragraphs_parallel()
    
    # 2. å¹¶è¡Œå¤„ç†è¡¨æ ¼ï¼ˆè¡¨æ ¼ä¹‹é—´æ— ä¾èµ–ï¼‰
    if document.tables:
        process_tables_parallel()
    
    # 3. å¹¶è¡Œå¤„ç†é¡µçœ‰é¡µè„šï¼ˆsectionä¹‹é—´æ— ä¾èµ–ï¼‰
    if document.sections:
        process_sections_parallel()
    
    # 4. å¤„ç†å†…åµŒå½¢çŠ¶ï¼ˆinline shapesï¼Œå¯èƒ½åŒ…å«æ–‡æœ¬æ¡†ï¼‰
    if document.inline_shapes:
        process_inline_shapes_parallel()
    
    # 5. æ‰¹æ³¨å†…å®¹ï¼ˆå•çº¿ç¨‹ï¼Œå› ä¸ºéœ€è¦ZIPæ“ä½œï¼‰
    if hasattr(document, 'comments') and document.comments:
        extract_comments(file_path, texts)
    
    logger.info(f"æ–‡æœ¬æå–å®Œæˆï¼Œå…±æå– {len(texts)} ä¸ªæ–‡æœ¬é¡¹")


# å·²åˆ é™¤å•çº¿ç¨‹ç‰ˆæœ¬ï¼Œç»Ÿä¸€ä½¿ç”¨å¤šçº¿ç¨‹æå–


def get_run_format_key(run):
    """è·å–runçš„æ ¼å¼æ ‡è¯†ï¼Œç”¨äºå¿«é€Ÿæ¯”è¾ƒ"""
    try:
        font_name = run.font.name if run.font.name else None
        font_size = run.font.size if run.font.size else None
        bold = run.bold
        italic = run.italic
        underline = run.underline
        color = run.font.color.rgb if run.font.color.rgb else None
        
        return (font_name, font_size, bold, italic, underline, color)
    except:
        return None

def are_runs_compatible(run1, run2, is_main_title=False):
    """æ£€æŸ¥ä¸¤ä¸ªrunæ˜¯å¦å…¼å®¹ï¼Œå¯ä»¥åˆå¹¶"""
    
    # å¦‚æœæ˜¯ä¸»æ ‡é¢˜ï¼Œæ”¾å®½æ ¼å¼å…¼å®¹æ€§æ£€æŸ¥
    if is_main_title:
        # ä¸»æ ‡é¢˜åªéœ€è¦æ£€æŸ¥å­—ä½“å¤§å°æ˜¯å¦ç›¸åŒ
        try:
            size1 = run1.font.size.pt if run1.font.size else None
            size2 = run2.font.size.pt if run2.font.size else None
            
            # å¦‚æœå­—ä½“å¤§å°ç›¸åŒæˆ–éƒ½ä¸ºç©ºï¼Œåˆ™è®¤ä¸ºå…¼å®¹
            if size1 == size2 or (size1 is None and size2 is None):
                return True
            else:
                return False
        except:
            # å¦‚æœè·å–å­—ä½“å¤§å°å¤±è´¥ï¼Œé»˜è®¤å…¼å®¹
            return True
    
    # åŸæœ‰çš„å…¼å®¹æ€§æ£€æŸ¥é€»è¾‘
    try:
        # è·å–æ ¼å¼ä¿¡æ¯
        format1 = get_run_format_key(run1)
        format2 = get_run_format_key(run2)
        
        # å¦‚æœæ ¼å¼å®Œå…¨ç›¸åŒï¼Œåˆ™å…¼å®¹
        if format1 == format2:
            return True
        
        # å¦‚æœæ ¼å¼å·®å¼‚å¾ˆå°ï¼ˆæ¯”å¦‚åªæ˜¯å­—ä½“åç§°ä¸åŒï¼‰ï¼Œä¹Ÿå¯ä»¥åˆå¹¶
        # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´å…¼å®¹æ€§åˆ¤æ–­çš„ä¸¥æ ¼ç¨‹åº¦
        
        return False
    except:
        # å¦‚æœè·å–æ ¼å¼ä¿¡æ¯å¤±è´¥ï¼Œé»˜è®¤ä¸å…¼å®¹
        return False


def conservative_run_merge(paragraph_runs, max_merge_length=1000, is_main_title=False):
    """ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥"""
    
    merged = []
    current_group = []
    current_length = 0
    original_count = len([r for r in paragraph_runs if check_text(r.text)])
    merged_count = 0
    
    # ç®€åŒ–è°ƒè¯•ä¿¡æ¯ï¼Œåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤º
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug(f"conservative_run_merge: è¾“å…¥{len(paragraph_runs)}ä¸ªrun, æœ‰æ•ˆ{original_count}ä¸ª, ä¸»æ ‡é¢˜{is_main_title}")
    
    # å¦‚æœæ˜¯ä¸»æ ‡é¢˜ï¼Œå¼ºåˆ¶åˆå¹¶æ‰€æœ‰run
    if is_main_title and original_count > 1:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ä¸»æ ‡é¢˜æ®µè½ï¼Œå¼ºåˆ¶åˆå¹¶æ‰€æœ‰{original_count}ä¸ªrun")
        
        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆrun
        all_runs = [r for r in paragraph_runs if check_text(r.text)]
        
        # åˆ›å»ºåˆå¹¶åçš„é¡¹
        merged_item = merge_compatible_runs(all_runs)
        merged.append(merged_item)
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"conservative_run_mergeå®Œæˆ: åˆå¹¶å{len(merged)}ä¸ªé¡¹")
        
        return merged
    
    # åŸæœ‰çš„åˆå¹¶é€»è¾‘
    for i, run in enumerate(paragraph_runs):
        # è·³è¿‡åŒ…å«å›¾ç‰‡çš„run
        if check_if_image(run):
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"Run {i}: '{run.text}' - åŒ…å«å›¾ç‰‡ï¼Œè·³è¿‡")
            # ä¿å­˜å½“å‰ç»„
            if current_group:
                merged.append(merge_compatible_runs(current_group))
                if len(current_group) > 1:
                    merged_count += len(current_group) - 1
                current_group = []
                current_length = 0
            
            # å›¾ç‰‡runå•ç‹¬å¤„ç†ï¼Œä½†ä¸æ·»åŠ åˆ°ç¿»è¯‘åˆ—è¡¨
            continue
        
        if not check_text(run.text):
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"Run {i}: '{run.text}' - æ— æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡")
            continue
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Run {i}: '{run.text}' - é•¿åº¦: {len(run.text)}, å½“å‰ç»„é•¿åº¦: {current_length}")
        
        # åªåˆå¹¶è¾ƒçŸ­çš„runï¼ˆé€šå¸¸æ˜¯ç©ºæ ¼ã€æ ‡ç‚¹ã€çŸ­è¯ã€çŸ­è¯­ï¼‰
        if len(run.text) <= 20 and current_length < max_merge_length:
            # æ£€æŸ¥æ ¼å¼å…¼å®¹æ€§ï¼Œä¼ é€’is_main_titleå‚æ•°
            if not current_group or are_runs_compatible(current_group[-1], run, is_main_title):
                current_group.append(run)
                current_length += len(run.text)
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"  æ·»åŠ åˆ°å½“å‰ç»„ï¼Œå½“å‰ç»„: {[r.text for r in current_group]}")
                continue
            else:
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"  æ ¼å¼ä¸å…¼å®¹ï¼Œä¸åˆå¹¶")
        
        # ä¿å­˜å½“å‰ç»„
        if current_group:
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(f"ä¿å­˜å½“å‰ç»„: {[r.text for r in current_group]}")
            merged.append(merge_compatible_runs(current_group))
            if len(current_group) > 1:
                merged_count += len(current_group) - 1  # è®°å½•åˆå¹¶çš„runæ•°é‡
            current_group = []
            current_length = 0
        
        # å½“å‰runå•ç‹¬å¤„ç†
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Run {i} å•ç‹¬å¤„ç†: '{run.text}'")
        merged.append({
            'text': run.text,
            'runs': [run],
            'type': 'single'
        })
    
    if current_group:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ä¿å­˜æœ€åçš„ç»„: {[r.text for r in current_group]}")
        merged.append(merge_compatible_runs(current_group))
        if len(current_group) > 1:
            merged_count += len(current_group) - 1
    
    # æ‰“å°åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
    if merged_count > 0:
        with print_lock:
            logger.info(f"Runåˆå¹¶ä¼˜åŒ–: åŸå§‹{original_count}ä¸ªrun -> åˆå¹¶å{len(merged)}ä¸ªï¼Œå‡å°‘äº†{merged_count}ä¸ªAPIè°ƒç”¨")
    
    if logger.isEnabledFor(logging.DEBUG):
        logger.debug(f"conservative_run_mergeå®Œæˆ: åˆå¹¶å{len(merged)}ä¸ªé¡¹")
    
    return merged


def smart_space_preservation(original_runs, translated_text):
    """æ™ºèƒ½ä¿æŒåŸå§‹runä¹‹é—´çš„ç©ºæ ¼å…³ç³»"""
    if not original_runs:
        return translated_text
    
    # åˆ†æåŸå§‹runä¹‹é—´çš„ç©ºæ ¼å…³ç³»
    space_pattern = []
    for i, run in enumerate(original_runs):
        if i > 0:
            prev_run = original_runs[i-1]
            # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ ¼åˆ†éš”
            if prev_run.text.endswith(' ') or run.text.startswith(' '):
                space_pattern.append(True)  # æœ‰ç©ºæ ¼åˆ†éš”
            else:
                space_pattern.append(False)  # æ— ç©ºæ ¼åˆ†éš”
        space_pattern.append(False)  # ç¬¬ä¸€ä¸ªrun
    
    # å¦‚æœåŸå§‹runä¹‹é—´æ²¡æœ‰ç©ºæ ¼åˆ†éš”ï¼Œç¿»è¯‘åä¹Ÿä¸æ·»åŠ 
    if not any(space_pattern):
        return translated_text
    
    # å¦‚æœæœ‰ç©ºæ ¼åˆ†éš”ï¼Œåœ¨é€‚å½“ä½ç½®æ·»åŠ ç©ºæ ¼
    # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å®ç°æ›´å¤æ‚çš„ç©ºæ ¼å¤„ç†é€»è¾‘
    return translated_text


def merge_compatible_runs(run_group):
    """åˆå¹¶å…¼å®¹çš„runç»„ï¼Œä¿æŒåŸå§‹æ ¼å¼ï¼ˆä¸æ·»åŠ ç©ºæ ¼ï¼‰"""
    
    # ç›´æ¥è¿æ¥æ–‡æœ¬ï¼Œä¸æ·»åŠ é¢å¤–ç©ºæ ¼
    # ç©ºæ ¼å¤„ç†å°†åœ¨ç¿»è¯‘åçš„åˆ†é…é˜¶æ®µè¿›è¡Œ
    merged_text = "".join(run.text for run in run_group)
    
    return {
        'text': merged_text,
        'runs': run_group,
        'type': 'merged'
    }


def extract_paragraph_with_context(paragraph, texts, context_type):
    """ä¸ºæ­£æ–‡æ®µè½æ·»åŠ ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥"""
    
    # è¿‡æ»¤æ‰è¶…é“¾æ¥çš„runï¼Œé¿å…é‡å¤å¤„ç†
    paragraph_runs = []
    for run in paragraph.runs:
        # æ£€æŸ¥runæ˜¯å¦å±äºè¶…é“¾æ¥
        is_hyperlink_run = False
        for hyperlink in paragraph.hyperlinks:
            if run in hyperlink.runs:
                is_hyperlink_run = True
                break
        
        if not is_hyperlink_run:
            paragraph_runs.append(run)
    
    # æš‚æ—¶æ³¨é‡Šæ‰runåˆå¹¶ï¼Œè®©æ¯ä¸ªæ®µè½ä¿æŒç‹¬ç«‹
    # ä½¿ç”¨ä¿å®ˆçš„runåˆå¹¶ç­–ç•¥
    # merged_runs = conservative_run_merge(paragraph_runs)
    
    # ç›´æ¥ä½¿ç”¨åŸå§‹runï¼Œä¸åˆå¹¶
    merged_runs = []
    for run in paragraph_runs:
        if check_text(run.text):
            merged_runs.append({
                'text': run.text,
                'type': 'merged',  # æ”¹ä¸ºmergedç±»å‹ï¼Œè¿™æ ·apply_translationsæ‰èƒ½å¤„ç†
                'runs': [run]
            })
    
    for i, merged_item in enumerate(merged_runs):
        if not check_text(merged_item['text']):
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«å›¾ç‰‡åˆ™è·³è¿‡ï¼Œé¿å…åœ¨å›¾ç‰‡åæ·»åŠ ç©ºè¡Œ
        has_image = False
        for run in merged_item['runs']:
            if check_if_image(run):
                has_image = True
                break
        
        if has_image:
            continue
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯æ®µè½è¾¹ç•Œ
        is_start = i == 0
        is_end = i == len(merged_runs) - 1
        
        # æ ¹æ®è¾¹ç•Œæƒ…å†µè·å–ä¸Šä¸‹æ–‡
        if is_start:
            # æ®µè½å¼€å¤´ï¼šåªè¦åæ–‡
            context_before = ""
            context_after = get_context_after_merged(merged_runs, i, 2)
        elif is_end:
            # æ®µè½ç»“å°¾ï¼šåªè¦å‰æ–‡
            context_before = get_context_before_merged(merged_runs, i, 2)
            context_after = ""
        else:
            # æ­£å¸¸æƒ…å†µï¼šå‰åæ–‡éƒ½è¦
            context_before = get_context_before_merged(merged_runs, i, 2)
            context_after = get_context_after_merged(merged_runs, i, 2)
        
        # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„ç¿»è¯‘æ–‡æœ¬
        context_parts = []
        if context_before:
            context_parts.append(f"[å‰æ–‡: {context_before}]")
        context_parts.append(merged_item['text'])
        if context_after:
            context_parts.append(f"[åæ–‡: {context_after}]")
        
        # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥ï¼Œé¿å…æ·»åŠ é¢å¤–ç©ºæ ¼
        context_text = "".join(context_parts)
        
        texts.append({
            "text": merged_item['text'],           # åŸå§‹æ–‡æœ¬
            "context_text": context_text,  # å¸¦ä¸Šä¸‹æ–‡çš„æ–‡æœ¬
            "type": "merged_run",
            "merged_item": merged_item,  # åŒ…å«runsåˆ—è¡¨
            "complete": False,
            "context_type": context_type,  # æ ‡è®°ä¸ºæ­£æ–‡
            "is_boundary": is_start or is_end
        })


def get_context_before(runs, current_index, window_size):
    """è·å–å‰æ–‡ä¸Šä¸‹æ–‡"""
    start_index = max(0, current_index - window_size)
    context_runs = runs[start_index:current_index]
    
    # æ”¶é›†æ–‡æœ¬
    context_texts = []
    for run in context_runs:
        if check_text(run.text):
            context_texts.append(run.text)
    
    return ''.join(context_texts)


def get_context_after(runs, current_index, window_size):
    """è·å–åæ–‡ä¸Šä¸‹æ–‡"""
    end_index = min(len(runs), current_index + window_size + 1)
    context_runs = runs[current_index + 1:end_index]
    
    # æ”¶é›†æ–‡æœ¬
    context_texts = []
    for run in context_runs:
        if check_text(run.text):
            context_texts.append(run.text)
    
    return ''.join(context_texts)


def get_context_before_merged(merged_runs, current_index, window_size):
    """è·å–åˆå¹¶runçš„å‰æ–‡ä¸Šä¸‹æ–‡ï¼Œä¿æŒåŸå§‹ç©ºæ ¼æ ¼å¼"""
    start_index = max(0, current_index - window_size)
    context_items = merged_runs[start_index:current_index]
    
    # æ”¶é›†æ–‡æœ¬ï¼Œä¿æŒåŸå§‹æ ¼å¼
    context_texts = []
    for item in context_items:
        if check_text(item['text']):
            context_texts.append(item['text'])
    
    # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥ï¼Œä¿æŒåŸå§‹ç©ºæ ¼
    return ''.join(context_texts)


def get_context_after_merged(merged_runs, current_index, window_size):
    """è·å–åˆå¹¶runçš„åæ–‡ä¸Šä¸‹æ–‡ï¼Œä¿æŒåŸå§‹ç©ºæ ¼æ ¼å¼"""
    end_index = min(len(merged_runs), current_index + window_size + 1)
    context_items = merged_runs[current_index + 1:end_index]
    
    # æ”¶é›†æ–‡æœ¬ï¼Œä¿æŒåŸå§‹æ ¼å¼
    context_texts = []
    for item in context_items:
        if check_text(item['text']):
            context_texts.append(item['text'])
    
    # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥ï¼Œä¿æŒåŸå§‹ç©ºæ ¼
    return ''.join(context_texts)


def extract_comments(file_path, texts):
    """æå–æ–‡æ¡£ä¸­çš„æ‰¹æ³¨"""
    try:
        with zipfile.ZipFile(file_path, 'r') as docx:
            if 'word/comments.xml' in docx.namelist():
                with docx.open('word/comments.xml') as f:
                    tree = ET.parse(f)
                    root = tree.getroot()

                    # å®šä¹‰å‘½åç©ºé—´
                    namespaces = {
                        'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                    }

                    # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ³¨
                    for comment in root.findall('.//w:comment', namespaces) or root.findall(
                            './/{*}comment'):
                        # å°è¯•ä¸åŒæ–¹å¼è·å–ID
                        comment_id = None
                        for attr_name in [
                            '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id',
                            'id']:
                            if attr_name in comment.attrib:
                                comment_id = comment.attrib[attr_name]
                                break

                        if not comment_id:
                            continue

                        # æ”¶é›†æ‰¹æ³¨ä¸­çš„æ‰€æœ‰æ–‡æœ¬
                        text_elements = comment.findall('.//w:t', namespaces) or comment.findall(
                            './/{*}t')
                        for t_elem in text_elements:
                            if t_elem.text and check_text(t_elem.text):
                                texts.append({
                                    "text": t_elem.text,
                                    "type": "comment",
                                    "comment_id": comment_id,
                                    "complete": False,
                                    "original_text": t_elem.text  # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºåŒ¹é…
                                })
    except Exception as e:
        logger.error(f"æå–æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")


def run_translation(trans, texts, max_threads):
    """æ‰§è¡Œå¤šçº¿ç¨‹ç¿»è¯‘"""
    if not texts:
        logger.info("æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„å†…å®¹")
        return

    event = threading.Event()
    
    with print_lock:
        logger.info(f"å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        logger.info(f"ç¿»è¯‘æœåŠ¡: {trans.get('server', 'unknown')}")  # ç¡®è®¤ä½¿ç”¨çš„ç¿»è¯‘æœåŠ¡

    # è¿›åº¦æ›´æ–°ç›¸å…³å˜é‡
    completed_count = 0
    total_count = len(texts)
    progress_lock = threading.Lock()
    
    def update_progress():
        """æ›´æ–°ç¿»è¯‘è¿›åº¦"""
        nonlocal completed_count
        with progress_lock:
            completed_count += 1
            progress_percentage = min((completed_count / total_count) * 100, 100.0)
            logger.info(f"ç¿»è¯‘è¿›åº¦: {completed_count}/{total_count} ({progress_percentage:.1f}%)")
            
            # æ›´æ–°æ•°æ®åº“è¿›åº¦
            try:
                from .to_translate import db
                db.execute("update translate set process=%s where id=%s", 
                         str(format(progress_percentage, '.1f')), 
                         trans['id'])
                
                # å¦‚æœè¿›åº¦è¾¾åˆ°100%ï¼Œç«‹å³æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
                if progress_percentage >= 100.0:
                    from datetime import datetime
                    import pytz
                    end_time = datetime.now(pytz.timezone('Asia/Shanghai'))
                    db.execute(
                        "update translate set status='done',end_at=%s,process=100 where id=%s",
                        end_time, trans['id']
                    )
                    logger.info("âœ… ç¿»è¯‘å®Œæˆï¼ŒçŠ¶æ€å·²æ›´æ–°ä¸ºå·²å®Œæˆ")
                    
            except Exception as e:
                logger.error(f"æ›´æ–°è¿›åº¦å¤±è´¥: {str(e)}")

    # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œç¿»è¯‘ä»»åŠ¡
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        # æäº¤æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
        futures = []
        for i in range(len(texts)):
            future = executor.submit(to_translate.get, trans, event, texts, i)
            futures.append(future)
            with print_lock:
                logger.info(f"æäº¤ç¿»è¯‘ä»»åŠ¡ {i}")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(futures):
            try:
                future.result()  # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                update_progress()  # æ›´æ–°è¿›åº¦
            except Exception as e:
                with print_lock:
                    logger.error(f"ç¿»è¯‘ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}")
                if not event.is_set():
                    event.set()  # è®¾ç½®äº‹ä»¶ï¼Œé€šçŸ¥å…¶ä»–çº¿ç¨‹åœæ­¢

    with print_lock:
        logger.info("æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡å·²å®Œæˆ")


def calculate_adaptive_font_size(original_text, translated_text, original_font_size):
    """æ ¹æ®ç¿»è¯‘åæ–‡æœ¬é•¿åº¦è®¡ç®—è‡ªé€‚åº”å­—ä½“å¤§å°"""
    try:
        if not original_font_size:
            return None
        
        # è®¡ç®—æ–‡æœ¬é•¿åº¦æ¯”ä¾‹ï¼Œä½¿ç”¨strip()å»é™¤å‰åç©ºæ ¼ç¡®ä¿å‡†ç¡®è®¡ç®—
        original_length = len(original_text.strip())
        translated_length = len(translated_text.strip())
        
        if original_length == 0:
            return original_font_size
        
        # è®¡ç®—é•¿åº¦æ¯”ä¾‹
        length_ratio = translated_length / original_length
        
        # å¦‚æœç¿»è¯‘åæ–‡æœ¬å˜é•¿ï¼Œé€‚å½“ç¼©å°å­—ä½“
        if length_ratio > 1.2:  # æ–‡æœ¬é•¿åº¦å¢åŠ è¶…è¿‡20%
            # æ ¹æ®é•¿åº¦æ¯”ä¾‹è®¡ç®—æ–°çš„å­—ä½“å¤§å°ï¼Œä½†ä¸è¦å°äºåŸå§‹å¤§å°çš„70%
            new_size = max(original_font_size * 0.7, original_font_size / length_ratio)
            return int(new_size)
        elif length_ratio < 0.8:  # æ–‡æœ¬é•¿åº¦å‡å°‘è¶…è¿‡20%
            # å¦‚æœæ–‡æœ¬å˜çŸ­ï¼Œå¯ä»¥é€‚å½“å¢å¤§å­—ä½“ï¼Œä½†ä¸è¦è¶…è¿‡åŸå§‹å¤§å°çš„120%
            new_size = min(original_font_size * 1.2, original_font_size / length_ratio)
            return int(new_size)
        else:
            # é•¿åº¦å˜åŒ–ä¸å¤§ï¼Œä¿æŒåŸå§‹å­—ä½“å¤§å°
            return original_font_size
            
    except Exception as e:
        logger.error(f"è®¡ç®—è‡ªé€‚åº”å­—ä½“å¤§å°å¤±è´¥: {str(e)}")
        return original_font_size


def calculate_adaptive_line_spacing(original_text, translated_text, original_line_spacing):
    """æ ¹æ®ç¿»è¯‘åæ–‡æœ¬é•¿åº¦è®¡ç®—è‡ªé€‚åº”è¡Œé—´è·"""
    try:
        # è®¡ç®—æ–‡æœ¬è¡Œæ•°å˜åŒ–
        original_lines = len(original_text.split('\n'))
        translated_lines = len(translated_text.split('\n'))
        
        if original_lines == 0:
            return original_line_spacing
        
        # è®¡ç®—è¡Œæ•°æ¯”ä¾‹
        line_ratio = translated_lines / original_lines
        
        # å¦‚æœè¡Œæ•°å¢åŠ ï¼Œé€‚å½“å¢åŠ è¡Œé—´è·
        if line_ratio > 1.1:  # è¡Œæ•°å¢åŠ è¶…è¿‡10%
            # å¢åŠ è¡Œé—´è·ï¼Œä½†ä¸è¦è¶…è¿‡åŸå§‹è¡Œé—´è·çš„150%
            new_spacing = min(original_line_spacing * 1.5, original_line_spacing * line_ratio)
            return new_spacing
        elif line_ratio < 0.9:  # è¡Œæ•°å‡å°‘è¶…è¿‡10%
            # å¦‚æœè¡Œæ•°å‡å°‘ï¼Œå¯ä»¥é€‚å½“å‡å°‘è¡Œé—´è·ï¼Œä½†ä¸è¦å°‘äºåŸå§‹è¡Œé—´è·çš„80%
            new_spacing = max(original_line_spacing * 0.8, original_line_spacing * line_ratio)
            return new_spacing
        else:
            # è¡Œæ•°å˜åŒ–ä¸å¤§ï¼Œä¿æŒåŸå§‹è¡Œé—´è·
            return original_line_spacing
            
    except Exception as e:
        logger.error(f"è®¡ç®—è‡ªé€‚åº”è¡Œé—´è·å¤±è´¥: {str(e)}")
        return original_line_spacing


def apply_adaptive_styles(run, original_text, translated_text, context_type=None):
    """åº”ç”¨è‡ªé€‚åº”æ ·å¼åˆ°run"""
    try:
        # è·å–åŸå§‹å­—ä½“å¤§å°
        original_font_size = None
        if run.font.size:
            try:
                original_font_size = run.font.size.pt
            except:
                original_font_size = None
        
        # è®¡ç®—è‡ªé€‚åº”å­—ä½“å¤§å°
        if original_font_size:
            adaptive_font_size = calculate_adaptive_font_size(original_text, translated_text, original_font_size)
            if adaptive_font_size and adaptive_font_size != original_font_size:
                # å¦‚æœæ˜¯æ–‡æœ¬æ¡†ï¼Œè¿›ä¸€æ­¥ç¼©å°20%
                if context_type == 'textbox':
                    adaptive_font_size = int(adaptive_font_size * 0.8)
                from docx.shared import Pt
                run.font.size = Pt(adaptive_font_size)
                logger.info(f"å­—ä½“å¤§å°è‡ªé€‚åº”: {original_font_size}pt -> {adaptive_font_size}pt")
            elif context_type == 'textbox':
                # å¯¹äºæ–‡æœ¬æ¡†ï¼Œå³ä½¿é•¿åº¦æ²¡å˜ï¼Œä¹Ÿç¼©å°20%
                adaptive_font_size = int(original_font_size * 0.8)
                from docx.shared import Pt
                run.font.size = Pt(adaptive_font_size)
                logger.info(f"æ–‡æœ¬æ¡†å­—ä½“å›ºå®šç¼©å°: {original_font_size}pt -> {adaptive_font_size}pt")
        
        # è·å–æ®µè½å¯¹è±¡å¹¶åº”ç”¨è¡Œé—´è·è‡ªé€‚åº”
        try:
            paragraph = run._element.getparent()
            if hasattr(paragraph, 'paragraph_format') and paragraph.paragraph_format:
                # è·å–åŸå§‹è¡Œé—´è·
                original_line_spacing = paragraph.paragraph_format.line_spacing
                if original_line_spacing:
                    adaptive_line_spacing = calculate_adaptive_line_spacing(original_text, translated_text, original_line_spacing)
                    if adaptive_line_spacing and adaptive_line_spacing != original_line_spacing:
                        paragraph.paragraph_format.line_spacing = adaptive_line_spacing
                        logger.info(f"è¡Œé—´è·è‡ªé€‚åº”: {original_line_spacing} -> {adaptive_line_spacing}")
        except Exception as e:
            logger.error(f"åº”ç”¨è¡Œé—´è·è‡ªé€‚åº”å¤±è´¥: {str(e)}")
                
    except Exception as e:
        logger.error(f"åº”ç”¨è‡ªé€‚åº”æ ·å¼å¤±è´¥: {str(e)}")


def apply_translations_with_adaptive_styles(document, texts):
    """åº”ç”¨ç¿»è¯‘ç»“æœåˆ°æ–‡æ¡£ï¼ŒåŒæ—¶åº”ç”¨è‡ªé€‚åº”æ ·å¼"""
    text_count = 0

    for item in texts:
        if not item.get('complete', False):
            continue

        if item['type'] == 'run':
            # æ£€æŸ¥runæ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
            if check_if_image(item['run']):
                continue
            
            # è·å–åŸå§‹æ–‡æœ¬å’Œç¿»è¯‘åæ–‡æœ¬
            original_text = item['run'].text
            translated_text = item.get('text', original_text)
            
            # åº”ç”¨ç¿»è¯‘ç»“æœ
            item['run'].text = translated_text
            
            # åº”ç”¨è‡ªé€‚åº”æ ·å¼
            apply_adaptive_styles(item['run'], original_text, translated_text)
            
            text_count += 1
        elif item['type'] == 'merged_run':
            # å¤„ç†åˆå¹¶çš„runï¼Œéœ€è¦å°†ç¿»è¯‘ç»“æœåˆ†é…å›åŸå§‹run
            merged_item = item['merged_item']
            translated_text = item.get('text', merged_item['text'])
            original_text = merged_item['text']
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
            has_image = False
            for run in merged_item['runs']:
                if check_if_image(run):
                    has_image = True
                    break
            
            if has_image:
                continue
            
            if merged_item['type'] == 'merged':
                # åˆå¹¶çš„runç»„ï¼Œéœ€è¦æ™ºèƒ½åˆ†é…ç¿»è¯‘ç»“æœ
                distribute_translation_to_runs_with_adaptive_styles(merged_item['runs'], translated_text, original_text)
            else:
                # å•ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
                run = merged_item['runs'][0]
                run.text = translated_text
                apply_adaptive_styles(run, original_text, translated_text)
            
            text_count += 1
        elif item['type'] == 'single_run':
            # å¤„ç†å•ä¸ªrunï¼Œç›´æ¥åº”ç”¨ç¿»è¯‘ç»“æœ
            logger.info(f"=== å¤„ç†single_runç±»å‹ ===")
            merged_item = item['merged_item']
            translated_text = item.get('text', merged_item['text'])
            original_text = merged_item['text']
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
            has_image = False
            for run in merged_item['runs']:
                if check_if_image(run):
                    has_image = True
                    break
            
            if has_image:
                continue
            
            # ç›´æ¥æ›¿æ¢runæ–‡æœ¬
            run = merged_item['runs'][0]
            run.text = translated_text
            logger.info(f"single_runæ–‡æœ¬æ›¿æ¢: '{original_text}' -> '{translated_text}'")
            
            text_count += 1
        elif item['type'] == 'main_title_runs':
            # å¤„ç†ä¸»æ ‡é¢˜çš„å¤šä¸ªrunï¼Œåœ¨runä¹‹é—´æ·»åŠ ç©ºæ ¼
            logger.info(f"=== ä¸»æ ‡é¢˜runç©ºæ ¼å¤„ç†å¼€å§‹ ===")
            runs = item['runs']
            translated_text = item.get('text', "")
            
            logger.info(f"ä¸»æ ‡é¢˜åŸæ–‡: '{translated_text}'")
            logger.info(f"Runæ•°é‡: {len(runs)}")
            
            # å°†ç¿»è¯‘æ–‡æœ¬æŒ‰runæ•°é‡åˆ†é…ï¼Œå¹¶åœ¨runä¹‹é—´æ·»åŠ ç©ºæ ¼
            words = translated_text.split()
            logger.info(f"ç¿»è¯‘æ–‡æœ¬åˆ†å‰²åçš„å•è¯: {words}")
            
            if len(words) >= len(runs):
                # å¹³å‡åˆ†é…å•è¯åˆ°æ¯ä¸ªrun
                words_per_run = len(words) // len(runs)
                remainder = len(words) % len(runs)
                
                current_word_index = 0
                for i, run in enumerate(runs):
                    # è®¡ç®—å½“å‰runåº”è¯¥åŒ…å«çš„å•è¯æ•°
                    if i < remainder:
                        run_word_count = words_per_run + 1
                    else:
                        run_word_count = words_per_run
                    
                    # æå–å½“å‰runçš„å•è¯
                    run_words = words[current_word_index:current_word_index + run_word_count]
                    run_text = ' '.join(run_words)
                    
                    # æ›´æ–°runæ–‡æœ¬
                    run.text = run_text
                    current_word_index += run_word_count
                    
                    logger.info(f"ä¸»æ ‡é¢˜run {i}: '{run_text}' (åˆ†é…å•è¯: {run_words})")
                    
                    # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œå¦‚æœä¸æ˜¯å°±åŠ ç©ºæ ¼
                    if not run_text.rstrip().endswith(('.,;:!?()[]{}"\'-')):
                        run.text = run_text + ' '
                        logger.info(f"å®Œæ•´æ ‡é¢˜run {i}ä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼ŒåŠ ç»“å°¾ç©ºæ ¼: '{run_text}' -> '{run.text}'")
                    else:
                        logger.info(f"å®Œæ•´æ ‡é¢˜run {i}ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œä¸åŠ ç©ºæ ¼: '{run_text}'")
            else:
                # å•è¯æ•°é‡å°‘äºrunæ•°é‡ï¼Œç®€å•åˆ†é…
                logger.info(f"å•è¯æ•°é‡({len(words)})å°‘äºrunæ•°é‡({len(runs)})ï¼Œè¿›è¡Œç®€å•åˆ†é…")
                for i, run in enumerate(runs):
                    if i < len(words):
                        run.text = words[i]
                        # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œå¦‚æœä¸æ˜¯å°±åŠ ç©ºæ ¼
                        if not words[i].rstrip().endswith(('.,;:!?()[]{}"\'-')):
                            run.text = words[i] + ' '
                            logger.info(f"å®Œæ•´æ ‡é¢˜run {i}ä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼ŒåŠ ç»“å°¾ç©ºæ ¼: '{words[i]}' -> '{run.text}'")
                    else:
                        run.text = ""
                    logger.info(f"å®Œæ•´æ ‡é¢˜run {i}: '{run.text}'")
            
            logger.info("=== ä¸»æ ‡é¢˜runç©ºæ ¼å¤„ç†å®Œæˆ ===")
            text_count += 1

    # ç¿»è¯‘å®Œæˆåï¼Œé‡æ–°è°ƒæ•´æ‰€æœ‰è¡¨æ ¼çš„å¸ƒå±€
    try:
        for table in document.tables:
            adjust_table_layout_for_translation(table)
    except Exception as e:
        logger.error(f"æœ€ç»ˆè°ƒæ•´è¡¨æ ¼å¸ƒå±€æ—¶å‡ºé”™: {str(e)}")

    # æ™ºèƒ½runæ‹¼æ¥å·²ç»åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç†

    return text_count


def distribute_translation_to_runs_with_adaptive_styles(runs, translated_text, original_text):
    """å°†ç¿»è¯‘ç»“æœæ™ºèƒ½åˆ†é…å›åŸå§‹runï¼ŒåŒæ—¶åº”ç”¨è‡ªé€‚åº”æ ·å¼"""
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
    if len(runs) == 1:
        runs[0].text = translated_text
        apply_adaptive_styles(runs[0], original_text, translated_text)
        return
    
    # è®¡ç®—åŸå§‹æ–‡æœ¬çš„æ€»é•¿åº¦
    original_total_length = sum(len(run.text) for run in runs)
    
    # å¦‚æœç¿»è¯‘åæ–‡æœ¬é•¿åº¦å˜åŒ–ä¸å¤§ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
    if abs(len(translated_text) - original_total_length) <= 2:
        # æŒ‰åŸå§‹æ¯”ä¾‹åˆ†é…
        current_pos = 0
        for run in runs:
            original_ratio = len(run.text) / original_total_length
            target_length = int(len(translated_text) * original_ratio)
            
            if current_pos < len(translated_text):
                end_pos = min(current_pos + target_length, len(translated_text))
                allocated_text = translated_text[current_pos:end_pos]
                run.text = allocated_text
                
                # åº”ç”¨è‡ªé€‚åº”æ ·å¼
                apply_adaptive_styles(run, run.text, allocated_text)
                
                current_pos = end_pos
            else:
                run.text = ""
    else:
        # é•¿åº¦å˜åŒ–è¾ƒå¤§ï¼Œå°è¯•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²
        words = translated_text.split()
        if len(words) >= len(runs):
            # æŒ‰runæ•°é‡åˆ†é…å•è¯
            for i, run in enumerate(runs):
                if i < len(words):
                    allocated_text = words[i]
                    run.text = allocated_text
                    apply_adaptive_styles(run, run.text, allocated_text)
                else:
                    run.text = ""
        else:
            # å•è¯æ•°é‡å°‘äºrunæ•°é‡ï¼Œå¹³å‡åˆ†é…
            chars_per_run = len(translated_text) // len(runs)
            for i, run in enumerate(runs):
                start_pos = i * chars_per_run
                end_pos = start_pos + chars_per_run if i < len(runs) - 1 else len(translated_text)
                allocated_text = translated_text[start_pos:end_pos]
                run.text = allocated_text
                apply_adaptive_styles(run, run.text, allocated_text)


def update_special_elements(docx_path, texts):
    """æ›´æ–°æ‰¹æ³¨ç­‰ç‰¹æ®Šå…ƒç´ """
    # ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„æ‰¹æ³¨
    comment_texts = [t for t in texts if t.get('type') == 'comment' and t.get('complete')]
    if not comment_texts:
        return  # æ²¡æœ‰éœ€è¦å¤„ç†çš„æ‰¹æ³¨

    temp_path = f"temp_{os.path.basename(docx_path)}"

    try:
        with zipfile.ZipFile(docx_path, 'r') as zin, \
                zipfile.ZipFile(temp_path, 'w') as zout:

            for item in zin.infolist():
                with zin.open(item) as file:
                    if item.filename == 'word/comments.xml':
                        try:
                            tree = ET.parse(file)
                            root = tree.getroot()

                            # å®šä¹‰å‘½åç©ºé—´
                            namespaces = {
                                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                            }

                            # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ³¨
                            for comment in root.findall('.//w:comment', namespaces) or root.findall(
                                    './/{*}comment'):
                                # å°è¯•ä¸åŒæ–¹å¼è·å–ID
                                comment_id = None
                                for attr_name in [
                                    '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id',
                                    'id']:
                                    if attr_name in comment.attrib:
                                        comment_id = comment.attrib[attr_name]
                                        break

                                if not comment_id:
                                    continue

                                # æŸ¥æ‰¾åŒ¹é…çš„ç¿»è¯‘ç»“æœ
                                matching_texts = [t for t in comment_texts if
                                                  t.get('comment_id') == comment_id]
                                if not matching_texts:
                                    continue

                                # æ›´æ–°æ‰¹æ³¨æ–‡æœ¬
                                text_elements = comment.findall('.//w:t',
                                                                namespaces) or comment.findall(
                                    './/{*}t')
                                for i, t_elem in enumerate(text_elements):
                                    if i < len(matching_texts):
                                        # æ‰¾åˆ°åŒ¹é…çš„åŸå§‹æ–‡æœ¬
                                        for match in matching_texts:
                                            if match.get('original_text') == t_elem.text:
                                                t_elem.text = match.get('text', t_elem.text)
                                                break

                            # å†™å…¥ä¿®æ”¹åçš„XML
                            modified_xml = ET.tostring(root, encoding='utf-8', xml_declaration=True)
                            zout.writestr(item.filename, modified_xml)
                        except Exception as e:
                            logger.error(f"å¤„ç†æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")
                            # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥å¤åˆ¶åŸæ–‡ä»¶
                            file.seek(0)
                            zout.writestr(item.filename, file.read())
                    else:
                        # ç›´æ¥å¤åˆ¶å…¶ä»–æ–‡ä»¶
                        file.seek(0)
                        zout.writestr(item.filename, file.read())

        # æ›¿æ¢åŸå§‹æ–‡ä»¶
        os.replace(temp_path, docx_path)
    except Exception as e:
        logger.error(f"æ›´æ–°æ‰¹æ³¨æ—¶å‡ºé”™: {str(e)}")
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
        if os.path.exists(temp_path):
            os.remove(temp_path)



def check_text(text):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”éçº¯æ ‡ç‚¹ï¼‰- ä¼˜åŒ–ç‰ˆæœ¬"""
    if not text or len(text) == 0:
        return False
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜ç¼–å·ï¼ˆå¦‚ "1."ã€"2."ã€"3."ï¼‰
    if TITLE_NUMBER_PATTERN.match(text.strip()):
        return True  # æ ‡é¢˜ç¼–å·åº”è¯¥ä¿ç•™
    
    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯å­—æ¯æˆ–æ•°å­—ï¼Œå¾ˆå¯èƒ½éœ€è¦ç¿»è¯‘
    if text[0].isalnum():
        return True
    
    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœå…¨æ˜¯ç©ºæ ¼ï¼Œä¸éœ€è¦ç¿»è¯‘
    if text.isspace():
        return False
    
    # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœé•¿åº¦å¾ˆçŸ­ä¸”å…¨æ˜¯æ ‡ç‚¹ï¼Œä¸éœ€è¦ç¿»è¯‘
    if len(text) <= 3 and all(not c.isalnum() for c in text):
        return False
    
    # æœ€åæ£€æŸ¥ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
    if SPECIAL_SYMBOLS_PATTERN.match(text):
        return False
    
    if NUMBERS_PATTERN.match(text):
        return False
    
    return not common.is_all_punc(text)


def apply_translations(document, texts):
    """åº”ç”¨ç¿»è¯‘ç»“æœåˆ°æ–‡æ¡£ï¼Œå®Œå…¨ä¿ç•™åŸå§‹æ ¼å¼"""
    text_count = 0

    for item in texts:
        if not item.get('complete', False):
            continue

        if item['type'] == 'run':
            # æ£€æŸ¥runæ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
            if check_if_image(item['run']):
                continue
            # ç›´æ¥æ›¿æ¢runæ–‡æœ¬ï¼Œä¿ç•™æ‰€æœ‰æ ¼å¼
            item['run'].text = item.get('text', item['run'].text)
            # å¦‚æœæ˜¯æ–‡æœ¬æ¡†ï¼Œåº”ç”¨è‡ªé€‚åº”æ ·å¼
            if item.get('context_type') == 'textbox':
                original_text = item['run'].text  # æ³¨æ„ï¼šè¿™é‡Œ original_text åº”è¯¥æ˜¯æ›¿æ¢å‰çš„ï¼Œä½†ç”±äºå·²æ›¿æ¢ï¼Œä½¿ç”¨ item['text'] ä½œä¸º translated
                translated_text = item['run'].text
                apply_adaptive_styles(item['run'], original_text, translated_text, context_type='textbox')
            text_count += 1
        elif item['type'] == 'merged_run':
            # å¤„ç†åˆå¹¶çš„runï¼Œéœ€è¦å°†ç¿»è¯‘ç»“æœåˆ†é…å›åŸå§‹run
            merged_item = item['merged_item']
            translated_text = item.get('text', merged_item['text'])
            
            # å¦‚æœæ˜¯æ ‡é¢˜ç¼–å·ï¼Œç›´æ¥è·³è¿‡ï¼Œä¿æŒåŸæ ·
            if item.get('is_title_number'):
                logger.info(f"è·³è¿‡æ ‡é¢˜ç¼–å·: '{merged_item['text']}'")
                continue
            
            # å¦‚æœæ˜¯ç›®å½•æ–‡æœ¬ï¼Œé™„åŠ åŸé¡µç 
            if item.get('is_toc_text'):
                translated_text += item['original_page_num']
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡ï¼Œå¦‚æœåŒ…å«åˆ™è·³è¿‡
            has_image = False
            for run in merged_item['runs']:
                if check_if_image(run):
                    has_image = True
                    break
            
            if has_image:
                continue
            
            if merged_item['type'] == 'merged':
                # åˆå¹¶çš„runç»„ï¼Œéœ€è¦æ™ºèƒ½åˆ†é…ç¿»è¯‘ç»“æœ
                distribute_translation_to_runs(merged_item['runs'], translated_text, merged_item['text'])
            else:
                # å•ä¸ªrunï¼Œç›´æ¥æ›¿æ¢
                merged_item['runs'][0].text = translated_text
            
            # å¦‚æœæ˜¯æ–‡æœ¬æ¡†ï¼Œåº”ç”¨è‡ªé€‚åº”æ ·å¼åˆ°æ¯ä¸ªrun
            if item.get('context_type') == 'textbox':
                original_text = merged_item['text']  # åŸå§‹åˆå¹¶æ–‡æœ¬
                # å¯¹äºæ¯ä¸ªrunï¼Œåº”ç”¨è‡ªé€‚åº”ï¼ˆç®€åŒ–ï¼šå¯¹ç¬¬ä¸€ä¸ªrunåº”ç”¨æ•´ä½“è°ƒæ•´ï¼‰
                for run in merged_item['runs']:
                    apply_adaptive_styles(run, original_text, translated_text, context_type='textbox')
            
            text_count += 1

    # ç¿»è¯‘å®Œæˆåï¼Œé‡æ–°è°ƒæ•´æ‰€æœ‰è¡¨æ ¼çš„å¸ƒå±€
    try:
        for table in document.tables:
            adjust_table_layout_for_translation(table)
    except Exception as e:
        logger.error(f"æœ€ç»ˆè°ƒæ•´è¡¨æ ¼å¸ƒå±€æ—¶å‡ºé”™: {str(e)}")

    return text_count


def distribute_translation_to_runs(runs, translated_text, original_text=None):
    """å°†ç¿»è¯‘åçš„æ–‡æœ¬åˆ†é…åˆ°å„ä¸ªrunä¸­"""
    if not runs:
        return
    
    logger.info(f"=== å¼€å§‹åˆ†é…ç¿»è¯‘ç»“æœåˆ°runs ===")
    logger.info(f"åŸå§‹æ–‡æœ¬: '{original_text}'")
    logger.info(f"ç¿»è¯‘æ–‡æœ¬: '{translated_text}'")
    logger.info(f"Runæ•°é‡: {len(runs)}")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºä¸»æ ‡é¢˜çš„å¤šä¸ªrun
    is_main_title = False
    if len(runs) > 1 and original_text:
        # æ£€æŸ¥åŸå§‹æ–‡æœ¬æ˜¯å¦åŒ…å«å¤šä¸ªä¸­æ–‡å­—ç¬¦ï¼Œå¯èƒ½æ˜¯å®Œæ•´æ ‡é¢˜
        chinese_char_count = sum(1 for c in original_text if '\u4e00' <= c <= '\u9fff')
        if chinese_char_count >= 6 and len(original_text) >= 8:  # è‡³å°‘6ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œæ€»é•¿åº¦è‡³å°‘8
            is_main_title = True
            logger.info(f"æ£€æµ‹åˆ°ä¸»æ ‡é¢˜runç»„ï¼ŒåŸå§‹æ–‡æœ¬: '{original_text}' (åŒ…å«{chinese_char_count}ä¸ªä¸­æ–‡å­—ç¬¦)")
    
    # ä¸»æ ‡é¢˜è¯†åˆ«å·²è¢«ç¦ç”¨ï¼Œç›´æ¥ä½¿ç”¨æ™ºèƒ½ç©ºæ ¼åˆ†å¸ƒ
    logger.info(f"è°ƒç”¨æ™ºèƒ½ç©ºæ ¼åˆ†å¸ƒå‡½æ•°...")
    smart_text_distribution_with_spaces(runs, translated_text, original_text)
    logger.info(f"=== ç¿»è¯‘ç»“æœåˆ†é…å®Œæˆ ===")


def debug_spacing_analysis(original_text, translated_text, runs):
    """è°ƒè¯•ç©ºæ ¼åˆ†æï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜"""
    logger.info(f"=== ç©ºæ ¼åˆ†æè°ƒè¯• ===")
    logger.info(f"åŸå§‹æ–‡æœ¬: '{original_text}' (é•¿åº¦: {len(original_text)})")
    logger.info(f"ç¿»è¯‘æ–‡æœ¬: '{translated_text}' (é•¿åº¦: {len(translated_text)})")
    logger.info(f"Runæ•°é‡: {len(runs)}")
    
    # æ£€æµ‹è¯­è¨€
    original_is_chinese = is_chinese_text(original_text)
    translated_is_english = is_english_text(translated_text)
    logger.info(f"åŸå§‹æ–‡æœ¬æ˜¯å¦ä¸­æ–‡: {original_is_chinese}")
    logger.info(f"ç¿»è¯‘æ–‡æœ¬æ˜¯å¦è‹±æ–‡: {translated_is_english}")
    
    # æ£€æµ‹ç©ºæ ¼éœ€æ±‚
    needs_spaces, spacing_type = detect_language_and_spacing_needs(original_text, translated_text)
    logger.info(f"éœ€è¦æ·»åŠ ç©ºæ ¼: {needs_spaces}")
    logger.info(f"ç©ºæ ¼ç±»å‹: {spacing_type}")
    
    # åˆ†æåŸå§‹run
    logger.info(f"åŸå§‹runå†…å®¹:")
    for i, run in enumerate(runs):
        logger.info(f"  Run {i}: '{run.text}' (é•¿åº¦: {len(run.text)})")
    
    # åˆ†æç¿»è¯‘åæ–‡æœ¬çš„å•è¯
    words = translated_text.split()
    logger.info(f"ç¿»è¯‘åå•è¯: {words}")
    logger.info(f"å•è¯æ•°é‡: {len(words)}")
    
    # åˆ†æç©ºæ ¼é—®é¢˜
    if needs_spaces and spacing_type == "chinese_to_english":
        logger.info(f"æ£€æµ‹åˆ°ä¸­è¯‘è‹±ï¼Œéœ€è¦æ·»åŠ ç©ºæ ¼")
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿åœ¨ä¸€èµ·çš„å•è¯
        for word in words:
            if len(word) > 15:  # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é•¿çš„å•è¯
                logger.info(f"  å‘ç°é•¿å•è¯: '{word}' (é•¿åº¦: {len(word)})")
                # å°è¯•åˆ†æè¿™ä¸ªé•¿å•è¯
                if any(c.isupper() for c in word[1:]):  # æ£€æŸ¥æ˜¯å¦æœ‰å¤§å†™å­—æ¯ï¼ˆå¯èƒ½æ˜¯å¤šä¸ªå•è¯è¿åœ¨ä¸€èµ·ï¼‰
                    logger.info(f"    å¯èƒ½åŒ…å«å¤šä¸ªå•è¯ï¼Œå»ºè®®æ·»åŠ ç©ºæ ¼")
    
    logger.info("==================")


def smart_text_distribution_with_spaces(runs, translated_text, original_text):
    """æ™ºèƒ½æ–‡æœ¬åˆ†å¸ƒï¼Œæ ¹æ®è¯­è¨€å’Œç©ºæ ¼éœ€æ±‚å†³å®šå¤„ç†æ–¹å¼"""
    
    # è°ƒè¯•ä¿¡æ¯
    debug_spacing_analysis(original_text, translated_text, runs)
    
    # æ£€æµ‹è¯­è¨€å’Œç©ºæ ¼éœ€æ±‚
    needs_spaces, spacing_type = detect_language_and_spacing_needs(original_text, translated_text)
    
    if needs_spaces:
        # ä¸­è¯‘è‹±ï¼Œéœ€è¦æ·»åŠ ç©ºæ ¼
        if len(runs) == 1:
            # å•ä¸ªrunï¼Œç›´æ¥èµ‹å€¼
            runs[0].text = translated_text
            
            # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œå¦‚æœä¸æ˜¯å°±åŠ ç©ºæ ¼
            if not translated_text.rstrip().endswith(('.,;:!?()[]{}"\'-')):
                runs[0].text = translated_text + ' '
                logger.info(f"å•runä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼ŒåŠ ç»“å°¾ç©ºæ ¼: '{translated_text}' -> '{runs[0].text}'")
            else:
                logger.info(f"å•runä»¥æ ‡ç‚¹ç»“å°¾ï¼Œä¸åŠ ç©ºæ ¼: '{translated_text}'")
        else:
            # å¤šä¸ªrunï¼Œä½¿ç”¨æ™ºèƒ½runæ‹¼æ¥
            logger.info(f"å¤šrunæƒ…å†µï¼Œä½¿ç”¨æ™ºèƒ½runæ‹¼æ¥")
            smart_run_concatenation(runs, translated_text)
    else:
        # ä¸éœ€è¦æ·»åŠ ç©ºæ ¼ï¼Œä¿æŒåŸæœ‰ç©ºæ ¼
        distribute_preserving_original_spaces(runs, translated_text, original_text)
    
    # è°ƒè¯•åˆ†é…ç»“æœ
    logger.info(f"åˆ†é…åçš„runå†…å®¹:")
    for i, run in enumerate(runs):
        logger.info(f"  Run {i}: '{run.text}' (é•¿åº¦: {len(run.text)})")
    logger.info("==================")


def smart_run_concatenation(runs, translated_text):
    """æ™ºèƒ½runæ‹¼æ¥ï¼Œåœ¨éœ€è¦çš„åœ°æ–¹è‡ªåŠ¨æ·»åŠ ç©ºæ ¼"""
    logger.info(f"=== æ™ºèƒ½runæ‹¼æ¥å¼€å§‹ ===")
    logger.info(f"ç¿»è¯‘æ–‡æœ¬: '{translated_text}'")
    logger.info(f"Runæ•°é‡: {len(runs)}")
    
    # å°†ç¿»è¯‘æ–‡æœ¬æŒ‰runæ•°é‡åˆ†é…
    words = translated_text.split()
    logger.info(f"ç¿»è¯‘æ–‡æœ¬åˆ†å‰²åçš„å•è¯: {words}")
    
    if len(words) >= len(runs):
        # å¹³å‡åˆ†é…å•è¯åˆ°æ¯ä¸ªrun
        words_per_run = len(words) // len(runs)
        remainder = len(words) % len(runs)
        
        current_word_index = 0
        for i, run in enumerate(runs):
            # è®¡ç®—å½“å‰runåº”è¯¥åŒ…å«çš„å•è¯æ•°
            if i < remainder:
                run_word_count = words_per_run + 1
            else:
                run_word_count = words_per_run
            
            # æå–å½“å‰runçš„å•è¯
            run_words = words[current_word_index:current_word_index + run_word_count]
            run_text = ' '.join(run_words)
            
            # æ›´æ–°runæ–‡æœ¬
            run.text = run_text
            current_word_index += run_word_count
            
            logger.info(f"Run {i}: '{run_text}'")
            
            # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œå¦‚æœä¸æ˜¯å°±åŠ ç©ºæ ¼
            if not run_text.rstrip().endswith(('.,;:!?()[]{}"\'-')):
                run.text = run_text + ' '
                logger.info(f"Run {i}ä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼ŒåŠ ç»“å°¾ç©ºæ ¼: '{run_text}' -> '{run.text}'")
            else:
                logger.info(f"Run {i}ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œä¸åŠ ç©ºæ ¼: '{run_text}'")
    else:
        # å•è¯æ•°é‡å°‘äºrunæ•°é‡ï¼Œç®€å•åˆ†é…
        for i, run in enumerate(runs):
            if i < len(words):
                run.text = words[i]
                # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œå¦‚æœä¸æ˜¯å°±åŠ ç©ºæ ¼
                if not words[i].rstrip().endswith(('.,;:!?()[]{}"\'-')):
                    run.text = words[i] + ' '
                    logger.info(f"Run {i}ä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼ŒåŠ ç»“å°¾ç©ºæ ¼: '{words[i]}' -> '{run.text}'")
                else:
                    logger.info(f"Run {i}ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œä¸åŠ ç©ºæ ¼: '{words[i]}'")
            else:
                run.text = ""
                logger.info(f"Run {i}: ç©ºrun")
            logger.info(f"Run {i}æœ€ç»ˆç»“æœ: '{run.text}'")
    
    # ç°åœ¨åœ¨runä¹‹é—´æ·»åŠ ç©ºæ ¼
    logger.info(f"=== å¼€å§‹æ·»åŠ runé—´ç©ºæ ¼ ===")
    for i in range(1, len(runs)):
        current_run = runs[i]
        prev_run = runs[i-1]
        
        # æ£€æŸ¥å‰ä¸€ä¸ªrunæ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾
        prev_text = prev_run.text.strip()
        current_text = current_run.text.strip()
        
        if not prev_text or not current_text:
            continue
            
        prev_ends_with_punct = prev_text[-1] in '.,;:!?'
        current_starts_with_punct = current_text[0] in '.,;:!?'
        
        # å¦‚æœå‰ä¸€ä¸ªrunä¸ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œä¸”å½“å‰runä¸ä»¥æ ‡ç‚¹å¼€å¤´ï¼Œå°±åœ¨å½“å‰runå‰é¢åŠ ç©ºæ ¼
        if not prev_ends_with_punct and not current_starts_with_punct:
            if not current_text.startswith(' '):
                current_run.text = ' ' + current_text
                logger.info(f"åœ¨run {i}å‰é¢åŠ ç©ºæ ¼: '{current_run.text}'")
        else:
            logger.info(f"run {i}ä¸éœ€è¦åŠ ç©ºæ ¼ (å‰ä¸€ä¸ªä»¥æ ‡ç‚¹ç»“å°¾: {prev_ends_with_punct}, å½“å‰ä»¥æ ‡ç‚¹å¼€å¤´: {current_starts_with_punct})")
    
    # æ£€æŸ¥æ¯ä¸ªrunçš„ç»“å°¾ï¼Œå¦‚æœä¸æ˜¯æ ‡ç‚¹ç»“å°¾å°±åŠ ç©ºæ ¼
    logger.info(f"=== æ£€æŸ¥æ¯ä¸ªrunç»“å°¾ï¼Œæ·»åŠ ç»“å°¾ç©ºæ ¼ ===")
    for i, run in enumerate(runs):
        if not run.text.strip():
            continue
            
        # æ£€æŸ¥æ˜¯å¦ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾
        run_text = run.text.strip()
        ends_with_punct = run_text[-1] in '.,;:!?()[]{}"\'-'
        
        if not ends_with_punct:
            # ä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼Œåœ¨ç»“å°¾åŠ ç©ºæ ¼
            run.text = run_text + ' '
            logger.info(f"Run {i}ä¸æ˜¯æ ‡ç‚¹ç»“å°¾ï¼ŒåŠ ç»“å°¾ç©ºæ ¼: '{run_text}' -> '{run.text}'")
        else:
            logger.info(f"Run {i}ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œä¸åŠ ç©ºæ ¼: '{run_text}'")
    
    logger.info("=== æ™ºèƒ½runæ‹¼æ¥å®Œæˆ ===")


def detect_language_and_spacing_needs(original_text, translated_text):
    """æ›´æ™ºèƒ½åœ°æ£€æµ‹è¯­è¨€ç±»å‹å’Œç©ºæ ¼éœ€æ±‚"""
    # æ£€æµ‹åŸå§‹æ–‡æœ¬è¯­è¨€
    original_is_chinese = is_chinese_text(original_text)
    translated_is_english = is_english_text(translated_text)
    translated_is_chinese = is_chinese_text(translated_text)
    
    # ä¸­è¯‘è‹±ï¼šéœ€è¦æ·»åŠ ç©ºæ ¼
    if original_is_chinese and translated_is_english:
        return True, "chinese_to_english"
    
    # è‹±è¯‘ä¸­ï¼šé€šå¸¸ä¸éœ€è¦ç©ºæ ¼
    if translated_is_chinese and not original_is_chinese:
        return False, "english_to_chinese"
    
    # å…¶ä»–æƒ…å†µï¼šä¿æŒåŸå§‹ç©ºæ ¼æ ¼å¼
    return False, "preserve_original"


def is_chinese_text(text):
    """æ£€æµ‹æ˜¯å¦ä¸ºä¸­æ–‡æ–‡æœ¬"""
    if not text:
        return False
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = 0
    total_chars = 0
    
    for char in text:
        if char.isspace() or char.isdigit() or char in '.,;:!?()[]{}"\'-':
            continue
        
        total_chars += 1
        # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡å­—ç¬¦ï¼ˆUnicodeèŒƒå›´ï¼‰
        if '\u4e00' <= char <= '\u9fff':
            chinese_chars += 1
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æœ¬
    return total_chars > 0 and chinese_chars / total_chars > 0.5


def is_english_text(text):
    """æ£€æµ‹æ˜¯å¦ä¸ºè‹±æ–‡æ–‡æœ¬"""
    if not text:
        return False
    
    # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦æ•°é‡
    english_chars = 0
    total_chars = 0
    
    for char in text:
        if char.isspace() or char.isdigit() or char in '.,;:!?()[]{}"\'-':
            continue
        
        total_chars += 1
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‹±æ–‡å­—ç¬¦
        if char.isalpha() and char.isascii():
            english_chars += 1
    
    # å¦‚æœè‹±æ–‡å­—ç¬¦å æ¯”è¶…è¿‡70%ï¼Œè®¤ä¸ºæ˜¯è‹±æ–‡æ–‡æœ¬
    return total_chars > 0 and english_chars / total_chars > 0.7


def detect_spacing_needs(original_text, translated_text):
    """æ£€æµ‹æ˜¯å¦éœ€è¦æ·»åŠ ç©ºæ ¼ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼‰"""
    needs_spaces, spacing_type = detect_language_and_spacing_needs(original_text, translated_text)
    
    if spacing_type == "chinese_to_english":
        # ä¸­è¯‘è‹±ï¼šå¼ºåˆ¶æ·»åŠ ç©ºæ ¼
        return True
    elif spacing_type == "english_to_chinese":
        # è‹±è¯‘ä¸­ï¼šä¿æŒåŸå§‹æ ¼å¼
        return False
    else:
        # å…¶ä»–æƒ…å†µï¼šåŸºäºç©ºæ ¼å­˜åœ¨æ€§åˆ¤æ–­
        original_has_spaces = ' ' in original_text
        translated_has_spaces = ' ' in translated_text
        
        # å¦‚æœåŸå§‹æ–‡æœ¬æ²¡æœ‰ç©ºæ ¼ä½†ç¿»è¯‘åæœ‰ç©ºæ ¼ï¼Œè¯´æ˜éœ€è¦æ·»åŠ ç©ºæ ¼
        if not original_has_spaces and translated_has_spaces:
            return True
        
        # å¦‚æœåŸå§‹æ–‡æœ¬æœ‰ç©ºæ ¼ä½†ç¿»è¯‘åæ²¡æœ‰ï¼Œè¯´æ˜éœ€è¦ä¿æŒç©ºæ ¼
        if original_has_spaces and not translated_has_spaces:
            return False
        
        # å¦‚æœéƒ½æœ‰ç©ºæ ¼ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ ¼å¼åŒ–
        if original_has_spaces and translated_has_spaces:
            # æ¯”è¾ƒç©ºæ ¼æ¨¡å¼ï¼Œå¦‚æœå·®å¼‚å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦é‡æ–°æ ¼å¼åŒ–
            original_word_count = len(original_text.split())
            translated_word_count = len(translated_text.split())
            if abs(original_word_count - translated_word_count) > 2:
                return True
    
    return False


def distribute_with_proper_spacing(runs, translated_text):
    """ä¸ºéœ€è¦æ·»åŠ ç©ºæ ¼çš„æ–‡æœ¬åˆ†é…runï¼Œç¡®ä¿å•è¯é—´æœ‰é€‚å½“ç©ºæ ¼ï¼ˆä¸“é—¨å¤„ç†ä¸­è¯‘è‹±ï¼‰"""
    # æš‚æ—¶æ³¨é‡Šæ‰æœ‰é—®é¢˜çš„å‡½æ•°è°ƒç”¨
    # ä½¿ç”¨ä¸“é—¨çš„runé—´ç©ºæ ¼å¤„ç†å‡½æ•°
    # ensure_proper_spacing_between_runs(runs, translated_text)
    
    # ç®€å•çš„ç©ºæ ¼å¤„ç†
    if len(runs) == 1:
        runs[0].text = translated_text
    else:
        # å¤šrunæƒ…å†µï¼Œç®€å•åˆ†é…
        words = translated_text.split()
        for i, run in enumerate(runs):
            if i < len(words):
                run.text = words[i]
            else:
                run.text = ""


def distribute_preserving_original_spaces(runs, translated_text, original_text):
    """ä¿æŒåŸå§‹ç©ºæ ¼æ ¼å¼çš„åˆ†é…"""
    # è®¡ç®—åŸå§‹æ–‡æœ¬çš„æ€»é•¿åº¦
    original_total_length = sum(len(run.text) for run in runs)
    
    # å¦‚æœç¿»è¯‘åæ–‡æœ¬é•¿åº¦å˜åŒ–ä¸å¤§ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
    if abs(len(translated_text) - original_total_length) <= 2:
        # æŒ‰åŸå§‹æ¯”ä¾‹åˆ†é…
        current_pos = 0
        for run in runs:
            original_ratio = len(run.text) / original_total_length
            target_length = int(len(translated_text) * original_ratio)
            
            if current_pos < len(translated_text):
                end_pos = min(current_pos + target_length, len(translated_text))
                run.text = translated_text[current_pos:end_pos]
                current_pos = end_pos
            else:
                run.text = ""
    else:
        # é•¿åº¦å˜åŒ–è¾ƒå¤§ï¼ŒæŒ‰å­—ç¬¦å¹³å‡åˆ†é…
        chars_per_run = len(translated_text) // len(runs)
        for i, run in enumerate(runs):
            start_pos = i * chars_per_run
            end_pos = start_pos + chars_per_run if i < len(runs) - 1 else len(translated_text)
            run.text = translated_text[start_pos:end_pos]


def adjust_table_layout_for_translation(table):
    """è°ƒæ•´è¡¨æ ¼å¸ƒå±€ä»¥é€‚åº”ç¿»è¯‘åçš„æ–‡æœ¬é•¿åº¦"""
    try:
        from docx.shared import Inches, Cm
        
        # è·å–è¡¨æ ¼çš„åˆ—æ•°å’Œè¡Œæ•°
        num_cols = len(table.columns)
        num_rows = len(table.rows)
        
        if num_cols == 0:
            return
        
        # è®¡ç®—æ¯åˆ—çš„ç†æƒ³å®½åº¦
        # æ ¹æ®åˆ—æ•°åˆ†é…å¯ç”¨å®½åº¦ï¼Œç•™å‡ºä¸€äº›è¾¹è·
        available_width = 6.0  # å‡è®¾é¡µé¢å®½åº¦ä¸º6è‹±å¯¸
        margin = 0.5  # å·¦å³è¾¹è·
        usable_width = available_width - margin * 2
        
        # ä¸ºæ¯åˆ—åˆ†é…å®½åº¦ï¼Œå¯ä»¥æ ¹æ®å†…å®¹è°ƒæ•´
        column_widths = []
        for col_idx in range(num_cols):
            # è®¡ç®—è¯¥åˆ—æ‰€æœ‰å•å…ƒæ ¼çš„æœ€å¤§æ–‡æœ¬é•¿åº¦
            max_text_length = 0
            for row_idx in range(num_rows):
                if col_idx < len(table.rows[row_idx].cells):
                    cell = table.rows[row_idx].cells[col_idx]
                    for paragraph in cell.paragraphs:
                        text_length = len(paragraph.text)
                        max_text_length = max(max_text_length, text_length)
            
            # æ ¹æ®æ–‡æœ¬é•¿åº¦è®¡ç®—åˆ—å®½ï¼ˆä¸­æ–‡å­—ç¬¦å¤§çº¦éœ€è¦0.1è‹±å¯¸å®½åº¦ï¼‰
            # æœ€å°åˆ—å®½ä¸º0.5è‹±å¯¸ï¼Œæœ€å¤§ä¸º2.0è‹±å¯¸
            estimated_width = max(0.5, min(2.0, max_text_length * 0.1))
            column_widths.append(estimated_width)
        
        # è°ƒæ•´åˆ—å®½
        for col_idx, width in enumerate(column_widths):
            if col_idx < len(table.columns):
                # è®¾ç½®åˆ—å®½
                table.columns[col_idx].width = Inches(width)
                
                # åŒæ—¶è®¾ç½®è¯¥åˆ—æ‰€æœ‰å•å…ƒæ ¼çš„å®½åº¦
                for row_idx in range(num_rows):
                    if col_idx < len(table.rows[row_idx].cells):
                        cell = table.rows[row_idx].cells[col_idx]
                        cell.width = Inches(width)
        
        # è®¾ç½®è¡¨æ ¼çš„è‡ªåŠ¨è°ƒæ•´å±æ€§
        table.autofit = True
        
        # è®¾ç½®è¡¨æ ¼æ ·å¼ï¼Œç¡®ä¿å†…å®¹ä¸ä¼šè¶…å‡ºè¾¹ç•Œ
        table.style = 'Table Grid'
        
    except Exception as e:
        logger.error(f"è°ƒæ•´è¡¨æ ¼å¸ƒå±€æ—¶å‡ºé”™: {str(e)}")


def process_table_with_layout_adjustment(table, local_texts):
    """å¤„ç†è¡¨æ ¼å¹¶è°ƒæ•´å¸ƒå±€"""
    # å¤„ç†è¡¨æ ¼å†…å®¹
    for row in table.rows:
        for cell in row.cells:
            for paragraph in cell.paragraphs:
                extract_paragraph_with_merge(paragraph, local_texts, "table", 0, 1)  # è¡¨æ ¼æ®µè½ç´¢å¼•ä¸º0
                # å¤„ç†è¡¨æ ¼ä¸­çš„è¶…é“¾æ¥
                for hyperlink in paragraph.hyperlinks:
                    extract_hyperlink_with_merge(hyperlink, local_texts)
                # å¤„ç†è¡¨æ ¼å•å…ƒæ ¼ä¸­çš„æ–‡æœ¬æ¡†
                for run in paragraph.runs:
                    if check_if_textbox(run):
                        # å¤„ç† DrawingML
                        drawing_elem = run.element.find('.//w:drawing', run.element.nsmap)
                        if drawing_elem is not None:
                            txbx_elem = drawing_elem.find('.//w:txbxContent', drawing_elem.nsmap)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", 0, 1)
                        
                        # å¤„ç† VML
                        namespaces = {**run.element.nsmap, 'v': 'urn:schemas-microsoft-com:vml'}
                        vtextbox_elem = run.element.find('.//v:textbox', namespaces)
                        if vtextbox_elem is not None:
                            txbx_elem = vtextbox_elem.find('.//w:txbxContent', namespaces)
                            if txbx_elem is not None:
                                for p_elem in txbx_elem:
                                    if p_elem.tag == qn('w:p'):
                                        tb_para = Paragraph(p_elem, paragraph)
                                        extract_paragraph_with_merge(tb_para, local_texts, "textbox", 0, 1)
    
    # è°ƒæ•´è¡¨æ ¼å¸ƒå±€
    adjust_table_layout_for_translation(table)


def convert_chinese_punctuation_to_english(text):
    """å°†ä¸­æ–‡æ ‡ç‚¹ç¬¦å·è½¬æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹ç¬¦å·"""
    if not text:
        return text
    
    # ä¸­æ–‡æ ‡ç‚¹åˆ°è‹±æ–‡æ ‡ç‚¹çš„æ˜ å°„
    punctuation_map = {
        'ï¼Œ': ',',  # é€—å·
        'ã€‚': '.',  # å¥å·
        'ï¼': '!',  # æ„Ÿå¹å·
        'ï¼Ÿ': '?',  # é—®å·
        'ï¼›': ';',  # åˆ†å·
        'ï¼š': ':',  # å†’å·
        'ï¼ˆ': '(',  # å·¦æ‹¬å·
        'ï¼‰': ')',  # å³æ‹¬å·
        'ã€': '[',  # å·¦æ–¹æ‹¬å·
        'ã€‘': ']',  # å³æ–¹æ‹¬å·
        'ã€Š': '<',  # å·¦å°–æ‹¬å·
        'ã€‹': '>',  # å³å°–æ‹¬å·
        '"': '"',   # ä¸­æ–‡å¼•å·
        '"': '"',   # ä¸­æ–‡å¼•å·
        ''': "'",   # ä¸­æ–‡å•å¼•å·
        ''': "'",   # ä¸­æ–‡å•å¼•å·
        'â€¦': '...', # çœç•¥å·
        'â€”': '-',   # ç ´æŠ˜å·
        'ï¼': '-',  # å…¨è§’è¿å­—ç¬¦
        'ã€€': ' ',  # å…¨è§’ç©ºæ ¼
    }
    
    # æ‰§è¡Œè½¬æ¢
    converted_text = text
    for chinese, english in punctuation_map.items():
        converted_text = converted_text.replace(chinese, english)
    
    # å¦‚æœæ–‡æœ¬æœ‰å˜åŒ–ï¼Œè®°å½•æ—¥å¿—
    if converted_text != text:
        logger.info(f"æ ‡ç‚¹ç¬¦å·è½¬æ¢: '{text}' -> '{converted_text}'")
    
    return converted_text



