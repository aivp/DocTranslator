# -*- coding: utf-8 -*-
# import tiktoken
import datetime
import hashlib
import logging
import os
import sys
import re
import openai
from . import common
from . import db
import time

from .baidu.main import baidu_translate

# å¯¼å…¥Qwenç¿»è¯‘æ¨¡å—
try:
    from .qwen_translate import qwen_translate, check_qwen_availability
    logging.info("âœ… æˆåŠŸå¯¼å…¥ qwen_translate æ¨¡å—")
except ImportError as e:
    logging.error(f"âŒ å¯¼å…¥ qwen_translate æ¨¡å—å¤±è´¥: {e}")
    # å¦‚æœQwenæ¨¡å—ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å‡½æ•°
    def qwen_translate(text, target_language, source_lang="auto", tm_list=None, terms=None, domains=None, prompt=None, prompt_id=None, max_retries=10, texts=None, index=None):
        logging.warning("âš ï¸ ä½¿ç”¨å¤‡ç”¨ qwen_translate å‡½æ•°ï¼Œä¸Šä¸‹æ–‡åŠŸèƒ½ä¸å¯ç”¨")
        return text
    def check_qwen_availability():
        return False, "Qwenæ¨¡å—æœªæ‰¾åˆ°"


def translate_text(trans, text, source_lang="auto", target_lang="en"):
    """
    ç¿»è¯‘å•ä¸ªæ–‡æœ¬
    
    Args:
        trans: ç¿»è¯‘é…ç½®å­—å…¸
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        source_lang: æºè¯­è¨€
        target_lang: ç›®æ ‡è¯­è¨€
        
    Returns:
        str: ç¿»è¯‘åçš„æ–‡æœ¬
    """
    try:
        # è·å–ç¿»è¯‘é…ç½®
        api_key = trans.get('api_key', '')
        api_url = trans.get('api_url', '')
        model = trans.get('model', 'gpt-3.5-turbo')
        server = trans.get('server', 'openai')
        app_id = trans.get('app_id', '')
        app_key = trans.get('app_key', '')
        
        # æ ¹æ®æœåŠ¡å™¨ç±»å‹é€‰æ‹©ç¿»è¯‘æ–¹æ³•
        if server == 'baidu':
            return baidu_translate(
                text=text,
                appid=app_id,
                app_key=app_key,
                from_lang=source_lang,
                to_lang=target_lang,
                use_term_base=False
            )
        elif server == 'qwen':
            # ä½¿ç”¨å®˜æ–¹æ–‡æ¡£çš„è¯­è¨€æ˜ å°„ (æ˜ å°„ä¸ºè‹±æ–‡å…¨æ‹¼)
            LANGUAGE_MAPPING = {
                # ä¸­æ–‡åç§°åˆ°è‹±æ–‡å…¨æ‹¼
                'ä¸­æ–‡': 'Chinese',
                'ç®€ä½“ä¸­æ–‡': 'Chinese',
                'ç¹ä½“ä¸­æ–‡': 'Traditional Chinese',
                'è‹±è¯­': 'English',
                'è‹±æ–‡': 'English',
                'ä¿„è¯­': 'Russian',
                'æ—¥è¯­': 'Japanese',
                'éŸ©è¯­': 'Korean',
                'è¥¿ç­ç‰™è¯­': 'Spanish',
                'æ³•è¯­': 'French',
                'è‘¡è„ç‰™è¯­': 'Portuguese',
                'å¾·è¯­': 'German',
                'æ„å¤§åˆ©è¯­': 'Italian',
                'æ³°è¯­': 'Thai',
                'è¶Šå—è¯­': 'Vietnamese',
                'å°åº¦å°¼è¥¿äºšè¯­': 'Indonesian',
                'é©¬æ¥è¯­': 'Malay',
                'é˜¿æ‹‰ä¼¯è¯­': 'Arabic',
                'å°åœ°è¯­': 'Hindi',
                'å¸Œä¼¯æ¥è¯­': 'Hebrew',
                'ç¼…ç”¸è¯­': 'Burmese',
                'æ³°ç±³å°”è¯­': 'Tamil',
                'ä¹Œå°”éƒ½è¯­': 'Urdu',
                'å­ŸåŠ æ‹‰è¯­': 'Bengali',
                'æ³¢å…°è¯­': 'Polish',
                'è·å…°è¯­': 'Dutch',
                'ç½—é©¬å°¼äºšè¯­': 'Romanian',
                'åœŸè€³å…¶è¯­': 'Turkish',
                'é«˜æ£‰è¯­': 'Khmer',
                'è€æŒè¯­': 'Lao',
                'ç²¤è¯­': 'Cantonese',
                'æ·å…‹è¯­': 'Czech',
                'å¸Œè…Šè¯­': 'Greek',
                'ç‘å…¸è¯­': 'Swedish',
                'åŒˆç‰™åˆ©è¯­': 'Hungarian',
                'ä¸¹éº¦è¯­': 'Danish',
                'èŠ¬å…°è¯­': 'Finnish',
                'ä¹Œå…‹å…°è¯­': 'Ukrainian',
                'ä¿åŠ åˆ©äºšè¯­': 'Bulgarian',
                'å¡å°”ç»´äºšè¯­': 'Serbian',
                'æ³°å¢å›ºè¯­': 'Telugu',
                'å—éè·å…°è¯­': 'Afrikaans',
                'äºšç¾å°¼äºšè¯­': 'Armenian',
                'é˜¿è¨å§†è¯­': 'Assamese',
                'é˜¿æ–¯å›¾é‡Œäºšæ–¯è¯­': 'Asturian',
                'å·´æ–¯å…‹è¯­': 'Basque',
                'ç™½ä¿„ç½—æ–¯è¯­': 'Belarusian',
                'æ³¢æ–¯å°¼äºšè¯­': 'Bosnian',
                'åŠ æ³°ç½—å°¼äºšè¯­': 'Catalan',
                'å®¿åŠ¡è¯­': 'Cebuano',
                'å…‹ç½—åœ°äºšè¯­': 'Croatian',
                'åŸƒåŠé˜¿æ‹‰ä¼¯è¯­': 'Egyptian Arabic',
                'çˆ±æ²™å°¼äºšè¯­': 'Estonian',
                'åŠ åˆ©è¥¿äºšè¯­': 'Galician',
                'æ ¼é²å‰äºšè¯­': 'Georgian',
                'å¤å‰æ‹‰ç‰¹è¯­': 'Gujarati',
                'å†°å²›è¯­': 'Icelandic',
                'çˆªå“‡è¯­': 'Javanese',
                'å¡çº³è¾¾è¯­': 'Kannada',
                'å“ˆè¨å…‹è¯­': 'Kazakh',
                'æ‹‰è„±ç»´äºšè¯­': 'Latvian',
                'ç«‹é™¶å®›è¯­': 'Lithuanian',
                'å¢æ£®å ¡è¯­': 'Luxembourgish',
                'é©¬å…¶é¡¿è¯­': 'Macedonian',
                'é©¬åŠ å¸Œè¯­': 'Maithili',
                'é©¬è€³ä»–è¯­': 'Maltese',
                'é©¬æ‹‰åœ°è¯­': 'Marathi',
                'ç¾ç´¢ä¸è¾¾ç±³äºšé˜¿æ‹‰ä¼¯è¯­': 'Mesopotamian Arabic',
                'æ‘©æ´›å“¥é˜¿æ‹‰ä¼¯è¯­': 'Moroccan Arabic',
                'å†…å¿—é˜¿æ‹‰ä¼¯è¯­': 'Najdi Arabic',
                'å°¼æ³Šå°”è¯­': 'Nepali',
                'åŒ—é˜¿å¡æ‹œç–†è¯­': 'North Azerbaijani',
                'åŒ—é»å‡¡ç‰¹é˜¿æ‹‰ä¼¯è¯­': 'North Levantine Arabic',
                'åŒ—ä¹Œå…¹åˆ«å…‹è¯­': 'Northern Uzbek',
                'ä¹¦é¢è¯­æŒªå¨è¯­': 'Norwegian BokmÃ¥l',
                'æ–°æŒªå¨è¯­': 'Norwegian Nynorsk',
                'å¥¥å…‹è¯­': 'Occitan',
                'å¥¥é‡Œäºšè¯­': 'Odia',
                'é‚¦é˜¿è¥¿æ¥ è¯­': 'Pangasinan',
                'è¥¿è¥¿é‡Œè¯­': 'Sicilian',
                'ä¿¡å¾·è¯­': 'Sindhi',
                'åƒ§ä¼½ç½—è¯­': 'Sinhala',
                'æ–¯æ´›ä¼å…‹è¯­': 'Slovak',
                'æ–¯æ´›æ–‡å°¼äºšè¯­': 'Slovenian',
                'å—é»å‡¡ç‰¹é˜¿æ‹‰ä¼¯è¯­': 'South Levantine Arabic',
                'æ–¯ç“¦å¸Œé‡Œè¯­': 'Swahili',
                'ä»–åŠ ç¦„è¯­': 'Tagalog',
                'å¡”ä¼Šå…¹-äºšä¸é˜¿æ‹‰ä¼¯è¯­': 'Ta\'izzi-Adeni Arabic',
                'æ‰˜æ–¯å…‹é˜¿å°”å·´å°¼äºšè¯­': 'Tosk Albanian',
                'çªå°¼æ–¯é˜¿æ‹‰ä¼¯è¯­': 'Tunisian Arabic',
                'å¨å°¼æ–¯è¯­': 'Venetian',
                'ç“¦è±è¯­': 'Waray',
                'å¨å°”å£«è¯­': 'Welsh',
                'è¥¿æ³¢æ–¯è¯­': 'Western Persian',
                # è‹±æ–‡å…¨æ‹¼åˆ°è‡ªèº« (ç¡®ä¿è‹±æ–‡å…¨æ‹¼æ˜ å°„åˆ°è‡ªèº«)
                'English': 'English',
                'Chinese': 'Chinese',
                'Traditional Chinese': 'Traditional Chinese',
                'Russian': 'Russian',
                'Japanese': 'Japanese',
                'Korean': 'Korean',
                'Spanish': 'Spanish',
                'French': 'French',
                'Portuguese': 'Portuguese',
                'German': 'German',
                'Italian': 'Italian',
                'Thai': 'Thai',
                'Vietnamese': 'Vietnamese',
                'Indonesian': 'Indonesian',
                'Malay': 'Malay',
                'Arabic': 'Arabic',
                'Hindi': 'Hindi',
                'Hebrew': 'Hebrew',
                'Burmese': 'Burmese',
                'Tamil': 'Tamil',
                'Urdu': 'Urdu',
                'Bengali': 'Bengali',
                'Polish': 'Polish',
                'Dutch': 'Dutch',
                'Romanian': 'Romanian',
                'Turkish': 'Turkish',
                'Khmer': 'Khmer',
                'Lao': 'Lao',
                'Cantonese': 'Cantonese',
                'Czech': 'Czech',
                'Greek': 'Greek',
                'Swedish': 'Swedish',
                'Hungarian': 'Hungarian',
                'Danish': 'Danish',
                'Finnish': 'Finnish',
                'Ukrainian': 'Ukrainian',
                'Bulgarian': 'Bulgarian',
                'Serbian': 'Serbian',
                'Telugu': 'Telugu',
                'Afrikaans': 'Afrikaans',
                'Armenian': 'Armenian',
                'Assamese': 'Assamese',
                'Asturian': 'Asturian',
                'Basque': 'Basque',
                'Belarusian': 'Belarusian',
                'Bosnian': 'Bosnian',
                'Catalan': 'Catalan',
                'Cebuano': 'Cebuano',
                'Croatian': 'Croatian',
                'Egyptian Arabic': 'Egyptian Arabic',
                'Estonian': 'Estonian',
                'Galician': 'Galician',
                'Georgian': 'Georgian',
                'Gujarati': 'Gujarati',
                'Icelandic': 'Icelandic',
                'Javanese': 'Javanese',
                'Kannada': 'Kannada',
                'Kazakh': 'Kazakh',
                'Latvian': 'Latvian',
                'Lithuanian': 'Lithuanian',
                'Luxembourgish': 'Luxembourgish',
                'Macedonian': 'Macedonian',
                'Maithili': 'Maithili',
                'Maltese': 'Maltese',
                'Marathi': 'Marathi',
                'Mesopotamian Arabic': 'Mesopotamian Arabic',
                'Moroccan Arabic': 'Moroccan Arabic',
                'Najdi Arabic': 'Najdi Arabic',
                'Nepali': 'Nepali',
                'North Azerbaijani': 'North Azerbaijani',
                'North Levantine Arabic': 'North Levantine Arabic',
                'Northern Uzbek': 'Northern Uzbek',
                'Norwegian BokmÃ¥l': 'Norwegian BokmÃ¥l',
                'Norwegian Nynorsk': 'Norwegian Nynorsk',
                'Occitan': 'Occitan',
                'Odia': 'Odia',
                'Pangasinan': 'Pangasinan',
                'Sicilian': 'Sicilian',
                'Sindhi': 'Sindhi',
                'Sinhala': 'Sinhala',
                'Slovak': 'Slovak',
                'Slovenian': 'Slovenian',
                'South Levantine Arabic': 'South Levantine Arabic',
                'Swahili': 'Swahili',
                'Tagalog': 'Tagalog',
                'Ta\'izzi-Adeni Arabic': 'Ta\'izzi-Adeni Arabic',
                'Tosk Albanian': 'Tosk Albanian',
                'Tunisian Arabic': 'Tunisian Arabic',
                'Venetian': 'Venetian',
                'Waray': 'Waray',
                'Welsh': 'Welsh',
                'Western Persian': 'Western Persian',
                # è¯­ç§ç¼–ç åˆ°è‹±æ–‡å…¨æ‹¼
                'zh': 'Chinese',
                'en': 'English',
                'ja': 'Japanese',
                'ko': 'Korean',
                'fr': 'French',
                'de': 'German',
                'es': 'Spanish',
                'ru': 'Russian',
                'it': 'Italian',
                'ar': 'Arabic',
                'th': 'Thai',
                'vi': 'Vietnamese',
                'id': 'Indonesian',
                'ms': 'Malay',
                'tl': 'Tagalog',
                'my': 'Burmese',
                'km': 'Khmer',
                'lo': 'Lao',
                'pt': 'Portuguese',
                'hi': 'Hindi',
                'he': 'Hebrew',
                'ta': 'Tamil',
                'ur': 'Urdu',
                'bn': 'Bengali',
                'pl': 'Polish',
                'nl': 'Dutch',
                'ro': 'Romanian',
                'tr': 'Turkish',
                'yue': 'Cantonese',
                'cs': 'Czech',
                'el': 'Greek',
                'sv': 'Swedish',
                'hu': 'Hungarian',
                'da': 'Danish',
                'fi': 'Finnish',
                'uk': 'Ukrainian',
                'bg': 'Bulgarian',
                'sr': 'Serbian',
                'te': 'Telugu',
                'af': 'Afrikaans',
                'hy': 'Armenian',
                'as': 'Assamese',
                'ast': 'Asturian',
                'eu': 'Basque',
                'be': 'Belarusian',
                'bs': 'Bosnian',
                'ca': 'Catalan',
                'ceb': 'Cebuano',
                'hr': 'Croatian',
                'arz': 'Egyptian Arabic',
                'et': 'Estonian',
                'gl': 'Galician',
                'ka': 'Georgian',
                'gu': 'Gujarati',
                'is': 'Icelandic',
                'jv': 'Javanese',
                'kn': 'Kannada',
                'kk': 'Kazakh',
                'lv': 'Latvian',
                'lt': 'Lithuanian',
                'lb': 'Luxembourgish',
                'mk': 'Macedonian',
                'mai': 'Maithili',
                'mt': 'Maltese',
                'mr': 'Marathi',
                'acm': 'Mesopotamian Arabic',
                'ary': 'Moroccan Arabic',
                'ars': 'Najdi Arabic',
                'ne': 'Nepali',
                'az': 'North Azerbaijani',
                'apc': 'North Levantine Arabic',
                'uz': 'Northern Uzbek',
                'nb': 'Norwegian BokmÃ¥l',
                'nn': 'Norwegian Nynorsk',
                'oc': 'Occitan',
                'or': 'Odia',
                'pag': 'Pangasinan',
                'scn': 'Sicilian',
                'sd': 'Sindhi',
                'si': 'Sinhala',
                'sk': 'Slovak',
                'sl': 'Slovenian',
                'ajp': 'South Levantine Arabic',
                'sw': 'Swahili',
                'acq': 'Ta\'izzi-Adeni Arabic',
                'sq': 'Tosk Albanian',
                'aeb': 'Tunisian Arabic',
                'vec': 'Venetian',
                'war': 'Waray',
                'cy': 'Welsh',
                'fa': 'Western Persian',
            }
            
            # è½¬æ¢è¯­è¨€ä»£ç 
            qwen_source_lang = LANGUAGE_MAPPING.get(source_lang, source_lang)
            qwen_target_lang = LANGUAGE_MAPPING.get(target_lang, target_lang)
            
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
            logging.info(f"ğŸ” Qwenç¿»è¯‘è°ƒè¯•ä¿¡æ¯:")
            logging.info(f"  åŸå§‹æºè¯­è¨€: {source_lang}")
            logging.info(f"  åŸå§‹ç›®æ ‡è¯­è¨€: {target_lang}")
            logging.info(f"  æ˜ å°„åæºè¯­è¨€: {qwen_source_lang}")
            logging.info(f"  æ˜ å°„åç›®æ ‡è¯­è¨€: {qwen_target_lang}")
            logging.info(f"  æ˜ å°„æ˜¯å¦ç”Ÿæ•ˆ: {qwen_source_lang != source_lang or qwen_target_lang != target_lang}")
            
            return qwen_translate(
                text=text,
                target_language=qwen_target_lang,
                source_lang="auto",
                prompt=trans.get('prompt'),
                prompt_id=trans.get('prompt_id'),
                texts=None,  # translate_textå‡½æ•°ä¸­æ²¡æœ‰textsæ•°ç»„
                index=None   # translate_textå‡½æ•°ä¸­æ²¡æœ‰index
            )
        else:
            # OpenAI ç¿»è¯‘ (å…¼å®¹æ–°æ—§ç‰ˆæœ¬)
            try:
                import openai
                
                # å°è¯•æ–°ç‰ˆæœ¬ API
                if hasattr(openai, 'OpenAI'):
                    client = openai.OpenAI(api_key=api_key, base_url=api_url if api_url else None)
                    response = client.chat.completions.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": f"è¯·å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘ä¸º{target_lang}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ ‡è®°ã€‚"},
                            {"role": "user", "content": text}
                        ],
                        temperature=0.3
                    )
                    return response.choices[0].message.content.strip()
                else:
                    # æ—§ç‰ˆæœ¬ API
                    openai.api_key = api_key
                    if api_url:
                        openai.api_base = api_url
                    
                    response = openai.ChatCompletion.create(
                        model=model,
                        messages=[
                            {"role": "system", "content": f"è¯·å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘ä¸º{target_lang}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ ‡è®°ã€‚"},
                            {"role": "user", "content": text}
                        ],
                        temperature=0.3
                    )
                    return response.choices[0].message.content.strip()
            except Exception as e:
                logging.error(f"OpenAI ç¿»è¯‘å¤±è´¥: {e}")
                # å¦‚æœ OpenAI å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Qwen ä½œä¸ºå¤‡ç”¨
                try:
                    return qwen_translate(
                        text=text,
                        target_language=target_lang,
                        source_lang="auto",
                        prompt=trans.get('prompt'),
                        prompt_id=trans.get('prompt_id'),
                        texts=None,  # å¤‡ç”¨æ–¹æ¡ˆä¸­æ²¡æœ‰textsæ•°ç»„
                        index=None   # å¤‡ç”¨æ–¹æ¡ˆä¸­æ²¡æœ‰index
                    )
                except:
                    return text  # æœ€åè¿”å›åŸæ–‡
            
    except Exception as e:
        logging.error(f"ç¿»è¯‘å¤±è´¥: {e}")
        return text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡


def get(trans, event, texts, index):
    if event.is_set():
        exit(0)
    # æ¢å¤çº¿ç¨‹æ•°ä¸º30ï¼Œæé«˜ç¿»è¯‘æ•ˆç‡
    max_threads = 30
    translate_id = trans['id']
    target_lang = trans['lang']
    
    # ============== æ¨¡å‹é…ç½® ==============
    model = trans['model']
    backup_model = trans['backup_model']
    
    # æ‰“å°å½“å‰é…ç½®ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
    if index == 0:
        try:
            # æ£€æŸ¥QwenæœåŠ¡å¯ç”¨æ€§
            if model == 'qwen-mt-plus':
                qwen_available, qwen_message = check_qwen_availability()
                logging.info(f"QwenæœåŠ¡æ£€æŸ¥: {qwen_message}")
                if not qwen_available:
                    logging.warning("è­¦å‘Š: QwenæœåŠ¡ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                else:
                    logging.info("Qwenç¿»è¯‘æœåŠ¡å·²å¯ç”¨")
        except:
            pass
    # ==========================================
    
    prompt = trans['prompt']
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ŒæŸ¥çœ‹promptå­—æ®µçš„å€¼
    logging.info(f"ğŸ” to_translate.py è°ƒè¯•ä¿¡æ¯:")
    logging.info(f"  trans['prompt']ç±»å‹: {type(prompt)}")
    logging.info(f"  trans['prompt']å€¼: {repr(prompt)}")
    logging.info(f"  trans['prompt']æ˜¯å¦ä¸ºç©º: {not prompt}")
    logging.info(f"  trans['prompt']é•¿åº¦: {len(prompt) if prompt else 0}")
    
    extension = trans['extension'].lower()
    text = texts[index]
    api_key = trans['api_key']
    api_url = trans['api_url']
    app_id = trans['app_id']
    app_key = trans['app_key']
    comparison_id = trans.get('comparison_id', 0)
    # ç¡®ä¿comparison_idä¸ä¸ºNoneï¼Œå¦‚æœæ˜¯Noneåˆ™è®¾ä¸º0
    if comparison_id is None:
        comparison_id = 0
    # ä¸è¦å¼ºåˆ¶è½¬æ¢ä¸ºintï¼Œä¿æŒåŸå§‹å­—ç¬¦ä¸²æ ¼å¼ä»¥æ”¯æŒå¤šä¸ªID
    server = trans.get('server', 'openai')
    old_text = text['text']
    md5_key = md5_encryption(
        str(api_key) + str(api_url) + str(old_text) + str(prompt) + str(backup_model) + str(
            model) + str(target_lang))

    # ============== ç™¾åº¦ç¿»è¯‘å¤„ç† ==============
    if server == 'baidu':
        try:
            if not text['complete']:
                content = baidu_translate(
                    text=old_text,
                    appid=app_id,
                    app_key=app_key,
                    from_lang='auto',
                    to_lang=target_lang,
                    use_term_base=comparison_id == 1  # ä½¿ç”¨æœ¯è¯­åº“
                )
                text['count'] = count_text(text['text'])
                if check_translated(content):
                    text['text'] = content  # ç™¾åº¦ç¿»è¯‘æ— éœ€è¿‡æ»¤<think>æ ‡ç­¾
                text['complete'] = True
        except Exception as e:
            logging.error(f"ç™¾åº¦ç¿»è¯‘é”™è¯¯: {str(e)}")
            if "retry" not in text:
                text["retry"] = 0
            text["retry"] += 1
            if text["retry"] <= 3:
                time.sleep(5)
                logging.info('ç™¾åº¦ç¿»è¯‘å‡ºé”™ï¼Œæ­£åœ¨é‡è¯•ï¼')
                return get(trans, event, texts, index)  # é‡æ–°å°è¯•
            text['complete'] = True

    # ============== AIç¿»è¯‘å¤„ç† ==============
    elif server == 'openai' or server == 'doc2x' or server == 'qwen':
        try:
            # mredis.set("threading_count",threading_num+1)
            # ç›®æ ‡è¯­è¨€æ˜ å°„å¤„ç†
            LANGUAGE_MAPPING = {
                'ä¸­æ–‡': 'Chinese',
                'è‹±è¯­': 'English',
                'è‹±æ–‡': 'English',  # å…¼å®¹æ—§ç‰ˆæœ¬
                'é˜¿æ‹‰ä¼¯è¯­': 'Arabic',
                'æ³•è¯­': 'French',
                'å¾·æ–‡': 'German',  # å…¼å®¹æ—§ç‰ˆæœ¬
                'å¾·è¯­': 'German',
                'è¥¿ç­ç‰™è¯­': 'Spanish',
                'è¥¿ç­ç‰™æ–‡': 'Spanish',  # å…¼å®¹æ—§ç‰ˆæœ¬
                'ä¿„è¯­': 'Russian',
                'ä¿„æ–‡': 'Russian',  # å…¼å®¹æ—§ç‰ˆæœ¬
                'æ„å¤§åˆ©è¯­': 'Italian',
                'æ„å¤§åˆ©æ–‡': 'Italian',  # å…¼å®¹æ—§ç‰ˆæœ¬
                'æ³°è¯­': 'Thai',
                'è¶Šå—è¯­': 'Vietnamese',
                'å°å°¼è¯­/é©¬æ¥è¯­': 'Indonesian',
                'å°å°¼è¯­': 'Indonesian',
                'é©¬æ¥è¯­': 'Malay',
                'è²å¾‹å®¾è¯­ï¼ˆä»–åŠ ç¦„è¯­ï¼‰': 'Filipino',
                'è²å¾‹å®¾è¯­': 'Filipino',
                'ä»–åŠ ç¦„è¯­': 'Filipino',
                'ç¼…ç”¸è¯­': 'Burmese',
                'æŸ¬åŸ”å¯¨è¯­ï¼ˆé«˜æ£‰è¯­ï¼‰': 'Khmer',
                'æŸ¬åŸ”å¯¨è¯­': 'Khmer',
                'é«˜æ£‰è¯­': 'Khmer',
                'è€æŒè¯­': 'Lao',
                'æŸ¬è¯­': 'Khmer',
                'è‘¡è„ç‰™è¯­': 'Portuguese'
            }
            
            # ä½¿ç”¨æ˜ å°„å­—å…¸å¤„ç†ç›®æ ‡è¯­è¨€
            target_lang = LANGUAGE_MAPPING.get(target_lang, target_lang)

            # è·å–æœ¯è¯­åº“å†…å®¹å¹¶è½¬æ¢ä¸ºtm_listæ ¼å¼ï¼ˆä»…å½“ä½¿ç”¨åƒé—®æ¨¡å‹ä¸”æœ‰æœ¯è¯­åº“æ—¶ï¼‰
            tm_list = None
            if model == 'qwen-mt-plus' and comparison_id:
                # æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç­›é€‰çš„æœ¯è¯­åº“ï¼ˆæ¥è‡ªOkapiTranslationServiceï¼‰
                filtered_terms = trans.get('filtered_terms')
                if filtered_terms:
                    # ä½¿ç”¨é¢„ç­›é€‰çš„æœ¯è¯­åº“
                    # logging.info(f"ä½¿ç”¨é¢„ç­›é€‰çš„æœ¯è¯­åº“ï¼Œé•¿åº¦: {len(filtered_terms)}")
                    tm_list = []
                    for line in filtered_terms.split('\n'):
                        if ':' in line:
                            source, target = line.split(':', 1)
                            tm_list.append({
                                "source": source.strip(),
                                "target": target.strip()
                            })
                    # logging.info(f"é¢„ç­›é€‰æœ¯è¯­åº“å¤„ç†å®Œæˆï¼Œå…± {len(tm_list)} æ¡æœ¯è¯­")
                else:
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„åŠ è½½çš„æœ¯è¯­åº“
                    preloaded_terms = trans.get('preloaded_terms')
                    if preloaded_terms:
                        # ä½¿ç”¨é¢„åŠ è½½çš„æœ¯è¯­åº“è¿›è¡Œç­›é€‰
                        try:
                            from .term_filter import optimize_terms_for_api
                            
                            # è®°å½•æœ¯è¯­åº“å¤„ç†å¼€å§‹æ—¶é—´
                            term_start_time = time.time()
                            filtered_terms = optimize_terms_for_api(old_text, preloaded_terms, max_terms=50)
                            term_end_time = time.time()
                            term_duration = term_end_time - term_start_time
                            
                            logging.info(f"ğŸ“š æœ¯è¯­åº“ç­›é€‰ç”¨æ—¶: {term_duration:.3f}ç§’, æ‰¾åˆ°æœ¯è¯­æ•°: {len(filtered_terms) if filtered_terms else 0}")
                            
                            if filtered_terms:
                                # è½¬æ¢ä¸ºtm_listæ ¼å¼
                                tm_list = []
                                for term in filtered_terms:
                                    tm_list.append({
                                        "source": term['source'],
                                        "target": term['target']
                                    })
                            else:
                                logging.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æœ¯è¯­")
                                tm_list = []
                                
                        except Exception as e:
                            logging.error(f"é¢„åŠ è½½æœ¯è¯­åº“ç­›é€‰å¤±è´¥: {str(e)}")
                            tm_list = []
                    else:
                        # æ²¡æœ‰é¢„åŠ è½½çš„æœ¯è¯­åº“ï¼Œä½¿ç”¨åŸæœ‰çš„ç­›é€‰é€»è¾‘
                        try:
                            # å¯¼å…¥æœ¯è¯­ç­›é€‰æ¨¡å—
                            from .main import get_filtered_terms_for_text
                            
                            # ä½¿ç”¨æœ¯è¯­ç­›é€‰åŠŸèƒ½ï¼Œæ ¹æ®å½“å‰æ–‡æœ¬å†…å®¹ç­›é€‰ç›¸å…³æœ¯è¯­
                            # è®°å½•æœ¯è¯­åº“å¤„ç†å¼€å§‹æ—¶é—´
                            term_start_time = time.time()
                            filtered_terms_str = get_filtered_terms_for_text(old_text, comparison_id, max_terms=50)
                            term_end_time = time.time()
                            term_duration = term_end_time - term_start_time
                            
                            logging.info(f"ğŸ“š æœ¯è¯­åº“å¤„ç†ç”¨æ—¶: {term_duration:.3f}ç§’, æ‰¾åˆ°æœ¯è¯­æ•°: {len(filtered_terms_str.split(chr(10))) if filtered_terms_str else 0}")
                            
                            if filtered_terms_str:
                                # å°†ç­›é€‰åçš„æœ¯è¯­å­—ç¬¦ä¸²è½¬æ¢ä¸ºtm_listæ ¼å¼
                                tm_list = []
                                for line in filtered_terms_str.split('\n'):
                                    if ':' in line:
                                        source, target = line.split(':', 1)
                                        tm_list.append({
                                            "source": source.strip(),
                                            "target": target.strip()
                                        })
                                
                                # logging.info(f"æœ¯è¯­ç­›é€‰å®Œæˆ: {len(tm_list)} ä¸ªæœ¯è¯­")
                            else:
                                logging.info("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æœ¯è¯­")
                                tm_list = []
                                
                        except Exception as e:
                            logging.error(f"æœ¯è¯­ç­›é€‰å¤±è´¥: {str(e)}")
                            # å¦‚æœç­›é€‰å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹é€»è¾‘
                            logging.info("å›é€€åˆ°åŸå§‹æœ¯è¯­åº“å¤„ç†é€»è¾‘")
                            
                        # æ”¯æŒå¤šä¸ªæœ¯è¯­åº“IDï¼Œé€—å·åˆ†éš”
                        comparison_ids = [int(id.strip()) for id in str(comparison_id).split(',') if id.strip().isdigit()]
                        
                        if comparison_ids:
                            all_terms = {}  # ç”¨äºå»é‡çš„å­—å…¸
                            
                            for comp_id in comparison_ids:
                                try:
                                    # ä» comparison_sub è¡¨è·å–æœ¯è¯­æ•°æ®
                                    terms = db.get_all("select original, comparison_text from comparison_sub where comparison_sub_id=%s", comp_id)
                                    
                                    if terms and isinstance(terms, list) and len(terms) > 0:
                                        for term in terms:
                                            if term and isinstance(term, dict) and term.get('original') and term.get('comparison_text'):
                                                source = term['original'].strip()
                                                target = term['comparison_text'].strip()
                                                if source not in all_terms:
                                                    all_terms[source] = target
                                    else:
                                        logging.warning(f"æœ¯è¯­åº“ {comp_id} æœªæ‰¾åˆ°æœ¯è¯­æ•°æ®")
                                        
                                except Exception as e:
                                    logging.error(f"æŸ¥è¯¢æœ¯è¯­åº“ {comp_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                                    continue
                            
                            # è½¬æ¢ä¸ºtm_listæ ¼å¼
                            if all_terms:
                                tm_list = []
                                for source, target in all_terms.items():
                                    tm_list.append({
                                        "source": source,
                                        "target": target
                                    })
                                
                                # logging.info(f"åŸå§‹æœ¯è¯­åº“å¤„ç†å®Œæˆï¼Œå…± {len(tm_list)} æ¡æœ¯è¯­")
                        else:
                            logging.warning(f"ä»»åŠ¡ {translate_id} æœ¯è¯­è¡¨IDæ ¼å¼æ— æ•ˆ: {comparison_id}")
            else:
                logging.info(f"ä»»åŠ¡ {translate_id} æœªä½¿ç”¨æœ¯è¯­åº“ï¼Œmodel: {model}, comparison_id: {comparison_id}")

            if text['complete'] == False:
                content = ''
                # ç‰¹åˆ«å¤„ç†PDFç±»å‹

                # elif extension == ".pdf":
                #     return handle_pdf(trans, event, texts, index)
                # elif extension == ".pdf":
                #     if text['type'] == "text":
                #         content = translate_html(text['text'], target_lang, model, prompt)
                #         time.sleep(0.1)
                #     else:
                #         content = get_content_by_image(text['text'], target_lang)
                #         time.sleep(0.1)
                # ---------------è¿™é‡Œå®ç°ä¸åŒæ¨¡å‹æ ¼å¼çš„è¯·æ±‚--------------
                if extension == ".md":
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼å…ƒç´ ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡ç¿»è¯‘
                    element_type = text.get('element_type', 'unknown')
                    preserve = text.get('preserve', False)
                    logging.info(f"MDå…ƒç´ æ£€æŸ¥: element_type={element_type}, preserve={preserve}, content={repr(text['text'][:50])}")
                    
                    if preserve or element_type == 'table_separator':
                        content = text['text']  # ç›´æ¥ä½¿ç”¨åŸæ–‡ï¼Œä¸ç¿»è¯‘
                        logging.info(f"âœ… è·³è¿‡è¡¨æ ¼åˆ†éš”è¡Œç¿»è¯‘: {element_type}, å†…å®¹: {repr(text['text'])}")
                    elif model == 'qwen-mt-plus':
                        logging.info(f"ğŸ” è°ƒç”¨ qwen_translate (MDæ–‡ä»¶): texts={texts is not None}, index={index}")
                        content = qwen_translate(text['text'], target_lang, source_lang="auto", tm_list=tm_list, prompt=prompt, prompt_id=trans.get('prompt_id'), texts=texts, index=index)
                    else:
                        content = req(text['text'], target_lang, model, prompt, True)
                else:
                    # ç»Ÿä¸€å¤„ç†ï¼šåªè¦æ˜¯qwen-mt-plusæ¨¡å‹ï¼Œéƒ½ä½¿ç”¨å¸¦ä¸Šä¸‹æ–‡çš„ç¿»è¯‘
                    if model == 'qwen-mt-plus':
                        logging.info(f"ğŸ” è°ƒç”¨ qwen_translate (ç»Ÿä¸€å¤„ç†): texts={texts is not None}, index={index}")
                        content = qwen_translate(text['text'], target_lang, source_lang="auto", tm_list=tm_list, prompt=prompt, prompt_id=trans.get('prompt_id'), texts=texts, index=index)
                    else:
                        # å…¶ä»–æ¨¡å‹ï¼šæ ¹æ®æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡é€‰æ‹©ç¿»è¯‘æ–¹å¼
                        if 'context_text' in text and text.get('context_type') == 'body':
                            # æ­£æ–‡æ®µè½ï¼šä½¿ç”¨å¸¦ä¸Šä¸‹æ–‡çš„æ–‡æœ¬
                            content = req(text['context_text'], target_lang, model, prompt, False)
                        else:
                            # å…¶ä»–å†…å®¹ï¼šä½¿ç”¨åŸå§‹æ–‡æœ¬
                            content = req(text['text'], target_lang, model, prompt, False)
                    # print("content", text['content'])
                text['count'] = count_text(text['text'])
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯data_inspection_failedå¯¼è‡´çš„ç©ºå­—ç¬¦ä¸²
                if content == "" and model == 'qwen-mt-plus':
                    # data_inspection_failedé”™è¯¯ï¼Œç›´æ¥è®¾ç½®ä¸ºå®ŒæˆçŠ¶æ€ï¼Œä¸è¿›è¡Œé‡è¯•
                    logging.warning(f"å†…å®¹æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡æ­¤å†…å®¹: {text['text'][:50]}...")
                    text['text'] = ""  # è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
                    text['complete'] = True
                elif check_translated(content):
                    # è¿‡æ»¤deepseekæ€è€ƒè¿‡ç¨‹
                    cleaned_content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
                    # æ¸…ç†ä¸Šä¸‹æ–‡æ ‡è®°
                    # cleaned_content = clean_translation_result(cleaned_content)
                    text['text'] = cleaned_content
                    text['complete'] = True
                else:
                    # ç¿»è¯‘å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­å¤„ç†
                    logging.warning(f"ç¿»è¯‘å¤±è´¥ï¼Œä¿ç•™åŸæ–‡: {text['text'][:50]}...")
                    text['complete'] = True
        except openai.AuthenticationError as e:
            # set_threading_num(mredis)
            return use_backup_model(trans, event, texts, index, "openaiå¯†é’¥æˆ–ä»¤ç‰Œæ— æ•ˆ")
        except openai.APIConnectionError as e:
            # set_threading_num(mredis)
            return use_backup_model(trans, event, texts, index,
                                    "è¯·æ±‚æ— æ³•ä¸openaiæœåŠ¡å™¨æˆ–å»ºç«‹å®‰å…¨è¿æ¥")
        except openai.PermissionDeniedError as e:
            # set_threading_num(mredis)
            texts[index] = text
            # return use_backup_model(trans, event, texts, index, "ä»¤ç‰Œé¢åº¦ä¸è¶³")
        except openai.RateLimitError as e:
            # set_threading_num(mredis)
            if "retry" not in text:
                trans['model'] = backup_model
                trans['backup_model'] = model
                time.sleep(1)
                logging.warning("è®¿é—®é€Ÿç‡è¾¾åˆ°é™åˆ¶,äº¤æ¢å¤‡ç”¨æ¨¡å‹ä¸æ¨¡å‹é‡æ–°é‡è¯•")
                get(trans, event, texts, index)
            else:
                return use_backup_model(trans, event, texts, index,
                                        "è®¿é—®é€Ÿç‡è¾¾åˆ°é™åˆ¶,10åˆ†é’Ÿåå†è¯•" + str(text['text']))
        except openai.InternalServerError as e:
            # set_threading_num(mredis)
            if "retry" not in text:
                trans['model'] = backup_model
                trans['backup_model'] = model
                time.sleep(1)
                logging.warning("å½“å‰åˆ†ç»„ä¸Šæ¸¸è´Ÿè½½å·²é¥±å’Œï¼Œäº¤æ¢å¤‡ç”¨æ¨¡å‹ä¸æ¨¡å‹é‡æ–°é‡è¯•")
                get(trans, event, texts, index)
            else:
                return use_backup_model(trans, event, texts, index,
                                        "å½“å‰åˆ†ç»„ä¸Šæ¸¸è´Ÿè½½å·²é¥±å’Œï¼Œè¯·ç¨åå†è¯•" + str(text['text']))
        except openai.APIStatusError as e:
            # set_threading_num(mredis)
            return use_backup_model(trans, event, texts, index, e.response)
        except Exception as e:
            # set_threading_num(mredis)
            exc_type, exc_value, exc_traceback = sys.exc_info()
            line_number = exc_traceback.tb_lineno  # å¼‚å¸¸æŠ›å‡ºçš„å…·ä½“è¡Œå·
            logging.error(f"Error occurred on line: {line_number}")
            logging.error(f"Error details: {e}")
            if "retry" not in text:
                text["retry"] = 0
            text["retry"] += 1
            if text["retry"] <= 3:
                trans['model'] = backup_model
                trans['backup_model'] = model
                logging.warning("å½“å‰æ¨¡å‹æ‰§è¡Œå¼‚å¸¸ï¼Œäº¤æ¢å¤‡ç”¨æ¨¡å‹ä¸æ¨¡å‹é‡æ–°é‡è¯•")
                time.sleep(1)
                get(trans, event, texts, index)
                return
            else:
                text['complete'] = True
            # traceback.print_exc()
            # print("translate error")
    
    texts[index] = text
    # print(text)
    if not event.is_set():
        # å¯¹äºWordæ–‡æ¡£ç¿»è¯‘ï¼Œä¸è°ƒç”¨processå‡½æ•°ï¼Œå› ä¸ºWordç¿»è¯‘æœ‰è‡ªå·±çš„è¿›åº¦æ›´æ–°æœºåˆ¶
        extension = trans.get('extension', '').lower()
        if extension not in ['.docx', '.doc']:
            process(texts, translate_id)
    # set_threading_num(mredis)
    return True  # è¿”å›ç»“æœè€Œä¸æ˜¯exit(0)


def get11(trans, event, texts, index):
    if event.is_set():
        exit(0)
    # æ¢å¤çº¿ç¨‹æ•°ä¸º30ï¼Œæé«˜ç¿»è¯‘æ•ˆç‡
    max_threads = 30
    # mredis=rediscon.get_conn()
    # threading_num=get_threading_num(mredis)
    # while threading_num>=max_threads:
    #    time.sleep(1)
    # print('transé…ç½®é¡¹', trans)
    translate_id = trans['id']
    target_lang = trans['lang']
    model = trans['model']
    backup_model = trans['backup_model']
    prompt = trans['prompt']
    extension = trans['extension'].lower()
    text = texts[index]
    api_key = trans['api_key']
    api_url = trans['api_url']
    old_text = text['text']
    md5_key = md5_encryption(
        str(api_key) + str(api_url) + str(old_text) + str(prompt) + str(backup_model) + str(
            model) + str(target_lang))
    try:
        # mredis.set("threading_count",threading_num+1)
        if text['complete'] == False:
            content = ''
            # ç‰¹åˆ«å¤„ç†PDFç±»å‹

            # elif extension == ".pdf":
            #     return handle_pdf(trans, event, texts, index)
            # elif extension == ".pdf":
            #     if text['type'] == "text":
            #         content = translate_html(text['text'], target_lang, model, prompt)
            #         time.sleep(0.1)
            #     else:
            #         content = get_content_by_image(text['text'], target_lang)
            #         time.sleep(0.1)
            # ---------------è¿™é‡Œå®ç°ä¸åŒæ¨¡å‹æ ¼å¼çš„è¯·æ±‚--------------
            if extension == ".md":
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼å…ƒç´ ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡ç¿»è¯‘
                element_type = text.get('element_type', 'unknown')
                preserve = text.get('preserve', False)
                logging.info(f"MDå…ƒç´ æ£€æŸ¥: element_type={element_type}, preserve={preserve}, content={repr(text['text'][:50])}")
                
                if preserve or element_type == 'table_separator':
                    content = text['text']  # ç›´æ¥ä½¿ç”¨åŸæ–‡ï¼Œä¸ç¿»è¯‘
                    logging.info(f"âœ… è·³è¿‡è¡¨æ ¼åˆ†éš”è¡Œç¿»è¯‘: {element_type}, å†…å®¹: {repr(text['text'])}")
                else:
                    content = req(text['text'], target_lang, model, prompt, True)
            else:
                content = req(text['text'], target_lang, model, prompt, False)
                # print("content", text['content'])
            text['count'] = count_text(text['text'])
            if check_translated(content):
                # è¿‡æ»¤deepseekæ€è€ƒè¿‡ç¨‹
                text['text'] = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
            text['complete'] = True
    except openai.AuthenticationError as e:
        # set_threading_num(mredis)
        return use_backup_model(trans, event, texts, index, "openaiå¯†é’¥æˆ–ä»¤ç‰Œæ— æ•ˆ")
    except openai.APIConnectionError as e:
        # set_threading_num(mredis)
        return use_backup_model(trans, event, texts, index, "è¯·æ±‚æ— æ³•ä¸openaiæœåŠ¡å™¨æˆ–å»ºç«‹å®‰å…¨è¿æ¥")
    except openai.PermissionDeniedError as e:
        # set_threading_num(mredis)
        texts[index] = text
        # return use_backup_model(trans, event, texts, index, "ä»¤ç‰Œé¢åº¦ä¸è¶³")
    except openai.RateLimitError as e:
        # set_threading_num(mredis)
        if "retry" not in text:
            trans['model'] = backup_model
            trans['backup_model'] = model
            time.sleep(1)
            logging.warning("è®¿é—®é€Ÿç‡è¾¾åˆ°é™åˆ¶,äº¤æ¢å¤‡ç”¨æ¨¡å‹ä¸æ¨¡å‹é‡æ–°é‡è¯•")
            get(trans, event, texts, index)
        else:
            return use_backup_model(trans, event, texts, index,
                                    "è®¿é—®é€Ÿç‡è¾¾åˆ°é™åˆ¶,10åˆ†é’Ÿåå†è¯•" + str(text['text']))
    except openai.InternalServerError as e:
        # set_threading_num(mredis)
        if "retry" not in text:
            trans['model'] = backup_model
            trans['backup_model'] = model
            time.sleep(1)
            logging.warning("å½“å‰åˆ†ç»„ä¸Šæ¸¸è´Ÿè½½å·²é¥±å’Œï¼Œäº¤æ¢å¤‡ç”¨æ¨¡å‹ä¸æ¨¡å‹é‡æ–°é‡è¯•")
            get(trans, event, texts, index)
        else:
            return use_backup_model(trans, event, texts, index,
                                    "å½“å‰åˆ†ç»„ä¸Šæ¸¸è´Ÿè½½å·²é¥±å’Œï¼Œè¯·ç¨åå†è¯•" + str(text['text']))
    except openai.APIStatusError as e:
        # set_threading_num(mredis)
        return use_backup_model(trans, event, texts, index, e.response)
    except Exception as e:
        # set_threading_num(mredis)
        exc_type, exc_value, exc_traceback = sys.exc_info()
        line_number = exc_traceback.tb_lineno  # å¼‚å¸¸æŠ›å‡ºçš„å…·ä½“è¡Œå·
        logging.error(f"Error occurred on line: {line_number}")
        logging.error(f"Error details: {e}")
        if "retry" not in text:
            text["retry"] = 0
        text["retry"] += 1
        if text["retry"] <= 3:
            trans['model'] = backup_model
            trans['backup_model'] = model
            logging.warning("å½“å‰æ¨¡å‹æ‰§è¡Œå¼‚å¸¸ï¼Œäº¤æ¢å¤‡ç”¨æ¨¡å‹ä¸æ¨¡å‹é‡æ–°é‡è¯•")
            time.sleep(1)
            get(trans, event, texts, index)
            return
        else:
            text['complete'] = True
        # traceback.print_exc()
        # print("translate error")
    texts[index] = text
    # print(text)
    if not event.is_set():
        process(texts, translate_id)
    # set_threading_num(mredis)
    exit(0)


def handle_pdf(trans, event, texts, index):
    try:
        from . import pdf_parser
        success = pdf_parser.start(trans)
        if success:
            texts[index]['complete'] = True
        else:
            return use_backup_model(trans, event, texts, index, "PDFè§£æå¤±è´¥")
    except Exception as e:
        return use_backup_model(trans, event, texts, index, str(e))



# def get_threading_num(mredis):
#    threading_count=mredis.get("threading_count")
#    if threading_count is None or threading_count=="" or int(threading_count)<0:
#        threading_num=0
#    else:
#        threading_num=int(threading_count)
#    return threading_num
# def set_threading_num(mredis):
#    threading_count=mredis.get("threading_count")
#    if threading_count is None or threading_count=="" or int(threading_count)<1:
#        mredis.set("threading_count",0)
#    else:
#        threading_num=int(threading_count)
#        mredis.set("threading_count",threading_num-1)

def md5_encryption(data):
    md5 = hashlib.md5(data.encode('utf-8'))  # åˆ›å»ºä¸€ä¸ªmd5å¯¹è±¡
    return md5.hexdigest()  # è¿”å›åŠ å¯†åçš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²


def req(text, target_lang, model, prompt, ext):
    # åˆ¤æ–­æ˜¯å¦æ˜¯mdæ ¼å¼
    if ext == True:
        # å¦‚æœæ˜¯ md æ ¼å¼ï¼Œè¿½åŠ æç¤ºæ–‡æœ¬
        prompt += "ã€‚ è¯·å¸®åŠ©æˆ‘ç¿»è¯‘ä»¥ä¸‹ Markdown æ–‡ä»¶ä¸­çš„å†…å®¹ã€‚è¯·æ³¨æ„ï¼Œæ‚¨åªéœ€ç¿»è¯‘æ–‡æœ¬éƒ¨åˆ†ï¼Œè€Œä¸åº”æ›´æ”¹ä»»ä½• Markdown æ ‡ç­¾æˆ–æ ¼å¼ã€‚ä¿æŒåŸæœ‰çš„æ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—ã€é“¾æ¥å’Œå…¶ä»– Markdown æ ‡ç­¾çš„å®Œæ•´æ€§ã€‚"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡æ ‡è®°
    if '[å‰æ–‡:' in text or '[åæ–‡:' in text:
        # æœ‰ä¸Šä¸‹æ–‡çš„æƒ…å†µï¼šå¢å¼ºæç¤ºè¯
        enhanced_prompt = f"""
        {prompt}
        
        ä¸¥æ ¼æŒ‡ä»¤ï¼š
        1. åªç¿»è¯‘æ–¹æ‹¬å·å¤–çš„æ–‡æœ¬ï¼Œä¸è¦ç¿»è¯‘æ–¹æ‹¬å·å†…çš„ä»»ä½•å†…å®¹
        2. çº¯å¤§å†™è‹±æ–‡çš„ä¸ºä¸“æœ‰åè¯ï¼Œä¸è¦ç¿»è¯‘
        3. è¯·ç»“åˆä¸Šä¸‹æ–‡çš„è¯­ä¹‰è¯­å¢ƒè¿›è¡Œç¿»è¯‘
        4. ä¸è¦è¾“å‡ºä»»ä½•æ–¹æ‹¬å·æ ‡è®°
        5. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ€è€ƒè¿‡ç¨‹
        6. åªè¿”å›çº¯ç¿»è¯‘ç»“æœ
        7. ç›®æ ‡è¯­è¨€ä¸º{target_lang}
        
        ç¤ºä¾‹ï¼š
        è¾“å…¥ï¼š"[å‰æ–‡: Hello] World [åæ–‡: Program]"
        è¾“å‡ºï¼š"ä¸–ç•Œ"
        
        è¾“å…¥ï¼š"[å‰æ–‡: ä½ å¥½] ä¸–ç•Œ [åæ–‡: ç¨‹åº]"
        è¾“å‡ºï¼š"world"
        
        è®°ä½ï¼šæ–¹æ‹¬å·å†…çš„å†…å®¹æ˜¯ä¸Šä¸‹æ–‡å‚è€ƒç”¨äºç¿»è¯‘å‰åçš„è¯­ä¹‰è¯­å¢ƒï¼Œä¸éœ€è¦ç¿»è¯‘å’Œè¾“å‡ºï¼
        """
    else:
        # æ²¡æœ‰ä¸Šä¸‹æ–‡çš„æƒ…å†µï¼šä½¿ç”¨åŸå§‹æç¤ºè¯
        enhanced_prompt = prompt
    
    # æ„å»º message
    message = [
        {"role": "system", "content": enhanced_prompt.replace("{target_lang}", target_lang)},
        {"role": "user", "content": text}
    ]
    # print(openai.base_url)
    logging.info(message)
    # ç¦ç”¨ OpenAI çš„æ—¥å¿—è¾“å‡º
    logging.getLogger("openai").setLevel(logging.WARNING)
    # ç¦ç”¨ httpx çš„æ—¥å¿—è¾“å‡º
    logging.getLogger("httpx").setLevel(logging.WARNING)
    response = openai.chat.completions.create(
        model=model,  # ä½¿ç”¨GPT-3.5ç‰ˆæœ¬
        messages=message,
        temperature=0.8
    )
    # for choices in response.choices:
    #     print(choices.message.content)
    content = response.choices[0].message.content
    # print(content)
    return content


def translate_html(html, target_lang, model, prompt):
    message = [
        {"role": "system",
         "content": "æŠŠä¸‹é¢çš„htmlç¿»è¯‘æˆ{},åªè¿”å›ç¿»è¯‘åçš„å†…å®¹".format(target_lang)},
        {"role": "user", "content": html}
    ]
    # print(openai.base_url)
    response = openai.chat.completions.create(
        model=model,
        messages=message
    )
    # for choices in response.choices:
    #     print(choices.message.content)
    content = response.choices[0].message.content
    return content


def get_content_by_image(base64_image, target_lang):
    # print(image_path)
    # file_object = openai.files.create(file=Path(image_path), purpose="è¿™æ˜¯ä¸€å¼ å›¾ç‰‡")
    # print(file_object)
    message = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå›¾ç‰‡ORCè¯†åˆ«ä¸“å®¶"},
        {"role": "user", "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": base64_image
                }
            },
            {
                "type": "text",
                # "text": "è¯»å–å›¾ç‰‡é“¾æ¥å¹¶æå–å…¶ä¸­çš„æ–‡æœ¬æ•°æ®,åªè¿”å›è¯†åˆ«åçš„æ•°æ®ï¼Œå°†æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡,å¹¶æŒ‰ç…§å›¾ç‰‡ä¸­çš„æ–‡å­—å¸ƒå±€è¿”å›htmlã€‚åªåŒ…å«body(ä¸åŒ…å«bodyæœ¬èº«)éƒ¨åˆ†",
                # "text": f"æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—æ•°æ®ï¼Œå°†æå–çš„æ–‡æœ¬ç¿»è¯‘æˆ{target_lang},åªè¿”å›åŸå§‹æ–‡æœ¬å’Œç¿»è¯‘ç»“æœ",
                "text": f"æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—æ•°æ®,å°†æå–çš„æ–‡æœ¬ç¿»è¯‘æˆ{target_lang},åªè¿”å›ç¿»è¯‘ç»“æœ",
            }
        ]}
    ]
    # print(message)
    # print(openai.base_url)
    response = openai.chat.completions.create(
        model="gpt-4o",  # ä½¿ç”¨GPT-3.5ç‰ˆæœ¬
        messages=message
    )
    # for choices in response.choices:
    #     print(choices.message.content)
    content = response.choices[0].message.content
    # return content
    # print(''.join(map(lambda x: f'<p>{x}</p>',content.split("\n"))))
    return ''.join(map(lambda x: f'<p>{x}</p>', content.split("\n")))


def check(model):
    try:
        message = [
            {"role": "system", "content": "ä½ é€šæ™“ä¸–ç•Œæ‰€æœ‰è¯­è¨€,å¯ä»¥ç”¨æ¥ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€"},
            {"role": "user", "content": "ä½ ç°åœ¨èƒ½ç¿»è¯‘å—ï¼Ÿ"}
        ]
        response = openai.chat.completions.create(
            model=model,
            messages=message
        )
        return "OK"
    except openai.AuthenticationError as e:
        return "openaiå¯†é’¥æˆ–ä»¤ç‰Œæ— æ•ˆ"
    except openai.APIConnectionError as e:
        return "è¯·æ±‚æ— æ³•ä¸openaiæœåŠ¡å™¨æˆ–å»ºç«‹å®‰å…¨è¿æ¥"
    except openai.PermissionDeniedError as e:
        return "ä»¤ç‰Œé¢åº¦ä¸è¶³"
    except openai.RateLimitError as e:
        return "è®¿é—®é€Ÿç‡è¾¾åˆ°é™åˆ¶,10åˆ†é’Ÿåå†è¯•"
    except openai.InternalServerError as e:
        return "å½“å‰åˆ†ç»„ä¸Šæ¸¸è´Ÿè½½å·²é¥±å’Œï¼Œè¯·ç¨åå†è¯•"
    except openai.APIStatusError as e:
        return e.response
    except Exception as e:
        return "å½“å‰æ— æ³•å®Œæˆç¿»è¯‘"


def process(texts, translate_id):
    total = 0
    complete = 0
    for text in texts:
        total += 1
        if text['complete']:
            complete += 1
    if total != complete:
        if (total != 0):
            process = format((complete / total) * 100, '.1f')
            db.execute("update translate set process=%s where id=%s", str(process), translate_id)


def complete(trans, text_count, spend_time):
    target_filesize = 1 #os.stat(trans['target_file']).st_size
    # ä½¿ç”¨Pythonæ—¶åŒºæ—¶é—´ï¼Œä¸start_atä¿æŒä¸€è‡´
    from datetime import datetime
    import pytz
    end_time = datetime.now(pytz.timezone('Asia/Shanghai'))  # ä½¿ç”¨ä¸œå…«åŒºæ—¶åŒºï¼Œä¸translate_service.pyä¿æŒä¸€è‡´
    
    # ç¡®ä¿target_filepathå­—æ®µè¢«æ­£ç¡®æ›´æ–°
    target_filepath = trans.get('target_file', '')
    
    db.execute(
        "update translate set status='done',end_at=%s,process=100,target_filesize=%s,word_count=%s,target_filepath=%s where id=%s",
        end_time, target_filesize, text_count, target_filepath, trans['id'])


def error(translate_id, message):
    # ä½¿ç”¨Pythonæ—¶åŒºæ—¶é—´ï¼Œä¸start_atä¿æŒä¸€è‡´
    from datetime import datetime
    import pytz
    end_time = datetime.now(pytz.timezone('Asia/Shanghai'))  # ä½¿ç”¨ä¸œå…«åŒºæ—¶åŒºï¼Œä¸translate_service.pyä¿æŒä¸€è‡´
    
    db.execute(
        "update translate set failed_count=failed_count+1,status='failed',end_at=%s,failed_reason=%s where id=%s",
        end_time, message, translate_id)


def count_text(text):
    count = 0
    for char in text:
        if common.is_chinese(char):
            count += 1;
        elif char is None or char == " ":
            continue
        else:
            count += 0.5
    return count


def init_openai(url, key):
    openai.api_key = key
    if "v1" not in url:
        if url[-1] == "/":
            url += "v1/"
        else:
            url += "/v1/"
    openai.base_url = url


def check_translated(content):
    if content.startswith("Sorry, I cannot") or content.startswith(
            "I am sorry,") or content.startswith(
            "I'm sorry,") or content.startswith("Sorry, I can't") or content.startswith(
        "Sorry, I need more") or content.startswith("æŠ±æ­‰ï¼Œæ— æ³•") or content.startswith(
        "é”™è¯¯ï¼šæä¾›çš„æ–‡æœ¬") or content.startswith("æ— æ³•ç¿»è¯‘") or content.startswith(
        "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•") or content.startswith(
        "å¯¹ä¸èµ·ï¼Œæˆ‘æ— æ³•") or content.startswith("ã”æŒ‡ç¤ºã®å†…å®¹ã¯") or content.startswith(
        "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“") or content.startswith("ĞŸÑ€Ğ¾ÑÑ‚Ğ¸Ñ‚Ğµï¼Œ") or content.startswith(
        "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ,") or content.startswith("Lo siento,"):
        return False
    else:
        return True


# def get_model_tokens(model,content):
#     encoding=tiktoken.encoding_for_model(model)
#     return en(encoding.encode(content))

def use_backup_model(trans, event, texts, index, message):
    if trans['backup_model'] != None and trans['backup_model'] != "":
        trans['model'] = trans['backup_model']
        trans['backup_model'] = ""
        get(trans, event, texts, index)
    else:
        if not event.is_set():
            error(trans['id'], message)
            print(message)
        event.set()


def clean_translation_result(text):
    """æ¸…ç†ç¿»è¯‘ç»“æœï¼Œç§»é™¤ä¸Šä¸‹æ–‡æ ‡è®°"""
    import re
    
    # ç§»é™¤ä¸Šä¸‹æ–‡æ ‡è®°
    text = re.sub(r'\[å‰æ–‡:.*?\]', '', text)
    text = re.sub(r'\[åæ–‡:.*?\]', '', text)
    
    # ç§»é™¤å¯èƒ½çš„è§£é‡Šæ€§å†…å®¹
    text = re.sub(r'ç¿»è¯‘.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'æ ¹æ®ä¸Šä¸‹æ–‡.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'ç­”æ¡ˆæ˜¯.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'ç»“æœ.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'åº”è¯¥æ˜¯.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'è¾“å‡º.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'è¾“å…¥.*?[:ï¼š]\s*', '', text)
    
    # ç§»é™¤å¯èƒ½çš„æ€è€ƒè¿‡ç¨‹
    text = re.sub(r'è®©æˆ‘åˆ†æ.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'æ ¹æ®.*?[:ï¼š]\s*', '', text)
    text = re.sub(r'è®°ä½.*?[:ï¼š]\s*', '', text)
    
    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()