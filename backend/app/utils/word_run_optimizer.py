"""
Wordæ–‡æ¡£Runä¼˜åŒ–å™¨ - å®‰å…¨åˆå¹¶æœºåˆ¶

æœ¬æ¨¡å—æä¾›äº†ä¸€ä¸ªå®‰å…¨ã€å¯é çš„Wordæ–‡æ¡£runåˆå¹¶æœºåˆ¶ï¼Œç”¨äºä¼˜åŒ–æ–‡æ¡£ç»“æ„ï¼Œå‡å°‘ä¸å¿…è¦çš„runåˆ†å‰²ã€‚
åœ¨ä¿è¯100%æ ¼å¼ä¸å˜çš„å‰æä¸‹ï¼Œåˆå¹¶é‚£äº›è¢«Wordé”™è¯¯åˆ†å‰²çš„runsã€‚

1. å®Œæ•´çš„æ ·å¼æå– ğŸ”

æå–30+ä¸ªæ ·å¼å±æ€§
åŒ…æ‹¬XMLçº§åˆ«çš„éšè—å±æ€§
ç”Ÿæˆå±æ€§å“ˆå¸Œå€¼å¿«é€Ÿæ¯”è¾ƒ

2. ç‰¹æ®Šè¾¹ç•Œæ£€æµ‹ ğŸš§
ç³»ç»Ÿä¼šæ£€æµ‹å¹¶ä¿æŠ¤ä»¥ä¸‹å…ƒç´ ï¼š

åŸŸä»£ç ï¼ˆField codesï¼‰
è¶…é“¾æ¥
ä¹¦ç­¾
æ‰¹æ³¨å’Œä¿®è®¢
è„šæ³¨/å°¾æ³¨
åˆ†éš”ç¬¦ï¼ˆæ¢è¡Œã€åˆ†é¡µç­‰ï¼‰
å›¾å½¢å¯¹è±¡
åˆ¶è¡¨ç¬¦

3. å®‰å…¨åˆå¹¶ç­–ç•¥ âœ…
åªæœ‰æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ¡ä»¶çš„runsæ‰ä¼šè¢«åˆå¹¶ï¼š

æ ·å¼å±æ€§100%ç›¸åŒ
éƒ½ä¸åŒ…å«ä»»ä½•ç‰¹æ®Šè¾¹ç•Œå…ƒç´ 
éƒ½ä¸æ˜¯ç©ºæ–‡æœ¬ï¼ˆç©ºæ–‡æœ¬å¯èƒ½æœ‰ç‰¹æ®Šç”¨é€”ï¼‰

ä¸»è¦åŠŸèƒ½ï¼š
1. æ·±åº¦åˆ†ærunçš„æ‰€æœ‰æ ·å¼å±æ€§ï¼ˆ30+ä¸ªå±æ€§ï¼‰
2. æ£€æµ‹ç‰¹æ®Šè¾¹ç•Œå…ƒç´ ï¼ˆè¶…é“¾æ¥ã€åŸŸä»£ç ã€æ‰¹æ³¨ç­‰ï¼‰
3. å®‰å…¨åˆå¹¶æ ¼å¼å®Œå…¨ç›¸åŒçš„ç›¸é‚»runs
4. ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹ï¼š
    >>> from word_run_optimizer import SafeRunMerger
    >>> merger = SafeRunMerger()
    >>> stats = merger.optimize_document("input.docx", "output.docx")
    >>> print(f"ä¼˜åŒ–å®Œæˆï¼Œå‡å°‘äº†{stats['merged_runs']}ä¸ªruns")

ä¾èµ–ï¼š
    - python-docx >= 0.8.10
    - lxml >= 4.6.0

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼š1.0.0
"""

import docx
from docx.shared import RGBColor, Pt
from docx.oxml import parse_xml, register_element_cls
from docx.oxml.ns import nsdecls
from docx.oxml.text.run import CT_R
import hashlib
import json
import logging
from typing import List, Dict, Tuple, Optional, Any
import copy
import os
import sys

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ç‰ˆæœ¬å…¼å®¹æ€§å¸¸é‡
DOCX_MIN_VERSION = "0.8.10"
PYTHON_MIN_VERSION = (3, 6)


class DocxCompatibilityError(Exception):
    """python-docxç‰ˆæœ¬å…¼å®¹æ€§é”™è¯¯"""
    pass


class RunOptimizationError(Exception):
    """Runä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é”™è¯¯"""
    pass


def check_compatibility():
    """
    æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§
    
    Raises:
        DocxCompatibilityError: å½“ç¯å¢ƒä¸æ»¡è¶³è¦æ±‚æ—¶
    """
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    if sys.version_info < PYTHON_MIN_VERSION:
        raise DocxCompatibilityError(
            f"éœ€è¦Python {PYTHON_MIN_VERSION[0]}.{PYTHON_MIN_VERSION[1]}æˆ–æ›´é«˜ç‰ˆæœ¬"
        )
    
    # æ£€æŸ¥python-docxç‰ˆæœ¬
    try:
        import docx
        docx_version = getattr(docx, '__version__', '0.0.0')
        # ç®€å•çš„ç‰ˆæœ¬æ¯”è¾ƒï¼ˆå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç‰ˆæœ¬æ¯”è¾ƒï¼‰
        if docx_version < DOCX_MIN_VERSION:
            raise DocxCompatibilityError(
                f"éœ€è¦python-docx {DOCX_MIN_VERSION}æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå½“å‰ç‰ˆæœ¬ï¼š{docx_version}"
            )
    except ImportError:
        raise DocxCompatibilityError("æœªå®‰è£…python-docxåº“")


class RunStyleAnalyzer:
    """
    Runæ ·å¼æ·±åº¦åˆ†æå™¨
    
    è´Ÿè´£æå–å’Œæ¯”è¾ƒrunçš„æ‰€æœ‰æ ·å¼å±æ€§ï¼ŒåŒ…æ‹¬ï¼š
    - åŸºæœ¬å­—ç¬¦æ ¼å¼ï¼ˆç²—ä½“ã€æ–œä½“ã€ä¸‹åˆ’çº¿ç­‰ï¼‰
    - å­—ä½“å±æ€§ï¼ˆå­—ä½“åç§°ã€å¤§å°ã€é¢œè‰²ç­‰ï¼‰
    - é«˜çº§æ ¼å¼ï¼ˆä¸Šä¸‹æ ‡ã€å­—ç¬¦é—´è·ç­‰ï¼‰
    - XMLçº§åˆ«çš„ç‰¹æ®Šå±æ€§
    """
    
    @staticmethod
    def extract_all_properties(run) -> Dict[str, Any]:
        """
        æå–runçš„æ‰€æœ‰æ ·å¼å±æ€§
        
        Args:
            run: python-docxçš„Runå¯¹è±¡
            
        Returns:
            Dict[str, Any]: åŒ…å«æ‰€æœ‰æ ·å¼å±æ€§çš„å­—å…¸
            
        Note:
            - æŸäº›å±æ€§å¯èƒ½åœ¨ä¸åŒç‰ˆæœ¬çš„Wordä¸­è¡¨ç°ä¸åŒ
            - Noneå€¼è¡¨ç¤ºå±æ€§æœªè®¾ç½®ï¼ˆç»§æ‰¿è‡ªçˆ¶çº§æ ·å¼ï¼‰
            - è¿”å›çš„å­—å…¸å¯ç”¨äºå®Œæ•´é‡å»ºrunæ ·å¼
        """
        properties = {}
        
        # å®šä¹‰å±æ€§åˆ—è¡¨ï¼Œä½¿ç”¨getattrå®‰å…¨è·å–
        # 1. åŸºæœ¬å­—ç¬¦æ ¼å¼å±æ€§
        basic_attrs = [
            ('bold', None),  # ç²—ä½“
            ('italic', None),  # æ–œä½“
            ('underline', None),  # ä¸‹åˆ’çº¿
            ('strike', None),  # åˆ é™¤çº¿
            ('double_strike', None),  # åŒåˆ é™¤çº¿
            ('outline', None),  # è½®å»“
            ('shadow', None),  # é˜´å½±
            ('emboss', None),  # æµ®é›•
            ('imprint', None),  # å°è®°
            ('all_caps', None),  # å…¨éƒ¨å¤§å†™
            ('small_caps', None),  # å°å‹å¤§å†™å­—æ¯
            ('hidden', None),  # éšè—
            ('web_hidden', None),  # Webéšè—
            ('spec_vanish', None),  # ç‰¹æ®Šéšè—
            ('rtl', None),  # ä»å³åˆ°å·¦
            ('cs_bold', None),  # å¤æ‚è„šæœ¬ç²—ä½“
            ('cs_italic', None),  # å¤æ‚è„šæœ¬æ–œä½“
            ('complex_script', None),  # å¤æ‚è„šæœ¬
        ]
        
        # å®‰å…¨è·å–åŸºæœ¬å±æ€§
        for attr_name, default_value in basic_attrs:
            try:
                properties[attr_name] = getattr(run, attr_name, default_value)
            except AttributeError:
                properties[attr_name] = default_value
            except Exception:
                properties[attr_name] = default_value
            
        
        # 2. å­—ä½“åŸºæœ¬å±æ€§
        if hasattr(run, 'font') and run.font:
            try:
                properties['font_name'] = getattr(run.font, 'name', None)
                
                # å­—ä½“å¤§å°
                font_size = getattr(run.font, 'size', None)
                if font_size:
                    properties['font_size'] = getattr(font_size, 'pt', None)
                else:
                    properties['font_size'] = None
                
                # å­—ä½“é¢œè‰²
                font_color = getattr(run.font, 'color', None)
                if font_color:
                    properties['font_color_rgb'] = getattr(font_color, 'rgb', None)
                    properties['font_color_theme'] = getattr(font_color, 'theme_color', None)
                    properties['font_color_brightness'] = getattr(font_color, 'brightness', None)
                else:
                    properties['font_color_rgb'] = None
                    properties['font_color_theme'] = None
                    properties['font_color_brightness'] = None
                
                # é«˜äº®é¢œè‰²
                properties['font_highlight'] = getattr(run.font, 'highlight_color', None)
                
                # 3. é«˜çº§å­—ä½“å±æ€§
                font_attrs = [
                    ('subscript', None),  # ä¸‹æ ‡
                    ('superscript', None),  # ä¸Šæ ‡
                    ('kern', None),  # å­—è·è°ƒæ•´
                    ('position', None),  # ä½ç½®è°ƒæ•´
                    ('spacing', None),  # å­—ç¬¦é—´è·
                    ('scale', None),  # å­—ç¬¦ç¼©æ”¾
                ]
                
                for attr_name, default_value in font_attrs:
                    properties[attr_name] = getattr(run.font, attr_name, default_value)
                    
            except Exception as e:
                logger.debug(f"å¤„ç†å­—ä½“å±æ€§æ—¶å‡ºç°å°é—®é¢˜: {str(e)}")
        
        # 4. æ ·å¼å¼•ç”¨
        try:
            if hasattr(run, 'style') and run.style:
                properties['style_id'] = getattr(run.style, 'style_id', None)
                properties['style_name'] = getattr(run.style, 'name', None)
            else:
                properties['style_id'] = None
                properties['style_name'] = None
        except Exception:
            properties['style_id'] = None
            properties['style_name'] = None
            
        
        # 5. XMLçº§åˆ«çš„ç‰¹æ®Šå±æ€§
        # è¿™äº›å±æ€§å¯èƒ½å½±å“ç‰¹æ®Šå¤„ç†é€»è¾‘
        try:
            if hasattr(run, '_element') and run._element is not None:
                element = run._element
                
                # XMLç©ºæ ¼ä¿ç•™å±æ€§
                properties['xml_space_preserve'] = element.get(
                    '{http://www.w3.org/XML/1998/namespace}space'
                )
                
                # è·å–run propertieså…ƒç´ 
                try:
                    rPr = element.find(
                        './/w:rPr', 
                        {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                    )
                    
                    if rPr is not None:
                        # è¯­è¨€è®¾ç½®ï¼ˆå½±å“æ‹¼å†™æ£€æŸ¥å’Œæ–­å­—ï¼‰
                        lang = rPr.find(
                            './/w:lang', 
                            {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                        )
                        if lang is not None:
                            properties['lang'] = lang.get(
                                '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val'
                            )
                            properties['lang_bidi'] = lang.get(
                                '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}bidi'
                            )
                            properties['lang_eastAsia'] = lang.get(
                                '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}eastAsia'
                            )
                        
                        # å‚ç›´å¯¹é½
                        vertAlign = rPr.find(
                            './/w:vertAlign', 
                            {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                        )
                        if vertAlign is not None:
                            properties['vert_align'] = vertAlign.get(
                                '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val'
                            )
                        
                        # æ–‡å­—æ•ˆæœ
                        effect = rPr.find(
                            './/w:effect', 
                            {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                        )
                        if effect is not None:
                            properties['effect'] = effect.get(
                                '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val'
                            )
                except Exception as e:
                    logger.debug(f"å¤„ç†XMLå±æ€§æ—¶å‡ºç°å°é—®é¢˜: {str(e)}")
        except Exception as e:
            logger.debug(f"è®¿é—®_elementæ—¶å‡ºç°é—®é¢˜: {str(e)}")
        
        return properties
    
    @staticmethod
    def properties_identical(props1: Dict[str, Any], props2: Dict[str, Any]) -> bool:
        """
        æ¯”è¾ƒä¸¤ä¸ªå±æ€§å­—å…¸æ˜¯å¦å®Œå…¨ç›¸åŒ
        
        Args:
            props1: ç¬¬ä¸€ä¸ªå±æ€§å­—å…¸
            props2: ç¬¬äºŒä¸ªå±æ€§å­—å…¸
            
        Returns:
            bool: å¦‚æœæ‰€æœ‰å±æ€§éƒ½ç›¸åŒè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
            
        Note:
            - Noneå’ŒFalseè¢«è§†ä¸ºä¸åŒçš„å€¼ï¼ˆWordä¸­æœ‰åŒºåˆ«ï¼‰
            - ä¼šæ¯”è¾ƒæ‰€æœ‰å±æ€§ï¼ŒåŒ…æ‹¬åªåœ¨ä¸€ä¸ªå­—å…¸ä¸­å­˜åœ¨çš„å±æ€§
        """
        # è·å–æ‰€æœ‰é”®çš„å¹¶é›†
        all_keys = set(props1.keys()) | set(props2.keys())
        
        for key in all_keys:
            val1 = props1.get(key)
            val2 = props2.get(key)
            
            # ä¸¥æ ¼æ¯”è¾ƒï¼ŒNoneå’ŒFalseä¸ç›¸ç­‰
            if val1 != val2:
                return False
        
        return True
    
    @staticmethod
    def calculate_property_hash(properties: Dict[str, Any]) -> str:
        """
        è®¡ç®—å±æ€§å­—å…¸çš„å“ˆå¸Œå€¼ï¼Œç”¨äºå¿«é€Ÿæ¯”è¾ƒ
        
        Args:
            properties: å±æ€§å­—å…¸
            
        Returns:
            str: å±æ€§çš„MD5å“ˆå¸Œå€¼ï¼ˆ32å­—ç¬¦åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
            
        Note:
            - ç›¸åŒçš„å±æ€§ç»„åˆæ€»æ˜¯äº§ç”Ÿç›¸åŒçš„å“ˆå¸Œå€¼
            - ç”¨äºå¿«é€Ÿåˆ¤æ–­ä¸¤ä¸ªrunçš„æ ·å¼æ˜¯å¦ç›¸åŒ
        """
        try:
            # å°†å±æ€§å­—å…¸è½¬æ¢ä¸ºç¨³å®šçš„å­—ç¬¦ä¸²è¡¨ç¤º
            # æ’åºç¡®ä¿ç›¸åŒå±æ€§äº§ç”Ÿç›¸åŒå“ˆå¸Œ
            sorted_items = sorted(properties.items())
            prop_str = json.dumps(sorted_items, sort_keys=True, default=str)
            return hashlib.md5(prop_str.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.warning(f"è®¡ç®—å±æ€§å“ˆå¸Œæ—¶å‡ºç°é”™è¯¯: {str(e)}")
            # è¿”å›ä¸€ä¸ªåŸºäºé”™è¯¯çš„å”¯ä¸€å€¼
            return hashlib.md5(f"error_{id(properties)}".encode()).hexdigest()


class RunBoundaryDetector:
    """
    Runè¾¹ç•Œæ£€æµ‹å™¨
    
    è¯†åˆ«runä¸­çš„ç‰¹æ®Šè¾¹ç•Œå…ƒç´ ï¼Œè¿™äº›å…ƒç´ é˜»æ­¢runåˆå¹¶ï¼š
    - åŸŸä»£ç ï¼ˆField codesï¼‰
    - è¶…é“¾æ¥ï¼ˆHyperlinksï¼‰
    - ä¹¦ç­¾ï¼ˆBookmarksï¼‰
    - æ‰¹æ³¨å’Œä¿®è®¢ï¼ˆComments and Revisionsï¼‰
    - å„ç§å¼•ç”¨ï¼ˆè„šæ³¨ã€å°¾æ³¨ç­‰ï¼‰
    - åˆ†éš”ç¬¦ï¼ˆæ¢è¡Œã€åˆ†é¡µç­‰ï¼‰
    - åµŒå…¥å¯¹è±¡ï¼ˆå›¾ç‰‡ã€å›¾è¡¨ç­‰ï¼‰
    """
    
    @staticmethod
    def detect_special_boundaries(run) -> Dict[str, bool]:
        """
        æ£€æµ‹runæ˜¯å¦åŒ…å«ç‰¹æ®Šè¾¹ç•Œå…ƒç´ 
        
        Args:
            run: python-docxçš„Runå¯¹è±¡
            
        Returns:
            Dict[str, bool]: å„ç§è¾¹ç•Œå…ƒç´ çš„å­˜åœ¨çŠ¶æ€
            
        Note:
            - è¿”å›çš„å­—å…¸åŒ…å«æ‰€æœ‰å¯èƒ½çš„è¾¹ç•Œç±»å‹
            - Trueè¡¨ç¤ºå­˜åœ¨è¯¥ç±»å‹çš„è¾¹ç•Œå…ƒç´ 
            - ä»»ä½•è¾¹ç•Œå…ƒç´ çš„å­˜åœ¨éƒ½ä¼šé˜»æ­¢runåˆå¹¶
        """
        boundaries = {
            'has_field': False,      # åŸŸä»£ç 
            'has_bookmark': False,   # ä¹¦ç­¾
            'has_hyperlink': False,  # è¶…é“¾æ¥
            'has_comment': False,    # æ‰¹æ³¨
            'has_revision': False,   # ä¿®è®¢æ ‡è®°
            'has_footnote': False,   # è„šæ³¨
            'has_endnote': False,    # å°¾æ³¨
            'has_break': False,      # åˆ†éš”ç¬¦ï¼ˆæ¢è¡Œã€åˆ†é¡µç­‰ï¼‰
            'has_tab': False,        # åˆ¶è¡¨ç¬¦
            'has_drawing': False,    # å›¾å½¢å¯¹è±¡
            'has_inline_shape': False,  # å†…è”å½¢çŠ¶
            'has_math': False,       # æ•°å­¦å…¬å¼
        }
        
        try:
            if hasattr(run, '_element') and run._element is not None:
                element = run._element
                
                # æ£€æŸ¥çˆ¶å…ƒç´ ä»¥ç¡®å®šrunçš„ä¸Šä¸‹æ–‡
                parent = element.getparent()
                if parent is not None:
                    parent_tag = parent.tag if hasattr(parent, 'tag') else ''
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨è¶…é“¾æ¥ä¸­
                    if parent_tag.endswith('hyperlink'):
                        boundaries['has_hyperlink'] = True
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨åŸŸä¸­
                    if parent_tag.endswith('fldSimple') or parent_tag.endswith('instrText'):
                        boundaries['has_field'] = True
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä¹¦ç­¾ä¸­
                    if parent_tag.endswith('bookmarkStart') or parent_tag.endswith('bookmarkEnd'):
                        boundaries['has_bookmark'] = True
                
                # æ£€æŸ¥runå†…éƒ¨çš„å­å…ƒç´ 
                for child in element:
                    if not hasattr(child, 'tag'):
                        continue
                        
                    tag = child.tag
                    
                    # åˆ†éš”ç¬¦
                    if tag.endswith('br') or tag.endswith('cr'):
                        boundaries['has_break'] = True
                    # åˆ¶è¡¨ç¬¦
                    elif tag.endswith('tab'):
                        boundaries['has_tab'] = True
                    # å›¾å½¢å’Œå›¾ç‰‡
                    elif tag.endswith('drawing') or tag.endswith('pict'):
                        boundaries['has_drawing'] = True
                    # å†…è”å¯¹è±¡
                    elif tag.endswith('object'):
                        boundaries['has_inline_shape'] = True
                    # è„šæ³¨å¼•ç”¨
                    elif tag.endswith('footnoteReference'):
                        boundaries['has_footnote'] = True
                    # å°¾æ³¨å¼•ç”¨
                    elif tag.endswith('endnoteReference'):
                        boundaries['has_endnote'] = True
                    # æ‰¹æ³¨å¼•ç”¨
                    elif tag.endswith('commentReference'):
                        boundaries['has_comment'] = True
                    # ä¿®è®¢æ ‡è®°
                    elif tag.endswith('ins') or tag.endswith('del'):
                        boundaries['has_revision'] = True
                    # æ•°å­¦å…¬å¼
                    elif tag.endswith('oMath') or tag.endswith('oMathPara'):
                        boundaries['has_math'] = True
                    # åŸŸä»£ç 
                    elif tag.endswith('fldChar') or tag.endswith('instrText'):
                        boundaries['has_field'] = True
        
        except Exception as e:
            logger.warning(f"æ£€æµ‹runè¾¹ç•Œæ—¶å‡ºç°é”™è¯¯: {str(e)}")
            # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œæ ‡è®°ä¸ºåŒ…å«æœªçŸ¥è¾¹ç•Œ
            boundaries['has_unknown'] = True
        
        return boundaries
    
    @staticmethod
    def can_merge_with_boundaries(boundaries1: Dict[str, bool], 
                                 boundaries2: Dict[str, bool]) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªåŒ…å«è¾¹ç•Œä¿¡æ¯çš„runæ˜¯å¦å¯ä»¥åˆå¹¶
        
        Args:
            boundaries1: ç¬¬ä¸€ä¸ªrunçš„è¾¹ç•Œæ£€æµ‹ç»“æœ
            boundaries2: ç¬¬äºŒä¸ªrunçš„è¾¹ç•Œæ£€æµ‹ç»“æœ
            
        Returns:
            bool: å¦‚æœå¯ä»¥å®‰å…¨åˆå¹¶è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
            
        Note:
            - åªæœ‰å½“ä¸¤ä¸ªrunéƒ½ä¸åŒ…å«ä»»ä½•ç‰¹æ®Šè¾¹ç•Œå…ƒç´ æ—¶æ‰èƒ½åˆå¹¶
            - è¿™æ˜¯ä¸€ä¸ªä¿å®ˆçš„ç­–ç•¥ï¼Œç¡®ä¿ä¸ä¼šç ´åæ–‡æ¡£ç»“æ„
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¾¹ç•Œå…ƒç´ å­˜åœ¨
        for key in boundaries1:
            if boundaries1.get(key, False) or boundaries2.get(key, False):
                return False
        
        # ä¹Ÿæ£€æŸ¥ç¬¬äºŒä¸ªå­—å…¸ä¸­ç‹¬æœ‰çš„é”®
        for key in boundaries2:
            if boundaries2.get(key, False):
                return False
        
        return True


class SafeRunMerger:
    """
    å®‰å…¨çš„Runåˆå¹¶å™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. åˆ†ææ®µè½ä¸­çš„æ‰€æœ‰runs
    2. è¯†åˆ«å¯ä»¥å®‰å…¨åˆå¹¶çš„runç»„
    3. æ‰§è¡Œåˆå¹¶æ“ä½œ
    4. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    
    è®¾è®¡åŸåˆ™ï¼š
    - å®‰å…¨ç¬¬ä¸€ï¼šåªåˆå¹¶100%ç¡®å®šä¸ä¼šæ”¹å˜æ ¼å¼çš„runs
    - ä¿å®ˆç­–ç•¥ï¼šæœ‰ç–‘é—®æ—¶ä¸åˆå¹¶
    - å®Œæ•´æ—¥å¿—ï¼šè®°å½•æ‰€æœ‰æ“ä½œä¾¿äºè°ƒè¯•
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åˆå¹¶å™¨
        
        Args:
            config: å¯é€‰çš„é…ç½®å­—å…¸ï¼Œæ”¯æŒä»¥ä¸‹é…ç½®é¡¹ï¼š
                - 'skip_empty_runs': bool, æ˜¯å¦è·³è¿‡ç©ºrunçš„åˆå¹¶ï¼ˆé»˜è®¤Trueï¼‰
                - 'min_group_size': int, æœ€å°åˆå¹¶ç»„å¤§å°ï¼ˆé»˜è®¤2ï¼‰
                - 'log_level': str, æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤'INFO'ï¼‰
        """
        self.analyzer = RunStyleAnalyzer()
        self.detector = RunBoundaryDetector()
        
        # é»˜è®¤é…ç½®
        self.config = {
            'skip_empty_runs': True,  # è·³è¿‡ç©ºrunï¼ˆå¯èƒ½æœ‰ç‰¹æ®Šç”¨é€”ï¼‰
            'min_group_size': 2,      # è‡³å°‘2ä¸ªrunæ‰æ‰§è¡Œåˆå¹¶
            'log_level': 'INFO'
        }
        
        # æ›´æ–°ç”¨æˆ·é…ç½®
        if config:
            self.config.update(config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.merge_stats = {
            'total_runs': 0,
            'merged_runs': 0,
            'merge_groups': 0,
            'skipped_runs': 0,
            'error_count': 0
        }
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        logger.setLevel(self.config['log_level'])
    
    def analyze_paragraph_runs(self, paragraph) -> List[Dict[str, Any]]:
        """
        åˆ†ææ®µè½ä¸­çš„æ‰€æœ‰runs
        
        Args:
            paragraph: python-docxçš„Paragraphå¯¹è±¡
            
        Returns:
            List[Dict]: æ¯ä¸ªrunçš„è¯¦ç»†åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                - index: runåœ¨æ®µè½ä¸­çš„ç´¢å¼•
                - text: runçš„æ–‡æœ¬å†…å®¹
                - properties: æ‰€æœ‰æ ·å¼å±æ€§
                - boundaries: è¾¹ç•Œå…ƒç´ æ£€æµ‹ç»“æœ
                - property_hash: å±æ€§å“ˆå¸Œå€¼
                - can_merge: æ˜¯å¦å¯ä»¥å‚ä¸åˆå¹¶
                
        Raises:
            RunOptimizationError: åˆ†æè¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯æ—¶
        """
        run_analysis = []
        
        try:
            for i, run in enumerate(paragraph.runs):
                try:
                    # æå–runä¿¡æ¯
                    properties = self.analyzer.extract_all_properties(run)
                    boundaries = self.detector.detect_special_boundaries(run)
                    property_hash = self.analyzer.calculate_property_hash(properties)
                    
                    # åˆ¤æ–­æ˜¯å¦å¯ä»¥å‚ä¸åˆå¹¶
                    can_merge = True
                    if self.config['skip_empty_runs'] and run.text == '':
                        can_merge = False
                        self.merge_stats['skipped_runs'] += 1
                    
                    # å¦‚æœæœ‰ä»»ä½•è¾¹ç•Œå…ƒç´ ï¼Œä¸èƒ½åˆå¹¶
                    if any(boundaries.values()):
                        can_merge = False
                    
                    analysis = {
                        'index': i,
                        'text': run.text,
                        'text_length': len(run.text),
                        'properties': properties,
                        'boundaries': boundaries,
                        'property_hash': property_hash,
                        'can_merge': can_merge
                    }
                    
                    run_analysis.append(analysis)
                    
                except Exception as e:
                    logger.error(f"åˆ†ærun {i} æ—¶å‡ºé”™: {str(e)}")
                    self.merge_stats['error_count'] += 1
                    
                    # æ·»åŠ é”™è¯¯æ ‡è®°çš„åˆ†æç»“æœ
                    run_analysis.append({
                        'index': i,
                        'text': run.text if hasattr(run, 'text') else '[ERROR]',
                        'text_length': 0,
                        'properties': {},
                        'boundaries': {'has_error': True},
                        'property_hash': f"error_{i}",
                        'can_merge': False,
                        'error': str(e)
                    })
            
        except Exception as e:
            logger.error(f"åˆ†ææ®µè½runsæ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
            raise RunOptimizationError(f"æ®µè½åˆ†æå¤±è´¥: {str(e)}")
        
        return run_analysis
    
    def find_mergeable_groups(self, run_analysis: List[Dict[str, Any]]) -> List[List[int]]:
        """
        æ‰¾å‡ºå¯ä»¥åˆå¹¶çš„runç»„
        
        Args:
            run_analysis: analyze_paragraph_runsè¿”å›çš„åˆ†æç»“æœ
            
        Returns:
            List[List[int]]: å¯åˆå¹¶çš„runç´¢å¼•ç»„åˆ—è¡¨
            
        Example:
            [[0, 1, 2], [5, 6]] è¡¨ç¤ºrun 0,1,2å¯ä»¥åˆå¹¶ï¼Œrun 5,6å¯ä»¥åˆå¹¶
        """
        if len(run_analysis) < self.config['min_group_size']:
            return []
        
        mergeable_groups = []
        current_group = []
        
        for i in range(len(run_analysis)):
            current_run = run_analysis[i]
            
            # å¦‚æœå½“å‰runä¸èƒ½åˆå¹¶ï¼Œç»“æŸå½“å‰ç»„
            if not current_run.get('can_merge', False):
                if len(current_group) >= self.config['min_group_size']:
                    mergeable_groups.append(current_group)
                current_group = []
                continue
            
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå¯åˆå¹¶çš„runï¼Œå¼€å§‹æ–°ç»„
            if not current_group:
                current_group = [i]
                continue
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥å½“å‰ç»„
            prev_run = run_analysis[current_group[-1]]
            
            # æ¯”è¾ƒå±æ€§å“ˆå¸Œ
            if prev_run['property_hash'] == current_run['property_hash']:
                current_group.append(i)
            else:
                # å±æ€§ä¸åŒï¼Œç»“æŸå½“å‰ç»„ï¼Œå¼€å§‹æ–°ç»„
                if len(current_group) >= self.config['min_group_size']:
                    mergeable_groups.append(current_group)
                current_group = [i]
        
        # å¤„ç†æœ€åä¸€ç»„
        if len(current_group) >= self.config['min_group_size']:
            mergeable_groups.append(current_group)
        
        logger.debug(f"æ‰¾åˆ° {len(mergeable_groups)} ä¸ªå¯åˆå¹¶ç»„")
        return mergeable_groups
    
    def merge_runs_in_paragraph(self, paragraph) -> int:
        """
        åˆå¹¶æ®µè½ä¸­çš„runs
        
        Args:
            paragraph: python-docxçš„Paragraphå¯¹è±¡
            
        Returns:
            int: æˆåŠŸåˆå¹¶çš„runsæ•°é‡
            
        Note:
            - ç›´æ¥ä¿®æ”¹ä¼ å…¥çš„paragraphå¯¹è±¡
            - ä¿è¯åˆå¹¶åæ ¼å¼å®Œå…¨ä¸å˜
            - å‡ºé”™æ—¶ä¼šå›æ»šåˆ°åŸå§‹çŠ¶æ€
        """
        try:
            # åˆ†æruns
            run_analysis = self.analyze_paragraph_runs(paragraph)
            if not run_analysis:
                return 0
            
            # æ‰¾å‡ºå¯åˆå¹¶ç»„
            mergeable_groups = self.find_mergeable_groups(run_analysis)
            if not mergeable_groups:
                return 0
            
            merged_count = 0
            
            # ä»åå‘å‰å¤„ç†ï¼Œé¿å…ç´¢å¼•å˜åŒ–é—®é¢˜
            for group in reversed(mergeable_groups):
                try:
                    if len(group) <= 1:
                        continue
                    
                    # è®°å½•åˆå¹¶å‰çš„ä¿¡æ¯ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
                    original_runs_info = []
                    for idx in group:
                        run = paragraph.runs[idx]
                        original_runs_info.append({
                            'text': run.text,
                            'properties': self.analyzer.extract_all_properties(run)
                        })
                    
                    # è·å–ç¬¬ä¸€ä¸ªrunä½œä¸ºä¿ç•™çš„run
                    first_run = paragraph.runs[group[0]]
                    
                    # åˆå¹¶æ–‡æœ¬
                    merged_text = ''.join(paragraph.runs[i].text for i in group)
                    
                    # ä¿å­˜ç¬¬ä¸€ä¸ªrunçš„å®Œæ•´æ ·å¼
                    first_run_props = self.analyzer.extract_all_properties(first_run)
                    
                    # æ›´æ–°ç¬¬ä¸€ä¸ªrunçš„æ–‡æœ¬
                    first_run.text = merged_text
                    
                    # åˆ é™¤å…¶ä»–runsï¼ˆä»åå‘å‰åˆ é™¤ï¼‰
                    for i in reversed(group[1:]):
                        try:
                            paragraph._element.remove(paragraph.runs[i]._element)
                            merged_count += 1
                        except Exception as e:
                            logger.error(f"åˆ é™¤run {i} æ—¶å‡ºé”™: {str(e)}")
                            # ç»§ç»­å¤„ç†å…¶ä»–runs
                    
                    self.merge_stats['merge_groups'] += 1
                    logger.debug(f"æˆåŠŸåˆå¹¶ {len(group)} ä¸ªruns: {group}")
                    
                except Exception as e:
                    logger.error(f"åˆå¹¶runç»„ {group} æ—¶å‡ºé”™: {str(e)}")
                    self.merge_stats['error_count'] += 1
                    # ç»§ç»­å¤„ç†å…¶ä»–ç»„
            
            return merged_count
            
        except Exception as e:
            logger.error(f"åˆå¹¶æ®µè½runsæ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
            self.merge_stats['error_count'] += 1
            return 0
    
    def optimize_document(self, doc_path: str, output_path: str, 
                         progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """
        ä¼˜åŒ–æ•´ä¸ªæ–‡æ¡£çš„runç»“æ„
        
        Args:
            doc_path: è¾“å…¥æ–‡æ¡£è·¯å¾„
            output_path: è¾“å‡ºæ–‡æ¡£è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(current, total)å‚æ•°
            
        Returns:
            Dict[str, Any]: ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
                - total_runs: åŸå§‹runæ€»æ•°
                - merged_runs: åˆå¹¶çš„runæ•°
                - merge_groups: åˆå¹¶ç»„æ•°
                - paragraphs_processed: å¤„ç†çš„æ®µè½æ•°
                - paragraphs_optimized: ä¼˜åŒ–çš„æ®µè½æ•°
                - optimization_rate: ä¼˜åŒ–ç‡
                - error_count: é”™è¯¯æ•°
                
        Raises:
            FileNotFoundError: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
            RunOptimizationError: ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯
        """
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(doc_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.merge_stats = {
            'total_runs': 0,
            'merged_runs': 0,
            'merge_groups': 0,
            'paragraphs_processed': 0,
            'paragraphs_optimized': 0,
            'skipped_runs': 0,
            'error_count': 0
        }
        
        try:
            # æ‰“å¼€æ–‡æ¡£
            logger.info(f"æ­£åœ¨æ‰“å¼€æ–‡æ¡£: {doc_path}")
            doc = docx.Document(doc_path)
            
            # ç»Ÿè®¡æ€»æ®µè½æ•°
            total_paragraphs = len(doc.paragraphs)
            logger.info(f"æ–‡æ¡£åŒ…å« {total_paragraphs} ä¸ªæ®µè½")
            
            # å¤„ç†æ¯ä¸ªæ®µè½
            for para_idx, paragraph in enumerate(doc.paragraphs):
                try:
                    # ç»Ÿè®¡åŸå§‹runæ•°
                    runs_before = len(paragraph.runs)
                    self.merge_stats['total_runs'] += runs_before
                    
                    # åªå¤„ç†æœ‰å¤šä¸ªrunsçš„æ®µè½
                    if runs_before > 1:
                        # æ‰§è¡Œåˆå¹¶
                        merged = self.merge_runs_in_paragraph(paragraph)
                        self.merge_stats['merged_runs'] += merged
                        
                        if merged > 0:
                            self.merge_stats['paragraphs_optimized'] += 1
                            logger.debug(f"æ®µè½ {para_idx}: åˆå¹¶äº† {merged} ä¸ªruns")
                    
                    self.merge_stats['paragraphs_processed'] += 1
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(para_idx + 1, total_paragraphs)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ®µè½ {para_idx} æ—¶å‡ºé”™: {str(e)}")
                    self.merge_stats['error_count'] += 1
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ®µè½
            
            # ä¿å­˜ä¼˜åŒ–åçš„æ–‡æ¡£
            logger.info(f"æ­£åœ¨ä¿å­˜ä¼˜åŒ–åçš„æ–‡æ¡£: {output_path}")
            doc.save(output_path)
            
            # è®¡ç®—ä¼˜åŒ–ç»Ÿè®¡
            if self.merge_stats['total_runs'] > 0:
                optimization_rate = (self.merge_stats['merged_runs'] / 
                                   self.merge_stats['total_runs'] * 100)
                self.merge_stats['optimization_rate'] = f"{optimization_rate:.1f}%"
            else:
                self.merge_stats['optimization_rate'] = "0%"
            
            logger.info(f"ä¼˜åŒ–å®Œæˆ: {self.merge_stats}")
            return self.merge_stats
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ–‡æ¡£æ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
            raise RunOptimizationError(f"æ–‡æ¡£ä¼˜åŒ–å¤±è´¥: {str(e)}")


class RunAnalysisReport:
    """
    Runåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
    
    æä¾›è¯¦ç»†çš„æ–‡æ¡£runç»“æ„åˆ†æï¼Œå¸®åŠ©äº†è§£ï¼š
    - Runåˆ†å¸ƒæƒ…å†µ
    - æ½œåœ¨çš„ä¼˜åŒ–ç©ºé—´
    - é—®é¢˜æ®µè½è¯†åˆ«
    - æ ·å¼å¤æ‚åº¦åˆ†æ
    """
    
    @staticmethod
    def generate_analysis_report(doc_path: str, 
                               detailed: bool = False) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æ¡£runåˆ†ææŠ¥å‘Š
        
        Args:
            doc_path: æ–‡æ¡£è·¯å¾„
            detailed: æ˜¯å¦ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼ˆåŒ…å«æ¯ä¸ªæ®µè½çš„è¯¦ç»†ä¿¡æ¯ï¼‰
            
        Returns:
            Dict[str, Any]: åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ï¼š
                - total_paragraphs: æ€»æ®µè½æ•°
                - total_runs: æ€»runæ•°
                - avg_runs_per_paragraph: å¹³å‡æ¯æ®µè½runæ•°
                - single_char_runs: å•å­—ç¬¦runæ•°
                - empty_runs: ç©ºrunæ•°
                - runs_with_special_boundaries: åŒ…å«ç‰¹æ®Šè¾¹ç•Œçš„runæ•°
                - style_variations: æ ·å¼å˜åŒ–æ•°
                - problematic_paragraphs: é—®é¢˜æ®µè½åˆ—è¡¨
                - optimization_potential: ä¼˜åŒ–æ½œåŠ›è¯„ä¼°
                - detailed_analysis: è¯¦ç»†åˆ†æï¼ˆå¦‚æœrequestedï¼‰
                
        Raises:
            FileNotFoundError: æ–‡æ¡£ä¸å­˜åœ¨
        """
        if not os.path.exists(doc_path):
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_path}")
        
        try:
            doc = docx.Document(doc_path)
            analyzer = RunStyleAnalyzer()
            detector = RunBoundaryDetector()
            
            # åˆå§‹åŒ–æŠ¥å‘Š
            report = {
                'file_path': doc_path,
                'file_size': os.path.getsize(doc_path),
                'total_paragraphs': len(doc.paragraphs),
                'total_runs': 0,
                'single_char_runs': 0,
                'empty_runs': 0,
                'runs_with_special_boundaries': 0,
                'style_variations': set(),
                'problematic_paragraphs': [],
                'run_length_distribution': {
                    '0': 0,      # ç©º
                    '1': 0,      # å•å­—ç¬¦
                    '2-5': 0,    # çŸ­
                    '6-20': 0,   # ä¸­ç­‰
                    '21-50': 0,  # é•¿
                    '50+': 0     # å¾ˆé•¿
                }
            }
            
            # è¯¦ç»†åˆ†ææ•°æ®
            if detailed:
                report['detailed_analysis'] = []
            
            # åˆ†ææ¯ä¸ªæ®µè½
            for para_idx, paragraph in enumerate(doc.paragraphs):
                runs = paragraph.runs
                para_text_length = len(paragraph.text)
                run_count = len(runs)
                
                report['total_runs'] += run_count
                
                # æ®µè½çº§ç»Ÿè®¡
                para_stats = {
                    'index': para_idx,
                    'run_count': run_count,
                    'text_length': para_text_length,
                    'avg_chars_per_run': para_text_length / run_count if run_count > 0 else 0,
                    'style_changes': 0,
                    'special_boundaries': 0
                }
                
                # æ£€æŸ¥é—®é¢˜æ®µè½ï¼ˆrunsè¿‡å¤šï¼‰
                if run_count > 10 and para_text_length < 100:
                    report['problematic_paragraphs'].append({
                        'index': para_idx,
                        'text_preview': paragraph.text[:50] + '...' if para_text_length > 50 else paragraph.text,
                        'run_count': run_count,
                        'avg_chars_per_run': para_stats['avg_chars_per_run'],
                        'severity': 'high' if run_count > 20 else 'medium'
                    })
                
                # åˆ†ææ¯ä¸ªrun
                prev_style_hash = None
                for run_idx, run in enumerate(runs):
                    run_text_length = len(run.text)
                    
                    # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
                    if run_text_length == 0:
                        report['empty_runs'] += 1
                        report['run_length_distribution']['0'] += 1
                    elif run_text_length == 1:
                        report['single_char_runs'] += 1
                        report['run_length_distribution']['1'] += 1
                    elif run_text_length <= 5:
                        report['run_length_distribution']['2-5'] += 1
                    elif run_text_length <= 20:
                        report['run_length_distribution']['6-20'] += 1
                    elif run_text_length <= 50:
                        report['run_length_distribution']['21-50'] += 1
                    else:
                        report['run_length_distribution']['50+'] += 1
                    
                    # æ£€æŸ¥ç‰¹æ®Šè¾¹ç•Œ
                    boundaries = detector.detect_special_boundaries(run)
                    if any(boundaries.values()):
                        report['runs_with_special_boundaries'] += 1
                        para_stats['special_boundaries'] += 1
                    
                    # æ”¶é›†æ ·å¼å˜åŒ–
                    try:
                        prop_hash = analyzer.calculate_property_hash(
                            analyzer.extract_all_properties(run)
                        )
                        report['style_variations'].add(prop_hash)
                        
                        # ç»Ÿè®¡æ®µè½å†…æ ·å¼å˜åŒ–
                        if prev_style_hash and prev_style_hash != prop_hash:
                            para_stats['style_changes'] += 1
                        prev_style_hash = prop_hash
                        
                    except Exception as e:
                        logger.warning(f"åˆ†ærunæ ·å¼æ—¶å‡ºé”™: {str(e)}")
                
                # æ·»åŠ åˆ°è¯¦ç»†åˆ†æ
                if detailed:
                    report['detailed_analysis'].append(para_stats)
            
            # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
            report['style_variations'] = len(report['style_variations'])
            report['avg_runs_per_paragraph'] = (report['total_runs'] / report['total_paragraphs'] 
                                               if report['total_paragraphs'] > 0 else 0)
            
            # è¯„ä¼°ä¼˜åŒ–æ½œåŠ›
            optimization_score = 0
            if report['avg_runs_per_paragraph'] > 5:
                optimization_score += 30
            if report['single_char_runs'] > report['total_runs'] * 0.1:
                optimization_score += 20
            if report['empty_runs'] > report['total_runs'] * 0.05:
                optimization_score += 10
            if len(report['problematic_paragraphs']) > report['total_paragraphs'] * 0.1:
                optimization_score += 20
            if report['style_variations'] < report['total_runs'] * 0.5:
                optimization_score += 20
            
            report['optimization_potential'] = {
                'score': optimization_score,
                'level': ('high' if optimization_score >= 60 else 
                         'medium' if optimization_score >= 30 else 'low'),
                'recommendation': (
                    'å¼ºçƒˆå»ºè®®ä¼˜åŒ–' if optimization_score >= 60 else
                    'å»ºè®®ä¼˜åŒ–' if optimization_score >= 30 else
                    'ä¼˜åŒ–ç©ºé—´æœ‰é™'
                )
            }
            
            # é—®é¢˜æ®µè½æ’åºï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦ï¼‰
            report['problematic_paragraphs'].sort(
                key=lambda x: x['run_count'], 
                reverse=True
            )
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
            raise RunOptimizationError(f"åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")


# ä¾¿æ·å‡½æ•°
def quick_optimize(input_path: str, output_path: str = None) -> Dict[str, Any]:
    """
    å¿«é€Ÿä¼˜åŒ–Wordæ–‡æ¡£çš„runç»“æ„
    
    Args:
        input_path: è¾“å…¥æ–‡æ¡£è·¯å¾„
        output_path: è¾“å‡ºæ–‡æ¡£è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œå°†æ·»åŠ '_optimized'åç¼€ï¼‰
        
    Returns:
        Dict[str, Any]: ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯
        
    Example:
        >>> stats = quick_optimize("document.docx")
        >>> print(f"ä¼˜åŒ–å®Œæˆï¼Œå‡å°‘äº†{stats['merged_runs']}ä¸ªruns")
    """
    if output_path is None:
        base, ext = os.path.splitext(input_path)
        output_path = f"{base}_optimized{ext}"
    
    merger = SafeRunMerger()
    return merger.optimize_document(input_path, output_path)


def analyze_document(doc_path: str, detailed: bool = False) -> None:
    """
    åˆ†æå¹¶æ‰“å°æ–‡æ¡£runç»“æ„æŠ¥å‘Š
    
    Args:
        doc_path: æ–‡æ¡£è·¯å¾„
        detailed: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    report = RunAnalysisReport.generate_analysis_report(doc_path, detailed)
    
    print(f"\n=== æ–‡æ¡£Runåˆ†ææŠ¥å‘Š ===")
    print(f"æ–‡ä»¶: {report['file_path']}")
    print(f"æ–‡ä»¶å¤§å°: {report['file_size'] / 1024:.1f} KB")
    print(f"\nåŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»æ®µè½æ•°: {report['total_paragraphs']}")
    print(f"  æ€»Runæ•°: {report['total_runs']}")
    print(f"  å¹³å‡æ¯æ®µè½Runæ•°: {report['avg_runs_per_paragraph']:.2f}")
    print(f"  å•å­—ç¬¦Runæ•°: {report['single_char_runs']} ({report['single_char_runs']/report['total_runs']*100:.1f}%)")
    print(f"  ç©ºRunæ•°: {report['empty_runs']}")
    print(f"  åŒ…å«ç‰¹æ®Šå…ƒç´ çš„Runæ•°: {report['runs_with_special_boundaries']}")
    print(f"  æ ·å¼å˜åŒ–æ•°: {report['style_variations']}")
    
    print(f"\nRuné•¿åº¦åˆ†å¸ƒ:")
    for length_range, count in report['run_length_distribution'].items():
        if count > 0:
            percentage = count / report['total_runs'] * 100
            print(f"  {length_range}: {count} ({percentage:.1f}%)")
    
    print(f"\nä¼˜åŒ–æ½œåŠ›è¯„ä¼°:")
    print(f"  è¯„åˆ†: {report['optimization_potential']['score']}/100")
    print(f"  çº§åˆ«: {report['optimization_potential']['level']}")
    print(f"  å»ºè®®: {report['optimization_potential']['recommendation']}")
    
    if report['problematic_paragraphs']:
        print(f"\né—®é¢˜æ®µè½ï¼ˆRunè¿‡å¤šï¼‰:")
        for i, p in enumerate(report['problematic_paragraphs'][:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  {i+1}. æ®µè½ {p['index']}: {p['run_count']} runs, "
                  f"å¹³å‡{p['avg_chars_per_run']:.1f}å­—ç¬¦/run, ä¸¥é‡ç¨‹åº¦: {p['severity']}")
            print(f"     é¢„è§ˆ: {p['text_preview']}")


# ä¸»ç¨‹åºç¤ºä¾‹
def main():
    """
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Word Runä¼˜åŒ–å™¨
    """
    # æ£€æŸ¥å…¼å®¹æ€§
    try:
        check_compatibility()
    except DocxCompatibilityError as e:
        print(f"å…¼å®¹æ€§é”™è¯¯: {e}")
        return
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ç¤ºä¾‹æ–‡æ¡£è·¯å¾„
    input_doc = "/Users/wangqiang/Downloads/ç‰¹æœºæ–‡ä»¶/è½¯æ–‡éƒ¨/é²²é¹c3.8/CN-é²²é¹å¸æœºæ‰‹å†Œ-åŸç‰ˆ.docx"
    output_doc = "/Users/wangqiang/Downloads/ç‰¹æœºæ–‡ä»¶/è½¯æ–‡éƒ¨/é²²é¹c3.8/CN-é²²é¹å¸æœºæ‰‹å†Œ-åŸç‰ˆ-runOptimized.docx"
    
    try:
        # 1. åˆ†ææ–‡æ¡£
        print("=== æ­¥éª¤1: åˆ†ææ–‡æ¡£ ===")
        analyze_document(input_doc)
        
        # 2. ä¼˜åŒ–æ–‡æ¡£
        print("\n=== æ­¥éª¤2: ä¼˜åŒ–æ–‡æ¡£ ===")
        
        # åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„åˆå¹¶å™¨
        config = {
            'skip_empty_runs': True,
            'min_group_size': 2,
            'log_level': 'INFO'
        }
        merger = SafeRunMerger(config)
        
        # å®šä¹‰è¿›åº¦å›è°ƒ
        def progress_callback(current, total):
            percentage = (current / total) * 100
            print(f"\rå¤„ç†è¿›åº¦: {percentage:.1f}% ({current}/{total})", end='')
        
        # æ‰§è¡Œä¼˜åŒ–
        stats = merger.optimize_document(input_doc, output_doc, progress_callback)
        print()  # æ¢è¡Œ
        
        # 3. æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
        print("\n=== ä¼˜åŒ–ç»“æœ ===")
        print(f"å¤„ç†æ®µè½æ•°: {stats['paragraphs_processed']}")
        print(f"ä¼˜åŒ–æ®µè½æ•°: {stats['paragraphs_optimized']}")
        print(f"åˆå¹¶Runç»„æ•°: {stats['merge_groups']}")
        print(f"å‡å°‘Runæ•°: {stats['merged_runs']}")
        print(f"è·³è¿‡çš„ç©ºRunæ•°: {stats['skipped_runs']}")
        print(f"é”™è¯¯æ•°: {stats['error_count']}")
        print(f"ä¼˜åŒ–ç‡: {stats['optimization_rate']}")
        
        # 4. åˆ†æä¼˜åŒ–åçš„æ–‡æ¡£
        print("\n=== æ­¥éª¤3: éªŒè¯ä¼˜åŒ–ç»“æœ ===")
        analyze_document(output_doc)
        
    except FileNotFoundError as e:
        print(f"æ–‡ä»¶é”™è¯¯: {e}")
    except RunOptimizationError as e:
        print(f"ä¼˜åŒ–é”™è¯¯: {e}")
    except Exception as e:
        print(f"æœªçŸ¥é”™è¯¯: {e}")
        logger.exception("å‘ç”ŸæœªçŸ¥é”™è¯¯")


if __name__ == "__main__":
    main()